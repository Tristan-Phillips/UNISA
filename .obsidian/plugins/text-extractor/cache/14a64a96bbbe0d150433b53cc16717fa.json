{"path":"Subjects/INF3703 - Databases II/Unsorted/INF3703/Stuvia-869018-inf3703-exams-solutions-2016-06-to-2018-06.pdf","text":"INF3703 Exams + Solutions (2016-06 to 2018-06) written by francoissmit www.stuvia.com Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal INF3703 exams + Solutions (Databases II) Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace 2018-06 Section A 1. Horizontal data fragmentation is a process in a distributed database design that breaks a table into subsets of columns from the original table. FALSE. Horizontal fragmentation divides a relation (table) into subsets of rows not columns. p575 2. Database Management System (DBMS) governs the storage and processing of logically related data over interconnected computer systems in which both data and processing are distributed among several sites. FALSE. The definition refers to a distributed database management system (DDBMS). p554 3. Like any IT infrastructure, Business Intelligence (BI) architecture is composed of many interconnected parts such as data, people, technology, and process that work together to facilitate and enhance a business management and governance. TRUE. p592 4. In the data preparation phase of data-mining, identifying, cleaning any data impurities and integrating data are the main tasks executed by data-miners. FALSE. In this phase, the data has already been integrated into the data warehouse. The data warehouse usually is the target for data-mining operations. 5. The characteristic of big data that refers to the changes in the meaning of the data based on context is known as veracity. FALSE. This is known as variability. 6. The three basic Open Database Connectivity (ODBC) architectures are ODBC driver that communicates directly to the DMBS, a high-level ODBC API, and a driver manager in charge of all database connections. TRUE. Components, not architectures (?) p683 7. A server side extension is a program that interacts directly with the web browser to handle specific types of requests and display the results. FALSE. Server side extensions interact directly with the web server. p693 8. All of the following (delivering, testing, training, organising and monitoring) except testing are the managerial role of a Database Administrator. FALSE. Testing is included. p733 9. Backup and recovery, system support, security auditing and monitoring, and performance auditing and monitoring are the four main areas of DBMS operations. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace FALSE. Performance monitoring and tuning (not auditing). p742 10. Serialisability is a transaction property ensuring that once transaction changes are done and committed, they cannot be undone or lost, even in the event of a system failure. FALSE. Serialisability is the property in which the selected order of concurrent transaction operations creates the same final database state that would have been produced if the transactions had been executed in a serial fashion. The question refers to Durability. p487 2018-6 SECTION B Question 2 [20] 2.1. Shopcheap is a South Africa (SA) based company. They also have other branches in different African countries like Nigeria and Angola. Shopcheap database has an EMPLOYEE table containing the following attributes: EM_NAME, EMP_SALARY, EMP_ADDRESS, EMP_BRANCH, EMP_DEPARTMENT, EMP_DOB. The employees’ data are distributed over three locations: South Africa, Nigeria, and Angola. The table is divided by location, that is, the South African employees’ data are stored in fragment A, Nigeria employees’ data are stored in fragment B and Angola employees’ data are stored in fragment C. See Figure 1 below. As a newly graduated Database Administrator (DBA) employed by Shopcheap to work with their IT team, one of the managers approached you to assist him in listing all employees born on the 10 of December 1988. Given the above scenario, do the following. a. Name and discuss the three levels of distributed transparency. [6]  Fragmentation transparency is the highest level of distribution transparency. The end user or programmer does not need to know that a database is partitioned. Therefore, neither fragment names nor fragment locations are specified prior to data access.  Location transparency exists when the end user or programmer must specify the database fragment names but does not need to specify where those fragments are located.  Local mapping transparency exists when the end user or programmer must specify both the fragment names and their locations. b. Use the request above to illustrate the use of various transparency levels. Do this by writing three different cases of SQL query with each case conforming to each level of distribution transparency. [14] Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Case 1 – Fragmentation transparency: SELECT * FROM EMPLOYEE WHERE EMP_DOB = ’10-DEC-1988’; Case 2 – Location transparency: SELECT * FROM A WHERE EMP_DOB = ’10-DEC-1988’ UNION SELECT * FROM B WHERE EMP_DOB = ’10-DEC-1988’ UNION SELECT * FROM C WHERE EMP_DOB = ’10-DEC-1988’; Case 3 – Local mapping transparency: SELECT * FROM A NODE SAF WHERE EMP_DOB = ’10-DEC-1988’ UNION SELECT * FROM B NODE NGA WHERE EMP_DOB = ’10-DEC-1988’ UNION SELECT * FROM A NODE ANG WHERE EMP_DOB = ’10-DEC-1988’; Question 3 [14] 3.1. Khumo Ndlovu is an inventory manager for a marketing research company based in Western Cape. He wants to analyse the use of suppliers by the company departments. Khumo realised that his friend Lesedi Ndindela has developed a spreadsheet-based data warehouse model that she uses to analyse sales data. Khumo is interested in developing a data warehouse like Lesedi’s so he can analyse orders by department, supplier and product. a. Identify the appropriate fact table components. [2] Khumo will require an ORDERS fact table. b. Identify the appropriate dimension tables. [3] Since Khumo wants to analyse by department, supplier and product, they will be the required dimension tables: DEPARTMENT, SUPPLIER and PRODUCT. c. Identify the attributes for the dimension tables. [2] Each dimension table will have a primary key (such as DEPT_ID, SUPP_ID, PROD_ID) as well as any additional useful attributes (see below). d. Draw a star schema diagram for this data warehouse. [3] Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace 3.2. What are the most common techniques you would use to improve the performance of a star schema? [4] The following four techniques are often used to optimize data warehouse design:  Normalizing dimensional tables  Maintaining multiple fact tables to represent different aggregation levels  Denormalising fact tables  Partitioning and replicating tables See p618-621 (12 th edition) for full explanations of these techniques. Question 4 [19] 4.1. Velocity, a key characteristic of Big Data refers to the rate at which new data enters the system as well as the rate at which the data must be processed. The velocity of processing is broken into two categories. With an aid of an illustrative diagram, show and discuss feedback loop as one of the categories of velocity of processing. Use any scenario of your choice for the illustrative diagram. You may also use the scenario used in your textbook. [11] Feedback loop processing refers to the analysis of the data to produce actionable results. The process of capturing the data, processing it into usable information, and then acting on that information is a feedback loop. Feedback loop processing to provide immediate results requires analysing large amounts of data within just a few seconds so that the results of the analysis can become a part of the product delivered to the user in real time. Feedback loop processing is also used to help organisations sift through data to help decision makers make faster strategic and tactical decisions, and it is a key component in data analytics. Figure 14.3 shows a feedback loop for providing recommendations for book purchases. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace 4.2. What does serialisability of a transaction mean? Where is it considered important? [4] Serialisability is a property in which the selected order of concurrent transaction operations creates the same final database state that would have been produced if the transactions had been executed in a serial fashion. This property is important in multiuser and distributed databases in which multiple transactions are likely to be executed concurrently. 4.3. How would you describe database statistics? Why are they important? [4] Database statistics refer to a number of measurements about database objects, such as the number of rows in a table, number of disk blocks used, maximum and average row length, number of columns in each row, and number of distinct values in each column. Such statistics provide a snapshot of database characteristics. They are important because they play a crucial role in query optimisation where the DBMS will use them in query processing to improve efficiency. Question 5 [27] 5.1. As newly graduated DBA that you are, you learnt that designing and implementing database is one of the DBA technical function. a. Describe to the IT team you work with the activities that are typically associated with the design and implementation services of the DBA technical function. [8] The DBA provides data-modelling and design services to end users. One of the primary activities of a DBA is to determine and enforce standards and procedures to be used, making sure that all activities are performed within this framework. The DBA then provides necessary assistance and support during the design of the database at the conceptual, logical, and physical levels. The DBA works with application programmers to ensure the quality and integrity of database design and ensuring transactions are correct, efficient and compliant. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace The DBA must provide oversight and assistance in determination and creation of storage space, data loading, conversion and database migration services, as well as the generation, compilation and storage of the application’s access plan. The DBA must develop, test, and implement operational procedures required by the new system before the application comes online. Fine-tuning and reconfiguring of the DBMS may also be required. b. What technical skills are desirable in the DBA’s personnel to perform design and implementation function? [2] Database design and programming skills. 5.2. Discuss in your own words what a database connectivity is. Although there are many ways to achieve database connectivity, name the five interfaces discussed in your textbook. [7] Database connectivity refers to the mechanisms through which application programs connect and communicate with data repositories. The means by which an application connects and talks to the data stored in persistent storage structures. The five interfaces are:  Native SQL connectivity (vendor provided)  Microsoft’s Open Database Connectivity (ODBC), Data Access Objects (DAO), and Remote Data Objects (RDO)  Microsoft’s Object Linking and Embedding for Database (OLE-DB)  Microsoft’s ActiveX Data Objects (ADO.NET)  Oracle’s Java Database Connectivity (JDBC) 5.3. How would you describe what cloud computing is? Explain why it is considered a “game changer”. [4] Cloud computing is a computing model that provides ubiquitous, on-demand access to a shared pool of configurable resources (e.g. networks, servers, storage, applications and services) that can be rapidly provisioned with minimal effort. Cloud computing can be called a “game changer” since it eliminates financial and technological barriers so organizations can leverage database technologies in their business processes with minimal effort and cost. In fact, cloud services have the potential to turn basic IT services into “commodity” services such as electricity, gas, and water, and to enable a revolution that could change not only the way that companies do business, but the IT business itself. 5.4. Name and contrast the types of cloud computing implementation. [6]  Public cloud. This type of cloud infrastructure is built by a third-party organization to sell cloud services to the general public, e.g. AWS, MS Azure. Cloud consumers share resources with other consumers transparently. Managed the third-party.  Private cloud. This type of internal cloud is built by an organization for the sole purpose of servicing its own needs. Could be managed internally or by third-party. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace  Community cloud. This type of cloud is built by and for a specific group of organizations that share a common trade, such as agencies of the federal government, the military, or higher education. Could be managed internally or by third-party. Question 6 [19] 6.1. All transaction properties work together to make sure that a database maintains data integrity and consistency for either a single-user or multi-user DBMS. Name and explain all the transaction properties and where necessary use examples in your explanations. [5]  Atomicity requires that all operations (SQL requests) of a transaction be completed; if not, the transaction is aborted. If a transaction T1 has four SQL requests, all four requests must be successfully completed; otherwise, the entire transaction is aborted. In other words, a transaction is treated as a single, indivisible, logical unit of work.  Consistency indicates the permanence of the database’s consistent state. A transaction takes a database from one consistent state to another. When a transaction is completed, the database must be in a consistent state. If any of the transaction parts violates an integrity constraint, the entire transaction is aborted.  Isolation means that the data used during the execution of a transaction cannot be used by a second transaction until the first one is completed. In other words, if transaction T1 is being executed and is using the data item X, that data item cannot be accessed by any other transaction (T2 … Tn) until T1 ends. Particularly useful in multiuser database environments.  Durability ensures that once transaction changes are done and committed, they cannot be undone or lost, even in the event of a system failure. 6.2. What is concurrency control in a database? Explain why it is important and its objective. [4] Coordinating the simultaneous execution of transactions in a multiuser database system is known as concurrency control. The objective of concurrency control is to ensure the serialisability of transactions in a multiuser database environment. It is important because the simultaneous execution of transactions over a shared database can create several data integrity and consistency problems such as lost updates, uncommitted data, and inconsistent retrievals. 6.3. With the help of a diagram, demonstrate the flow of the phases in query processing and the steps required in each phase. [10] The DBMS processes a query in three phases: 1. Parsing. The DBMS parses the SQL query and chooses the most efficient access/execution plan. Using the query optimizer, the parsing steps involve validating the query and decomposing into more atomic components. An efficient access plan is created (which indexes to use, best join options) and stored in SQL cache if it doesn’t exist already. 2. Execution. The DBMS executes the SQL query using the chosen execution plan. Proper locks are acquired if necessary and the data is retrieved and placed in the data cache. 3. Fetching. The DBMS fetches the data and sends the result set back to the client. Matching rows are retrieved, sorted, grouped and aggregated (if required) and returned to the client. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Question 7 [27] 7.1. Briefly explain the three levels of distribution transparency. [6] See 2.1a. 7.2. “Big data” is one of the emerging concepts in database management systems. What do you understand by big data and what are its basic components? [10] Big Data generally refers to a set of data that displays the characteristics of volume, velocity, and variety (the “3 Vs”) to an extent that makes the data unsuitable for management by a relational database management system. These data sets may be analysed to reveal patterns, trends and associations. The characteristics (3 Vs) can be defined as follows:  Volume – the quantity of data to be stored. The storage capacities associated with Big Data are extremely large. Systems need to either scale up or scale out to handle the volume.  Velocity – the speed at which data is entering the system as well as the rate at which the data must be processed. Data streams need to be processed and filtered to decide which data to keep/discard when it is not feasible to keep all the data. Feedback loop processing can provide immediate actionable results and requires analysing large amounts of data within seconds.  Variety – the variations in the structure of the data to be stored. Big Data is mostly semi- structured or unstructured and thus not suitable for relational databases. Big Data requires that the data be captured in whatever format it naturally exists, without any attempt to impose a data model or structure to the data. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Besides the 3 Vs there are a few additional characteristics:  Variability refers to the changes in the meaning of the data based on context.  Veracity refers to the trustworthiness of the data. Can decision makers reasonably rely on the accuracy of the data and the information generated from it?  Value, also called viability, refers to the degree to which the data can be analysed to provide meaningful information that can add value to the organization. Just because a set of data can be captured does not mean that it should be captured.  Visualization is the ability to graphically present the vast amounts of data in such a way as to make it understandable. 7.3. Discuss in your own words what a database connectivity is. Although there are many ways to achieve database connectivity, name the five interfaces discussed in your textbook. [7] See 5.2. 7.4. The DBMS processes query in three phases. Identify the first two phases, and outline what is accomplished in each phase. [4] See 6.3. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace 2017-10 SECTION A 1. SQL data service is an in-house data management service providing relational data storage, access and management to companies of all sizes. FALSE. It is cloud computing-based, not in-house. p716 2. Extensible Markup Language (XML) as a metalanguage was developed because HTML does not allow the manipulation of data elements. TRUE. HTML can only describe how to display the content in a web browser. p702 3. For better understanding of OLE-DB model, its functionalities are divided into two types of objects, namely CUSTOMERS and PROVIDERS. FALSE. CONSUMERS and PROVIDERS. p685 4. Normalising dimensional tables, denormalising fact tables, partitioning and replicating tables and maintaining single fact tables to represent different aggregation levels are the 4 techniques that are often used to optimise data warehouse design. FALSE. ...maintaining multiple facts tables... p617 5. Monitoring results to evaluate the outcomes of business decisions is outside the scope of Business Intelligence (BI). FALSE. It is part of the BI framework. p592 6. A Distributed Database Management System (DDBMS) property which guarantees that database transactions maintain the integrity and consistency of the distributed database is known as “distributed transparency”. FALSE. Distribution transparency is a feature that allows a distributed database to look like a single logical database to the end-user. The question refers to transaction transparency. p565 7. Database systems can be classified on the basis of how process distribution and data distribution are supported and one type of process describes a scenario where all processes run on a computer sharing a single data repository is multiple-site processing, single-site data (MPSD). TRUE. p562 8. SQL performance tuning is a set of activities and procedures designed to ensure that end-users query is processed by the DBMS in the minimum amount of time while making the best use of resources. FALSE. This is the definition of database performance tuning of which SQL performance tuning is a part. p516 Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace 9. Database recovery techniques are based on the serialisability transaction property which indicates that the data used during the execution of a transaction cannot be used by a second transaction until the first one is completed. FALSE. Serialisability is a property in which the selected order of concurrent transaction operations creates the same final database state that would’ve been produced if the transactions had been executed in a serial fashion. The question refers to isolation. p487 10. A lock that exists when concurrent transactions are granted Read access on the basis of a common lock is known as a shared lock. It is the lock that must be used when the potential for conflict exists. FALSE. First half is true, but an exclusive lock must be used when the potential for conflict exists. p499 2017-10 SECTION B Question 2 [24] 2.1. As a prospective database administrator, you understand that backup and recovery functions constitute a very important component of today’s Database Management Systems (DBMSs). Name and describe the three levels of backup that may be used for database recovery. [6]  A full backup, or dump, of the entire database. In this case, all database objects are backed up in their entirety.  A differential backup of the database, in which only the objects that have been updated or modified since the last full backup are backed up.  A transaction log backup, which backs up only the transaction log operations that are not reflected in a previous backup copy of the database. In this case, no other database objects are backed up. p455 2.2. Concurrency control with optimistic approach does not require locking or time stamping techniques, instead a transaction is executed without restrictions until it is committed. Using an optimistic approach, each transaction moves through different phases. Name these phases and briefly describe what happens in each phase. [6] 1. During the read phase, the transaction reads the database, executes the needed computations, and makes the updates to a private copy of the database values. All update operations of the transaction are recorded in a temporary update file, which is not accessed by the remaining transactions. 2. During the validation phase, the transaction is validated to ensure that the changes made will not affect the integrity and consistency of the database. If the validation test is positive, the transaction goes to the write phase. If the validation test is negative, the transaction is restarted and the changes are discarded. 3. During the write phase, the changes are permanently applied to the database. p504 2.3. Explain to the organisation for which you have developed a database what is a database consistency state and how it is achieved. [3] Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace  A consistent database state is one in which all data integrity constraints are satisfied.  To ensure consistency of the database, every transaction must begin with the database in a known consistent state. If the database is not in a consistent state, the transaction will yield an inconsistent database that violates its integrity and business rules.  All transactions are controlled and executed by the DBMS to guarantee database integrity. p484 2.4. Explain to your organisation the three main problems resulting from concurrent execution of transactions in a multi-user database system. Use an example to illustrate your explanation. [9] The schedule for the concurrent execution of multiple transactions must exhibit the property of serialisability, that the selected order of concurrent transaction operations creates the same final database state as if they had been executed in a serial fashion. p487. This coordination is called concurrency control. The 3 main problems that can result without this control are: 1. Lost updates – occurs when two concurrent transactions, T1 and T2, are updating the same data element and one of the updates is lost (overwritten by the other transaction). E.g. two transactions, T1 and T2, update a product quantity value. Both read the current value, e.g. 35 at the same time. T1 adds 100 to the value, the total is now 135. T2 now subtracts 30 from the value it read originally and gets 5. The final value of 5 gets written when it should be 105. T1’s change was lost. 2. Uncommitted data – When the first transaction, T1, is rolled back after a second transaction, T2, has accessed the uncommitted data. E.g. T1 updates the stored quantity value from 35 to 135. T2 reads this value and subtracts 30 to get 105. Now T1 is rolled back and the value is 35 again. T2 then overwrites this with 105. The correct outcome of T2 should’ve been 35-30 = 5. 3. Inconsistent retrieval – when a transaction accesses data before and after one or more transactions finish working with such data. E.g. T1 calculates some aggregate function (e.g. sum of all product quantities) while T2 is updating the same data (e.g. adding to the product quantity values). p490 Question 3 [18] 3.1. In your assignment 2 you developed a database or improved a database for an organisation of your choice. Assuming that the database was implemented, explain to the organisation the sequence of interaction between their end-users and the DBMS through the use of queries to generate information. [4] 1. The end-user (client-end) application generates a query. 2. The query is sent to the DBMS (server end). 3. The DBMS (server end) executes the query. 4. The DBMS sends the resulting data set to the end-user (client-end) application. p516 3.2. Differentiate between DBMS performance tuning and SQL performance tuning. [4] Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace  SQL performance tuning – Activities to help generate a SQL query that returns the correct answer in the least amount of time, using the minimum amount of resources at the server end.  DBMS performance tuning – Activities to ensure that clients’ requests are addressed as quickly as possible while making optimum use of existing resources.  SQL performance tuning is done on the client-side (query optimisation) and DBMS performance tuning is done on the server-side (caching, optimiser modes). p517 3.3. Although the number of processes and names of typical DBMS processes vary from vendor to vendor, the functionality is similar. Looking at the figure on page 4, list and describe some typical DBMS processes the figure illustrates. [10]  Listener – The listener process listens for clients’ requests and handles the processing of the SQL requests to other DBMS processes. Once a request is received, the listener passes the request to the appropriate user process.  User – The DBMS creates a user process to manage each client session. Therefore, when you log on to the DBMS, you are assigned a user process. This process handles all requests you submit to the server. There are many user processes – at least one per logged-in client.  Scheduler – The scheduler process organizes the concurrent execution of SQL requests.  Lock manager – This process manages all locks placed on database objects, including disk pages.  Optimizer – The optimizer process analyses SQL queries and finds the most efficient way to access the data. You will learn more about this process later in the chapter. p519-520 Question 4 [18] 4.1. What is transaction transparency? What are some of the basic concepts that you should know to understand how transactions are managed in a DDBMS? [6] Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Transaction transparency is a DDBMS property that ensures database transactions will maintain the distributed database’s integrity and consistency, and that the transaction will be completed only when all database sites involved in the transaction complete their part of the transaction. p568 Basic concepts:  Remote request – A features that allows a single SQL statement to access data in a single remote data processor (DP).  Remote transaction – A feature that allows a transaction (formed by several requests) to access data in a single remote (DP).  Distributed request – allows a single SQL statement to access data in several remote DPs in a distributed database.  Distributed transaction – accesses data in several remote DPs in a distributed database. p569 4.2. The following data structure and constraints exist for a magazine publishing company. The company publishes one regional magazine each in city Pretoria (PTA), Florida (FL), Durban (DB), and Cape Town (CT).  The company has 300,000 customers (subscribers) distributed throughout the four cities listed above.  On the first day of each month, an annual subscription INVOICE is printed and sent to each customer whose subscription is due for renewal. The INVOICE entity contains a REGION attribute to indicate the city (PTA, FL, DB, CT) in which the customer resides. See below the table CUSTOMER and INVOICE. CUSTOMER(CUS_NUM, CUS_NAME, CUS_ADDRESS, CUS_CITY, CUST_ZIP, CUS_SUBSDATE) INVOICE(INV_NUM, INV_REGION, CUST_NUM, INV_DATE, INV_TOTAL)  The company’s management is aware of the problems associated with centralised management and has decided that it is time to decentralise the management of the subscriptions in its four regional subsidiaries. Each subscription site will handle its own customers and invoices data. The company’s management, however, wants to have access to customers and invoices data to generate annual reports and to issue ad hoc queries, such as: List all current customers by region List all new customers by region Report all invoices by customer and by region Given these requirements, what type of data fragmentation is needed for each table? Draw the structure for each fragmentation. [12]  The CUSTOMER table must be partitioned horizontally by city (CUS_CITY attributes).  The INVOICE table must be partitioned horizontally by region (INV_REGION attribute).  Horizontal fragmentation of the CUSTOMER table: Fragment Name Location Condition Node Name Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace C1 Pretoria CUS_CITY = ‘PTA’ PTA C2 Florida CUS_CITY = ‘FL’ FL C3 Durban CUS_CITY = ‘DB’ DB C4 Cape Town CUS_CITY = ‘CT’ CT  Horizontal fragmentation of the INVOICE table: Fragment Name Location Condition Node Name I1 Pretoria INV_REGION = ‘PTA’ PTA I2 Florida INV_REGION = ‘FL’ FL I3 Durban INV_REGION = ‘DB’ DB I4 Cape Town INV_REGION = ‘CT’ CT  Example fragments for Pretoria:  Fragment: C1 Location: Pretoria Node: PTA CUS_NUM CUS_NAME CUS_ADDRESS CUS_CITY CUS_ZIP CUS_SUBSDATE 18384 John Smith 123 Free Street Pretoria 8000 18 Jan 2010 21653 Jane Doe 45 Loop Street Pretoria 8000 26 Oct 2016 p575-578 Question 5 [20] 5.1. Mr E. Smith is a manager of a small distribution company. Due to the fact that the business of the company is expanding fast in other places, Elvis identifies that it is time to manage the massive information pool to help assist and guide the accelerating growth. Mr Smith approached you as a newly employed database designer in the company, and asked you to design a data warehouse that will enable the company employees, particularly himself, to study sales figures by year, region, salesperson and product. Draw a star schema diagram for the multimedia distributed company data warehouse. Each table should have at least two attributes, primary and foreign keys. [8] Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace p616 5.2. Discuss the main types of objects in the OLE-DB model. [6]  Consumers are objects (applications or processes) that request and use data. Consumers request data by invoking the methods exposed by the data provider objects (public interface) and passing the required parameters.  Providers are objects that manage the connection with a data source and provide data to the consumers. o Data providers provide data to other processes. Database vendors create data provider objects that expose the functionality of the underlying data source. o Service providers provide additional functionality to consumers. The service provider is located between the data provider and the consumer. The service provider requests data from the data provider, transforms the data, and then provides the transformed data to the data consumer. p685-686 5.3. What are the three main components in the basic ODBC architecture? What are their functions? [6] The basic ODBC architecture has three main components:  A high-level ODBC API through which application programs access ODBC functionality.  A driver manager that is in charge of managing all database connections.  An ODBC driver that communicates directly to the DBMS. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace 2017-06 Section A - Question 1 1. See 2017-10 (1.8). 2. Working with data in data files is much faster than working with data caches because the DBMS does not have to wait for the hard disk to retrieve the data and no I/O operations are needed to work within the data file. FALSE. Switch “data file” and “data cache” around for a true statement. p519 3. Query parsing is a phase in which all rows that match the specified condition(s) are retrieved, sorted and grouped. FALSE. This refers to the SQL fetching phase. Query parsing is when the DBMS analyses the query and chooses the most efficient access/execution plan. p522 4. Data replication and partitioning is a star schema performance improvement technique that splits and makes a copy of a database table. FALSE. Those are performance improvement techniques for distributed DBMS. p578 5. Non-volatile ensures that data are never deleted and new data are continually added, so the warehouse is always growing. TRUE. p608 6. Distributed DBMS governs the storage and processing of logically related data over interconnected computer systems. It also ensures standard communication protocols at the database level. FALSE. There are no standard communication protocols at the database level. Different vendors employ different, often incompatible techniques to manage the distribution of data and processing in a DBMS environment. p557 7. The reason behind the development of star schema was because the existing relational modelling techniques such as ER and normalisation did not yield a database structure that served advanced data analysis requirements well. TRUE. 8. The level of transparency supported by the DDBMS differs from system to system. Location and local mapping are examples of distributed transparency. TRUE. Levels, not examples (?) p565 9. The data cache is where the data read from the database data files are stored before the data have been read or after the data are written to the database data file. FALSE. Switch “before” and “after” for a true statement. p519 Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace 10. Cloud computing allows organisations to quickly and economically add information technology services such as applications, storage, servers, processing power, database and infrastructure to its IT portfolio. TRUE. p710 Question 2 2.1) Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace  Consumer: objects that request and use data. Consumers request data by invoking methods exposed by the data provider objects and passing the required parameters  Providers: objects that manage the connection with a data source and provide data to the consumers. 2 categories:  Data providers: provide data to other processes. DB vendors create data provider objects that expose the functionality of the underlying data source.  Service providers: provide additional functionality to consumers. It is located between the data provider and the consumer. Service provider requests data from the data provider, transforms the data, and then provides the transformed data to the data consumer. i.e. the service provider acts like a data consumer of the data provider and as a data provider for the data consumer. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Question 3 Question 3 [14] 3.1) Explain in details what a data mart is and provide reasons why some organizations prefer to create a data mart instead of data warehouse. [6] A data mart is a small, single-subject data warehouse subset that provides decision support to a small group of people. A data mart could be created from data extracted from a larger data warehouse for the specific purpose of supporting faster data access to a target group or function. Some organisations prefer a data mart because:  Lower cost and shorter implementation time (6 months to a year vs 1-3 years).  A company’s employees are more likely to embrace minor changes that lead to improved decision support with a data mart rather than major changes that a data warehouse would impose.  People at different organisational levels will likely require data with different summarization, aggregation and presentation formats.  Data marts can serve as a test vehicle for organisations exploring the potential benefits of a data warehouse. Think: LCSI, CEEMC, SAP, TV 3.2) Evis Velekhaya is a manager of a small multimedia distributed company. Due to the fact that the business of the company is fast growing, Elvis identifies that it is time to manage the massive information pool to help assist and guide the accelerating growth. As newly employed database designer in the company, Evis asked you to develop a data warehouse application that will enable the company employees particularly himself to study sales figures by year, region, salesperson and product Draw a star schema diagram for the multimedia distributed company data warehouse. The diagram should have at least two attributes, primary and foreign keys. [8] Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Question 4 [18] 4.1) Distributed transparency allows a physically dispersed database to be managed as though it were a centralized database. The level of transparency supported by the DDBMS varies from system to system. Briefly explain the three (3) levels of distribution transparency. [6] 1 Fragmentation transparency is the highest level of distribution transparency. The end user or programmer does not need to know that a database is partitioned. Therefore, neither fragment names nor fragment locations are specified prior to data access. 2 Location transparency exists when the end user or programmer must specify the database fragment names but does not need to specify where those fragments are located. 3 Local mapping transparency exists when the end user or programmer must specify both the fragment names and their locations. Think local thus gives location and mapping gives fragment names. Think: FLLM, also since we dealing in fragment we give fragment names or location 4.2) Discuss the three properties of the CAP theorem. [6]  Consistency – In a distributed database, all nodes should see the same data at the same time, which means that the replicas should be immediately updated. However, this involves dealing with latency and network partitioning delays.  Availability – Simply speaking, a request is always fulfilled by the system. No received request is ever lost. If you are buying tickets online, you do not want the system to stop in the middle of the operation.  Partition tolerance – The system continues to operate even in the event of a node failure. The system will fail only if all nodes fail. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace 4.3) The DBMS processes query in three phases. Identify those phases, and outline what is accomplished in each phase. [6] The DBMS processes a query in three phases: 1. Parsing. The DBMS parses the SQL query and chooses the most efficient access/execution plan. Using the query optimizer, the parsing steps involve validating the query and decomposing into more atomic components. An efficient access plan is created (which indexes to use, best join options) and stored in SQL cache if it doesn’t exist already. 2. Execution. The DBMS executes the SQL query using the chosen execution plan. Proper locks are acquired if necessary and the data is retrieved and placed in the data cache. 3. Fetching. The DBMS fetches the data and sends the result set back to the client. Matching rows are retrieved, sorted, grouped and aggregated (if required) and returned to the client. Question 5 [26] 5.1) Suppose the database system of your organization has failed. Do the following: a. Explain to the management of your organisation what database recovery process is. [2] Database recovery is the process of restoring a database to a previous consistent state; restoring data that has been lost, accidentally deleted, corrupted or made inaccessible. b. Describe to them the database recovery process you will follow to recover the database. [6] I would use transaction recovery procedures, which are based on the atomic transaction property. The transaction log contains data for database recovery purposes. Database transaction recovery uses data in the transaction log to recover a database from an inconsistent state to a consistent state.  Write-ahead-log protocol – Transaction logs are written before any data is actually updated.  Redundant transaction logs ensure that a disk failure will not impair the recovery.  Database checkpoints are performed regularly to write buffer content to disk and update the logs. Checkpoints play an important role in transaction recovery. Think DB recovery process: TRP based ATP, log; WRD c. Also describe to them the use of deferred-write and write-through techniques. [6]  Deferred-write technique (aka deferred update) – the transaction operations do not immediately update the physical database, only the transaction log is updated. The database is physically updated only with data from committed transactions, using information from the transaction log. If the transaction aborts before it reaches its commit point, no changes (no ROLLBACK or undo) need to be made to the database because it was never updated.  Write-through technique (aka immediate update) – the database is immediately updated by transaction operations during the transaction’s execution, even before the transaction reaches its commit point. If the transaction aborts before it reaches its Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace commit point, a ROLLBACK or undo operation needs to be done to restore the database to a consistent state. Think: Deferred wait until update, write through immediate update 5.2) In question 2.1 we assume that you and your team have designed and developed a database for ABCM. According to the description provided in question 2, ABCM has branches across South Africa provinces and abroad with its main office based in Johannesburg. This implies that ABCM database is a multi-user database and should therefore implement concurrency control. Explain to ABCM top management: a. What concurrency control is? [2] Coordinating the simultaneous execution of transactions in a multiuser database system is known as concurrency control. The objective of concurrency control is to ensure the serialisability of transactions in a multiuser database environment b. What the objectives of concurrency control would be for their company. [4] c. The objective of concurrency control is to ensure the serialisability of transactions in a multiuser database environment d. Importance of concurrency control for their company. [6] It is important because the simultaneous execution of transactions over a shared database can create several data integrity and consistency problems such as lost updates, uncommitted data, and inconsistent retrievals. Think LUI Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace 2016-10 SECTION A 1. Database recovery techniques are based on the durability transaction property, which ensures that once a transaction changes are done (committed), they cannot be undone or lost. FALSE. Recovery techniques are based on the atomic transaction property which states that all portions of the transaction must be treated as a single, logical unit of work in which all operations are applied and completed to produce a consistent database. 2. DBMS and end users interact through the use of systems to generate information in different sequence. FALSE. The DBMS and end-users interact through the use of queries to generate information in a standard sequence. p516 3. Replication transparency, hardware independence, partition processing and transaction processing are examples of C.J. Date’s twelve commandments for distributed databases. FALSE. Partition processing is not listed in the twelve commandments. p583 4. The implementation phase of database design includes installing the DBMS, creating the database storage structure, introduce changes and loading the database. FALSE. The implementation phase includes creating the database storage structure, loading data into the database, and providing for data management. 5. The Data Administrator (DA) and Database Administrator (DBA) functions overlap. TRUE. p730 6. See 2017-06 (1.8). 7. Data mining is a small, single-subject data warehouse subset that provides decision support to a small group of people. FALSE. The definition is for a data mart. p610 8. Attribute hierarchy is a component of star schemas that provide top-down data organisation. TRUE. p614 9. The first stage of logical database design is to translate the physical database design into a set of relational database structure. FALSE. Logical design is the task of creating a conceptual data model that could be implemented in any DBMS. p468 10. SQL transactions are formed by several SQL statements and database requests. Each database request originates one I/O database operation. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace FALSE. Each database request originates several I/O database operations. p510 2016-10 SECTION B Question 2 [30] 2.1. Data security, privacy and integrity are important database functions in authorization management. Briefly describe seven activities (procedures) a DBA should execute in his/her management roles in order to enforce these functions. [14] Authorization management defines procedures that protect and guarantee database security and integrity. Those procedures include the following: User access management – This function is designed to limit access to the database; it likely includes at least the following procedures:  Define each user to the database – At the OS level, the DBA can request the creation of a unique user ID for each end user who logs on to the system. At the DBMS level, the DBA can either create a different user ID or employ the same one to authorize the user to access the DBMS. o Assign passwords to each user – The DBA also performs this function at both the OS and DBMS levels. The database passwords can be assigned with predetermined expiration dates. o Define user groups – Classifying users into groups according to common access needs can help the DBA control and manage the access privileges of individual users. o Assign access privileges – The DBA assigns access privileges to specific users to access certain databases, e.g. read-only, read, write, and delete privileges. o Control physical access – Physical security can prevent unauthorized users from directly accessing the DBMS installation and facilities, e.g. secured entrances, password- protected workstations, CCTV and biometric technology.  View definition – The DBA must define data views to protect and control the scope of the data that are accessible to an authorized user.  DBMS access control – Database access can be controlled by placing limits on the use of DBMS query and reporting tools. The DBA must make sure the tools are used properly and only by authorized personnel.  DBMS usage monitoring – The DBA must also audit the use of data in the database, e.g. using an audit log, which automatically records a brief description of database operations performed by all users. Such audit trails enable the DBA to pinpoint access violations. p748 2.2. Identify and explain the critical success factors in the development and implementation of a successful data administration strategy. [12] Critical success factors include the following managerial, technological, and corporate culture issues:  Management commitment – The commitment of top-level management is necessary to enforce the use of standards, procedures, planning, and controls.  Thorough analysis of the company situation – The current state of the corporate data administration must be analysed to understand the company’s position and to have a clear Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace vision of what must be done, e.g. how are database analysis, design, documentation, implementation, standards, codification, and other issues handled?  End-user involvement – Successful change requires that people be able to adapt to it. Users should have an open communication channel to upper management to ensure success of the implementation.  Defined standards – Analysts and programmers must be familiar with appropriate methodologies, procedures, and standards. If not, they might need training.  Training – The vendor must train DBA personnel in the use of the DBMS and other tools. End users must be trained to use the tools, standards, and procedures. Key personnel should be trained first, so they can train others.  A small pilot project – A small project is recommended to ensure that the DBMS will work in the company, that it produces expected output, and that the personnel have been trained properly. p755 2.3. Discuss the distinction between top-down and bottom-up approaches in database design. [4] There are two classical approaches to database design:  Top-down design starts by identifying the data sets and then defines the data elements for each of those sets. This process involves the identification of different entity types and the definition of each entity’s attributes.  Bottom-up design first identifies the data elements (items) and then groups them together in data sets. In other words, it first defines attributes, and then groups them to form entities. p473 Question 3 [20] 3.1. Briefly describe what cloud computing is and how it relates to the concept of cloud services. [4] See 2018-6 (5.3) 3.2. Cloud computing can be implemented in three different types depending on who the target customers are. List and explain (WITH EXAMPLE) the different implementation types. [8]  Public cloud – This type of cloud infrastructure is built by a third-party organization to sell cloud services to the general public. The public cloud is the most common type of cloud implementation; examples include Amazon Web Services (AWS), Google Application Engine, and Microsoft Azure. In this model, cloud consumers share resources with other consumers transparently. The public cloud infrastructure is managed exclusively by the third-party provider.  Private cloud – This type of internal cloud is built by an organization for the sole purpose of servicing its own needs. Private clouds are often used by large, geographically dispersed organizations to add agility and flexibility to internal IT services. The cloud infrastructure could be managed by internal IT staff or an external third party.  Community cloud – This type of cloud is built by and for a specific group of organizations that share a common trade, such as agencies of the federal government, the military, or Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace higher education. The cloud infrastructure could be managed by internal IT staff or an external third party. p712 3.3. Outline the steps required in the development of an ER diagram. [8] 1. Identify, analyse, and refine the business rules. 2. Identify the main entities, using the results of Step 1. 3. Define the relationships among the entities, using the results of Steps 1 and 2. 4. Define the attributes, primary keys, and foreign keys for each of the entities. 5. Normalize the entities. 6. Complete the initial ER diagram. 7. Validate the ER model against the end users’ information and processing requirements. 8. Modify the ER model, using the results of Step 7. p461 Question 4 [12] 4.1. Database transaction recovery focuses on the different methods used to recover a database from an inconsistent to a consistent state by using the data in the transaction log. Discuss the four important concepts that affect the recovery process. [8]  The write-ahead-log protocol ensures that transaction logs are always written before any database data is actually updated. This protocol ensures that, in case of a failure, the database can later be recovered to a consistent state using the data in the transaction log.  Redundant transaction logs ensure that a physical disk failure will not impair the DBMS’s ability to recover data.  Database buffers are temporary storage areas in primary memory used to speed up disk operations. The DBMS software reads the data from the physical disk and stores a copy of it on a “buffer” in primary memory. When a transaction updates data, it actually updates the copy of the data in the buffer because that process is much faster. Later, all buffers that contain updated data are written to a physical disk during a single operation, saving significant time.  Database checkpoints are operations in which the DBMS writes all of its updated buffers in memory (aka dirty buffers) to disk. A checkpoint operation is also registered in the transaction log. As a result of this operation, the physical database and the transaction log will be in sync. This synchronization is required because update operations update the copy of the data in the buffers and not in the physical database. Checkpoints also play an important role in transaction recovery. p506-507 4.2. What is an exclusive lock, and under what circumstances is it granted? [4] An exclusive lock exists when access is reserved specifically for the transaction that locked the object. The exclusive lock must be used when the potential for conflict exists. An exclusive lock is issued when a transaction requests permission to update a data item and no locks are held on that data item by any other transaction. An exclusive lock does not allow other transactions to access the database. p499 Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Question 5 [18] 5.1. State out the difference between the following database terms: a. A rule-based optimizer and a cost based optimizer [2] A rule-based optimizer uses pre-set rules and points to determine the best approach to execute a query, assigning a “fixed cost” to each SQL operation. A cost-based optimizer uses sophisticated algorithms based on statistics about the objects being accessed to determine the best approach to execute a query. p528 b. SQL performance tuning and DBMS performance tuning [2] On the client side, the objective is to generate a SQL query that returns the correct answer in the least amount of time, using the minimum amount of resources at the server end. On the server side, the DBMS environment must be properly configured to respond to clients’ requests in the fastest way possible, while making optimum use of existing resources. p517 c. Data cache and SQL cache [2] The data cache, or buffer cache, is a shared, reserved memory area that stores the most recently accessed data blocks in RAM. The SQL cache, or procedure cache, is a shared, reserved memory area that stores the processed versions of the most recently executed SQL statements or PL/SQL procedures, including triggers and functions. p519 5.2. A fully distributed database management system must perform all of the functions of a centralised DBMS. Name these functions. [8] 1. Receive the request of an application or end user. 2. Validate, analyse, and decompose the request. The request might include mathematical and logical operations. 3. Map the request’s logical-to-physical data components. 4. Decompose the request into several disk I/O operations. 5. Search for, locate, read, and validate the data. 6. Ensure database consistency, security, and integrity. 7. Validate the data for the conditions, if any, specified by the request. 8. Present the selected data in the required format. p559 5.3. A star schema is a data modelling technique that is used to map multidimensional decision support data into a relational database. What are the four star schema performance- improvement techniques? [4] See 2018-6 (3.2) Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace 2016-06 Section A - Question 1 1. The objective of SQL performance tuning on the client side is to generate a SQL query that returns the correct answer in the least amount of time, using the maximum amount of resources on the server side. FALSE. Should be “using the minimum amount of resources at the server end”. 2. All database transactions must display at least the properties of atomicity, consistency, convenience and isolation. FALSE. The properties of atomicity, consistency, isolation and durability (ACID). 3. Database recovery techniques are based on the isolation transaction property which indicate that the data used during execution of a transaction cannot be used by a second transaction until the first one is completed. FALSE. Recovery techniques are based on the atomic transaction property which states that all portions of the transaction must be treated as a single, logical unit of work in which all operations are applied and completed to produce a consistent database. 4. In a distributed database, concurrency control becomes important because multi-site and multiple-process operations are more likely to create data inconsistencies and deadlocked transactions than single-site systems. TRUE. p571 5. Open Database Connectivity (ODBC) is a Microsoft middleware that adds object-oriented functionality for access to relational and non-relational. FALSE. ODBC provides a database access API to Windows applications. The question refers to OLE-DB. p685 6. All transaction management commands are processed during the parsing and execution phases of query processing. TRUE. p524 7. See 2017-06 (1.4). 8. Platform as a Service (PaaS) is the cloud service provider that offers consumers the ability to provision their own resources on demand. FALSE. PaaS is a model in which the cloud service provider can build and deploy consumer- created applications using the provider’s cloud infrastructure (e.g. AWS, MS Azure). The question refers to Infrastructure as a Service. 9. See 2016-10 (1.7). Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace 10. Fragmentation transparency is the highest level of transparency where the end user or programmer does not need to know that a database is partitioned. TRUE. p565 Question 2 2.1) Not applicable because Chapter 9 2.2) Name and describe the activities that are typically associated with the Database Administrator (DBA) technical function? - Evaluating, selecting, and installing the DBMS and related utilities: 1st step of DBA is to determine company needs and then selecting the DBMS, utility software and supporting hardware to meet these needs. - Designing and implementing DBs and applications: DBA provides data- modelling and design services to end-users. Primary activity of DBA is to determine and enforce standards and procedures to be used then the DBA must ensure the database-modelling and design activities must conform to the procedures and standards. - Testing and evaluating DBs and applications: DBA must provide testing and evaluation services for all DB and end user applications. - Operating the DBMS, utilities, and applications: The DBA must ensure system performance is satisfactory by establishing DBMS performance goals and then fixing performance issues. DBA must also consider available storage resources and deal with such issues. DBA must also manage backups effectively. - Training and supporting users: The DBA must train people to use the DBMS and it’s tools effectively. DBA must give support to end-users and programmers for when they encounter issues. - Maintaining the DBMS, utilities, and applications: The DBA must manage the maintenance of the DBMS by managing the physical or secondary storage devices. Maintenance for the DBA also includes upgrading the DBMS and utility software with added features. 2.3) What are the desirable technical skills/understanding a DBA’s personnel should have? - They must have the right mix of technical and managerial skills to provide a smooth transition to the new shared-data-environment - They must have excellent interpersonal and communications skills combined with broad organizational and business understanding - Personnel must have broad DB design and programming skills Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Question 3 3.1) Not applicable because Chapter 9 3.2) Not applicable because Chapter 9 3.3) With the help of a diagram, demonstrate the flow of the phases in query processing and the steps required in each phase. Question 4 4.1) All transaction properties work together to make sure that a database (DB) maintains data integrity and consistency for either a single-user or a multi-user DBMS. Name and explain all the transaction and consistency for either a single-user or a multi-user DBMS. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace  Atomicity requires that all operations (SQL requests) of a transaction be completed; if not, the transaction is aborted. If a transaction T1 has four SQL requests, all four requests must be successfully completed; otherwise, the entire transaction is aborted. In other words, a transaction is treated as a single, indivisible, logical unit of work.  Consistency indicates the permanence of the database’s consistent state. A transaction takes a database from one consistent state to another. When a transaction is completed, the database must be in a consistent state. If any of the transaction parts violates an integrity constraint, the entire transaction is aborted.  Isolation means that the data used during the execution of a transaction cannot be used by a second transaction until the first one is completed. In other words, if transaction T1 is being executed and is using the data item X, that data item cannot be accessed by any other transaction (T2 … Tn) until T1 ends. Particularly useful in multiuser database environments.  Durability ensures that once transaction changes are done and committed, they cannot be undone or lost, even in the event of a system failure. 4.2) What is concurrency control in a DB? Explain why it is important and its objective?  Concurrency control is coordinating the simultaneous execution of transactions in a multiuser DB system.  Objective of concurrency control is to ensure the serializability of transactions in a multiuser DB environment.  Concurrency control is important because the simultaneous execution of transactions over a shared DB can create several data integrity and consistency problems. Question 5 5.1) Distributed DB design includes all the design elements of centralized DB, however it introduces three new design issues. Discuss the three new issues introduced by distributed DB design. The design of a distributed DB introduces 3 new issues: Data fragmentation: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace  Data fragmentation allows you to break a single object into 2 or more segments or fragments.  The object might be a user’s Db, a system DB, or a table.  Each fragment can be stored at any site over a computer network.  3 data fragmentation strategies exist: - Horizontal fragmentation: division of a relation into subsets of tuples (rows). - Vertical Fragmentation: refers to the division of a relation into attribute subsets. - Mixed fragmentation: refers to a combination of horizontal and vertical strategies. Data Replication:  Data Replication refers to the storage of data copies at multiple sites served by a computer network.  Fragment copies can be stored at several sites to serve specific information requests.  Fragment copies can enhance data availability and response time and data copies can help to reduce communication and total query costs.  Replicated data:  Replicated data is subject to the mutual consistency rule, which requires that all copies of data fragments be identical to maintain data consistency among replicas. i.e. DDBMS updates replicas  2 styles of replication: - Push replication - Pull replication Data Allocation Data allocation describes the process of deciding where to locate data. There are 3 data allocation strategies:  Centralized data allocation: entire DB is stored at one site  Partitioned data allocation: the DB is divided into 2 or more disjoint parts and stored at 2 or more sites  Replicated data allocation: Copies of one or more DB fragments are stored at several sites. Powered by TCPDF (www.tcpdf.org) Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal","libVersion":"0.2.3","langs":""}