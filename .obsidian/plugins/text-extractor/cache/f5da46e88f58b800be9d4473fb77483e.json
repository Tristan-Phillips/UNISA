{"path":"UNISA/98906 - BSc Science in Computing/INF3720 - Human-Computer Interaction II/Unsorted/Slides/chapter_14_5e.pdf","text":"Chapter 14 INTRODUCING EVALUATION The aims • Explain the key concepts and terms used in evaluation • Introduce range of different types of evaluation methods. • Show how different evaluation methods are used for different purposes at different stages of the design process and in different contexts of use. • Show how evaluators mixed and modified to meet the demands of evaluating novel systems. • Discuss some of the practical challenges of doing evaluation. • Illustrate through case studies how methods discussed in Chapters 8, 9 and 10 are used in evaluation and describe some methods that are specific to evaluation. • Provide an overview of methods that are discussed in detail in the next two chapters. www.id-book.com 2 Why, what, where and when to evaluate Iterative design & evaluation is a continuous process that examines: • Why: to check users’ requirements and that they can use the product and they like it. • What: a conceptual model, early prototypes of a new system and later, more complete prototypes and to compare it with competitors’ products. • Where: in natural, in-the wild, and laboratory settings. • When: throughout design; finished products can be evaluated to collect information to inform new products. www.id-book.com 3 Bruce Tognazzini tells you why you need to evaluate “Iterative design, with its repeating cycle of design and testing, is the only validated methodology in existence that will consistently produce successful results. If you don’t have user-testing as an integral part of your design process you are going to throw buckets of money down the drain.” See AskTog.com for topical discussions about design and evaluation. www.id-book.com 4 Types of evaluation • Controlled settings that directly involve users (eg. usability and research labs). • Natural settings involving users (eg online communities and products that are used in public places). Often there is little or no control over what users do, especially in in-the-wild settings. • Any setting that doesn’t directly involve users (eg consultants and researchers critique the prototypes, and may predict and model how successful they will be when used by users. www.id-book.com 5 Living labs • People’s use of technology in their everyday lives can be evaluated in living labs. • Such evaluations are too difficult to do in a usability lab. • An early example was the Aware Home that was embedded with a complex network of sensors and audio/video recording devices (Abowd et al., 2000). • More recent examples include whole blocks and cities that house hundreds of people, eg Verma et al’s, 2017 research in Switzerland. • Many citizen science projects can also be thought of as living labs eg iNaturalist.com • These examples illustrate how the concept of a lab is changing to include other spaces where people’s use of technology can be studied in realistic environments. www.id-book.com 6 Evaluation case studies • A classic experimental investigation into the physiological responses of players of a computer game • An ethnographic study of visitors at the the Royal Highland show in which participants are directed and tracked using a cell phone app. • Crowdsourcing in which the opinions and reactions of volunteers (ie the crowd) inform technology evaluation. www.id-book.com 7 Challenge & engagement in a collaborative immersive game • Physiological measures were used • Players were more engaged when playing against another person than when playing against a computer. • Why was the physiological data collected normalized? www.id-book.com 8 Challenge & engagement in a collaborative immersive game www.id-book.com 9 The display shows the physiological data (top right), participants and a screen of the game they played . Source: Mandryk and Inkpen (2004) The Physiological Indicatorsfor the Evaluation of Co-located Collaborative Play. CSCW’2004, pp 102-111. Reproduced with permission of ACM Publications. Example of physiological data www.id-book.com 10 A participants’ skin response when scoring a goal against a friend (a), and another participants’ response when when engaging in a hockey fight against a friend versus against the computer (b). Source: Mandryk and Inkpen (2004) The Physiological Indicatorsfor the Evaluation of Co-located Collaborative Play. CSCW’2004, pp 102-111. Reproduced with permission of ACM Publications. Ethnobot app used at the Royal Highland Show? www.id-book.com 11 Notice that the Ethnobat directed Billy to a particular place (ie Aberdeenshire Village). Next, Ethnobot asks “…what’s going on?” The screen shows five of the experience buttons from which Billy needs to select a response. Source: Tallyn et al. (2018) Reproduced with permission from ACM Publishers. Experience responses submitted in Ethnobot www.id-book.com 12 The number of prewritten experience responses submitted by participants to the pre-established questions that Ethnobot ” asked them about their experiences. Source: Tallyn et al. (2018) Reproduced with permission from ACM Publishers. What did we learn from the case studies? • How to observe users in the lab and in natural settings. • How evaluators excerpt different levels of control in the lab and in natural settings and in crowdsourcing evaluation studies. • Use of different evaluation methods • How to develop different data collection and analysis techniques to evaluate user experience goals such as challenge and engagement. • The ability to run experiments on the Internet that are quick and inexpensive using crowdsourcing. • About recruiting a large number of participants using Mechanical Turk. www.id-book.com 13 Evaluation methods Method Controlled settings Natural settings Without users Observing x x Asking users x x Asking experts x x Testing x Modeling x www.id-book.com 14 The language of evaluation Analytics Analytical evaluation Biases Controlled experiment Crowdsourcing Ecological validity Expert review or crit Field study Formative evaluation Heuristic evaluation Informed consent form In the wild evaluation Living laboratory Predictive evaluation Reliability Scope Summative evaluation Usability laboratory User studies Usability testing Users or participants Validity www.id-book.com 15 Participants’ rights and getting their consent • Participants need to be told why the evaluation is being done, what they will be asked to do and informed about their rights. • Informed consent forms provide this information and act as a contract between participants and researchers. • The design of the informed consent form, the evaluation process, data analysis and data storage methods are typically approved by a high authority, eg. Institutional Review Board. www.id-book.com 16 Things to consider when interpreting data • Reliability: does the method produce the same results on separate occasions? • Validity: does the method measure what it is intended to measure? • Ecological validity: does the environment of the evaluation distort the results? • Biases: Are there biases that distort the results? • Scope: How generalizable are the results? www.id-book.com 17 Key points • Evaluation and design are very closely integrated. • Some of the same data gathering methods are used in evaluation as for establishing requirements and identifying users’ needs, e.g. observation, interviews, and questionnaires. • Evaluations can be done in controlled settings such as laboratories, less controlled field settings, or where users are not present. • Usability testing and experiments enable the evaluator to have a high level of control over what gets tested, whereas evaluators typically impose little or no control on participants in field studies. • Different methods can be combined to get different perspectives. • Participants need to be made aware of their rights. • It is important not to over-generalize findings from an evaluation. www.id-book.com 18","libVersion":"0.2.3","langs":""}