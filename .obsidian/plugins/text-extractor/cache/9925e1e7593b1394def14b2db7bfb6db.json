{"path":"UNISA/98906 - BSc Science in Computing/COS3751 - Techniques of Artificial Intelligence/Telegram Notes/Chapter3-2-heuristic-search.pdf","text":"1 Heuristic (Informed) Search (Where we try to be smarter in h h lt ti ) 1 how we choose among alternatives) R&N III: Chapter 3.5 R&N II: Chap. 4, Sect. 4.1–3 Search Algorithm 1. INSERT(initial-node,FRINGE) 2. Repeat: a. If empty(FRINGE) then return failure b. n  REMOVE(FRINGE) 2 c. s  STATE(n) d. If GOAL?(s) then return path or goal state e. For every state s’ in SUCCESSORS(s) i. Create a node n’ as a successor of n ii. INSERT(n’,FRINGE) Best-First Search  It exploits state description to estimate how promising each search node is  An evaluation function f maps each search node N to positive real number f(N) 3 p( )  Traditionally, the smaller f(N), the more promising N  Best-first search sorts the fringe in increasing f [random order is assumed among nodes with equal values of f] Best-First Search  It exploits state description to estimate how promising each search node is  An evaluation function f maps every search node N to positive real number “Best” only refers to the value of f 4 p f(N)  Usually, the smaller f(N), the more promising N  Best-first search sorts the fringe in increasing f [random order is assumed among nodes with equal values of f] Best only refers to the value of f, not to the quality of the actual path. Best-first search does not generate optimal paths in general  There are no limitations on f. Any function of your choice is acceptable. But will it help the search algorithm? How to construct an evaluation function? 5  The classical approach is to construct f(N) as an estimator of a solution path through N  The heuristic function h(N) estimates the distance of STATE(N) to a goal state Its value is independent of the current search tree; it depends only on STATE(N) Heuristic Function 6 py ( ) and the goal test  Example:  h1(N) = number of misplaced tiles = 6 14 7 5 2 63 8 STATE(N) 64 7 1 5 2 8 3 Goal state 2 Other Examples 14 7 5 2 63 8 STATE(N) 64 7 1 5 2 8 3 Goal state 7  h1(N) = number of misplaced tiles = 6  h2(N) = sum of the (Manhattan) distances of every tile to its goal position = 2 + 3 + 0 + 1 + 3 + 0 + 3 + 1 = 13  h3(N) = sum of permutation inversions = 4 + 0 + 3 + 1 + 0 + 1 + 0 + 0 = 9 Other Examples (Robot Navigation) yN N 8 xN xg yg 22 gg1N Nh (N) = (x -x ) +(y -y ) (Euclidean distance) h2(N) = |xN-xg| + |yN-yg| (Manhattan distance) 8-Puzzle 5 3 4 3 4 3 f(N) = h(N) = number of misplaced tiles 9 4 5 3 4 4 2 1 2 0 4 3 6 5 8-Puzzle f(N) = h(N) =  distances of tiles to goal 10 5 6 4 4 2 1 2 0 5 3 Can we Prove Anything?  If the state space is finite and we discard nodes that revisit states, the search is complete, but in general is not optimal If th st t s is fi it d d t 11  If the state space is finite and we do not discard nodes that revisit states, in general the search is not complete  If the state space is infinite, in general the search is not complete Best-First  Efficiency Local-minimum problem 12 f(N) = h(N) = straight distance to the goal 3 Classical Evaluation Functions  h(N): heuristic function [Independent of search tree]  g(N): cost of the best path found so far between the initial node and N 13 between the initial node and N [Dependent on search tree]  f(N) = h(N)  greedy best-first search  f(N) = g(N) + h(N) 1+5 3+3 3+4 2+3 8-Puzzle f(N) = g(N) + h(N) with h(N) = number of misplaced tiles 14 0+4 1+5 1+3 3+4 3+4 3+2 4+1 5+2 5+0 2+4 2+3 Algorithm A Search • Orders open list according to F(n) = G(n) + H(n) 15 • Must have search package keep a record of what the best path found so far is so that the G(n) is as accurate as possible. (go to handout on Algorithm A Search) Admissible Heuristic  Let h*(N) be the cost of the optimal path from N to a goal node  The heuristic function h(N) is admissible 16 if: 0  h(N)  h*(N)  An admissible heuristic function is always optimistic ! Admissible Heuristic  Let h*(N) be the cost of the optimal path from N to a goal node  The heuristic function h(N) is admissible 17 if: 0  h(N)  h*(N)  An admissible heuristic function is always optimistic ! G is a goal node  h(G) = 0 Algorithm A* • Algorithm A Search where we can prove that the heuristic function is admissible. Thi h i t d t fi d 18 • This search is guaranteed to find an optimal solution if a solution exists! 4  h (N) = number of misplaced tiles = 6 8-Puzzle Heuristics 14 7 5 2 63 8 STATE(N) 64 7 1 5 2 8 3 Goal state 19  h1(N) = number of misplaced tiles = 6 is admissible  h2(N) = sum of the (Manhattan) distances of every tile to its goal position = 2 + 3 + 0 + 1 + 3 + 0 + 3 + 1 = 13 is admissible  h3(N) = sum of permutation inversions = 4 + 0 + 3 + 1 + 0 + 1 + 0 + 0 = 9 is ??? [left as an exercise] Robot Navigation Heuristics 20 Cost of one horizontal/vertical step = 1 Cost of one diagonal step = 2 22 gg1N Nh (N) = (x -x ) +(y -y ) h2(N) = |xN-xg| + |yN-yg| are both admissible A* Search (most popular algorithm in AI)  f(N) = g(N) + h(N), where: • g(N) = cost of best path found so far to N • h(N) = admissible heuristic function 21  for all arcs: 0 <   c(N,N’)  “modified” search algorithm is used  Best-first search is called A* search Result #1 A* is complete and optimal [This result holds if nodes revisiting states are not discarded] 22 states are not discarded] Proof (1/2) 1) If a solution exists, A* terminates and returns a solution For each node N on the fringe, f(N)d(N), where d(N) is the depth of N in the tree 23 p As long as A* hasn’t terminated, a node K on the fringe lies on a solution path Since each node expansion increases the length of one path, K will eventually be selected for expansion Proof (2/2) 2) Whenever A* chooses to expands a goal node, the path to this node is optimal C*= h*(initial-node) G’: non optimal goal node in the fringe 24 G: non-optimal goal node in the fringe f(G’) = g(G’) + h(G’) = g(G’)  C* A node K in the fringe lies on an optimal path: f(K) = g(K) + h(K)  C* So, G’ is not be selected for expansion 5 Time Limit Issue  When a problem has no solution, A* runs for ever if the state space is infinite or states can be revisited an arbitrary number of times (the search tree can grow arbitrarily large). In other case, it may take a huge amount of time to terminate  So, in practice, A* must be given a time limit. If it has not found a solution within this limit, it stops. Th th i t k if th bl h 25 Then there is no way to know if the problem has no solution or A* needed more time to find it  In the past, when AI systems were “small” and solving a single search problem at a time, this was not too much of a concern. As AI systems become larger, they have to solve a multitude of search problems concurrently. Then, a question arises: What should be the time limit for each of them? More on this in the lecture on Motion Planning ... Time Limit Issue  When a problem has no solution, A* runs for ever if the state space is infinite or states can be revisited an arbitrary number of times (the search tree can grow arbitrarily large). In other case, it may take a huge amount of time to terminate  So, in practice, A* must be given a time limit. If it has not found a solution within this limit, it stops. Th th i t k if th bl h Hence, the usefulness of a simple test, like in the (2n-1)-puzzle, that determines if the goal is reachable 26 Then there is no way to know if the problem has no solution or A* needed more time to find it  In the past, when AI systems were “small” and solving a single search problem at a time, this was not too much of a concern. As AI systems become larger, they have to solve a multitude of search problems concurrently. Then, a question arises: What should be the time limit for each of them? More on this in future lectures ... Unfortunately, such a test rarely exists 88--PuzzlePuzzle 1+5 3+3 3+4 2+3 f(N) = g(N) + h(N) with h(N) = number of misplaced tiles 27 0+4 1+5 1+3 3+4 3+4 3+2 4+1 5+2 5+0 2+4 2+3 Robot Navigation 28 Robot Navigation 587 46233 546 f(N) = h(N), with h(N) = Manhattan distance to the goal (not A*) 29 0211 7 3 7 7 63 2 8 6 45 365 24435 5 6 4 5 Robot Navigation 587 46233 546 f(N) = h(N), with h(N) = Manhattan distance to the goal (not A*) 30 0211 7 3 7 7 63 2 8 6 45 365 24435 5 6 4 57 0 6 Robot Navigation f(N) = g(N)+h(N), with h(N) = Manhattan distance to goal (A*) 587 46233 5468+38+3 7+47+4 6+56+3 5+65+6 4+74+7 3+83+8 2+92+9 3+10 31 0211 7 3 7 7 63 2 8 6 45 365 24435 5 6 4 57+0 6+1 6+1 8+1 7+0 7+2 6+1 7+2 6+1 8+1 7+2 7+2 6+36+3 5+45+4 4+54+5 3+63+6 2+7 5+6 2+7 3+8 4+7 3+8 2+9 3+8 2+9 1+101+10 0+110+11 How to create an admissible h?  An admissible heuristic can usually be seen as the cost of an optimal solution to a relaxed problem (one obtained by removing constraints)  In robot navigation: 32 g • The Manhattan distance corresponds to removing the obstacles • The Euclidean distance corresponds to removing both the obstacles and the constraint that the robot moves on a grid  More on this topic later What to do with revisited states? c = 1 2 h = 100 1 The heuristic h is 33 100 2 1 0 90 clearly admissible What to do with revisited states? c = 1 2 h = 100 1 f = 1+100 2+1 34 100 2 1 0 90 104 4+90 ? If we discard this new node, then the search algorithm expands the goal node next and returns a non-optimal solution 1 2 100 1 1+100 2+1 What to do with revisited states? 35 100 2 1 0 90 104 4+902+90 102 Instead, if we do not discard nodes revisiting states, the search terminates with an optimal solution But ... If we do not discard nodes revisiting states, the size of the search tree can be exponential in the number of visited states 36 1 2 11 1 2 1 1 1+1 1+1 2+1 2+1 2+1 2+1 44 44 44 44 7  It is not harmful to discard a node revisiting a state if the new path to this state has higher cost than the previous one  A* remains optimal, but the size of the search tree can still be exponential in the 37 p worst case  Fortunately, for a large family of admissible heuristics – consistent heuristics – there is a much easier way of dealing with revisited states Consistent Heuristic A heuristic h is consistent if 1) for each node N and each child N’ of N: h(N)  c(N,N’) + h(N’) [Intuition: h gets more and more N c(N,N’) 38 [g precise as we get deeper in the search tree] 2) for each goal node G: h(G) = 0 The heuristic is also said to be monotone N’ h(N) h(N’) (triangle inequality) Consistency Violation N c(N,N’) If h tells that N is 100 units from the goal, then moving from N along an arc 10 39 N’ h(N) h(N’) (triangle inequality) costing 10 units should not lead to a node N’ that h estimates to be 10 units away from the goal  A consistent heuristic is also admissible  An admissible heuristic may not be nsist nt b t m n dmissibl h isti s Admissibility and Consistency 40 consistent, but many admissible heuristics are consistent 8-Puzzle 12 3 45 6 78 12 3 4 5 67 8 41 STATE(N) goal  h1(N) = number of misplaced tiles  h2(N) = sum of the (Manhattan) distances of every tile to its goal position are both consistent Robot navigation 42 Cost of one horizontal/vertical step = 1 Cost of one diagonal step = 2 22 gg1N Nh(N) = (x -x ) +(y -y ) h2(N) = |xN-xg| + |yN-yg| are both consistent 8 If h is consistent, then whenever A* expands a node, it has already found an optimal path to this node’s state Result #2 43 NN1 SS1 The path to N is the optimal path to S N2 N2 can be discarded Proof 1) Consider a node N and its child N’ Since h is consistent: h(N)  c(N,N’)+h(N’) f(N) = g(N)+h(N)  g(N)+c(N,N’)+h(N’) = f(N’) So, f is non-decreasing along any path 44 2) If K is selected for expansion, then any other node K’ in the fringe verifies f(K’) f(K) So, if one node K’ lies on another path to the state of K, the cost of this other path is no smaller than the path to K Revisited States with Consistent Heuristic  When a node is expanded, store its state into CLOSED  When a new node N is generated: 45 • If STATE(N) is in CLOSED, discard N • If there exits a node N’ in the fringe such that STATE(N’) = STATE(N), discard the node – N or N’ – with the largest f Worst-Case Complexity of A* when State Space is Finite (1/3)  Assume a state graph of n states and r arcs  Two cases: a) If the number of successors of any state is O(n), then r = O(n2); the state graph is dense 46 b) If it is O(1), then r = O(n); the graph is sparse [In most search problem, the graph is sparse]  CLOSED is implemented as a hash-table with O(1) access time  Heuristic h is consistent Worst-Case Complexity of A* when State Space is Finite (2/3)  The fringe is implemented as a list  linear scan to find best node  Number of attempted add-to-fringe operations: O(r) 47  Time to add a node to the fringe: O(1)  Number of node expansions: O(n)  Time to select a node from the fringe: O(n)  Total time complexity: O(r + n2) = O(n2)  Space complexity: O(n) [A node need not pointing to its children] Worst-Case Complexity of A* when State Space is Finite (3/3)  The fringe is implemented as a priority queue  Number of attempted add-to-fringe operations: O(r)  Time to add a node to the fringe O(log n) 48 Time to add a node to the fringe O(log n)  Number of node expansions O(n)  Time to select a node from the fringe O(log n)  Total time complexity: O(r log n + n log n) • If dense state graph: O(n2 log n) • If sparse state graph: O(n log n)  Space complexity: O(n) 9 Worst-Case Complexity of A* when State Space is Finite (3/3)  The fringe is implemented as a priority queue  Number of attempted add-to-fringe operations: O(r)  Time to add a node to the fringe O(log n) So, for large state spaces with reasonable bhi f t ( tt h ) 49 Time to add a node to the fringe O(log n)  Number of node expansions O(n)  Time to select a node from the fringe O(log n)  Total time complexity: O(r log n + n log n) • If dense state graph: O(n2 log n) • If sparse state graph: O(n log n)  Space complexity: O(n) branching factors ( sparse state graphs), it is preferable to implement the fringe as a priority queue Is A* with some consistent heuristic all what we need? No !  The previous result only says that A*’s worst- case time complexity is low-polynomial in the 50 case time complexity is low-polynomial in the size of the state space, but this size may be exponential in other parameters (e.g., path lengths) depending on the input description  The state space can even be infinite  There are very dumb consistent heuristics h  0  It is consistent (hence, admissible) !  A* with h0 is uniform-cost search Bdth fi t d if t 51  Breadth-first and uniform-cost are particular cases of A* Heuristic Accuracy Let h1 and h2 be two consistent heuristics such that for all nodes N: h1(N)  h2(N) h2 is said to be more accurate (or more informed) 52 2 ( f ) than h1  h1(N) = number of misplaced tiles  h2(N) = sum of distances of every tile to its goal position  h2 is more accurate than h1 14 7 5 2 63 8 STATE(N) 64 7 1 5 2 8 3 Goal state Result #3  Let h2 be more informed than h1  Let A1* be A* using h1 and A2* be A* using h2 Wh l ti i t ll th 53  Whenever a solution exists, all the nodes expanded by A2*, except possibly the goal node, are also expanded by A1* Proof  C* = h*(initial-node)  Every node N such that f(N)  C* is eventually expanded  Every node N such that h(N)  C*g(N) i ll d d 54 is eventually expanded  Since h1(N)  h2(N), every non-goal node expanded by A2* is also expanded by A1*  If f(N) = C*, N is a goal node; only one such node is expanded [it may not be the same one for A1* and A2*] 10 Effective Branching Factor  It is used as a measure the effectiveness of a heuristic  Let n be the total number of nodes generated by A* for a particular problem 55 generated by A* for a particular problem and d the depth of the solution  The effective branching factor b* is defined by n = 1 + b* + (b*)2 +...+ (b*)d Experimental Results (see R&N for details)  8-puzzle with:  h1 = number of misplaced tiles  h2 = sum of distances of tiles to their goal positions  Random generation of many problem instances  Average branching factors (number of expanded nodes): 56 gg (p ) dIDS A1*A2* 2 2.45 1.79 1.79 6 2.73 1.34 1.30 12 2.78 (3,644,035) 1.42 (227) 1.24 (73) 16 -- 1.45 1.25 20 -- 1.47 1.27 24 -- 1.48 (39,135) 1.26 (1,641)  By solving relaxed problems at each node  In the 8-puzzle, the sum of the distances of each tile to its goal position (h2) corresponds to solving 8 simple problems: How to create good heuristic? 5 8 1 2 3 57 It ignores negative interactions among tiles 14 7 5 2 63 8 64 7 1 5 2 8 3 5 5  For example, we could consider two more complex relaxed problems: Can we do better? 14 7 5 2 63 8 64 7 1 5 2 8 3 58   h = d1234 + d5678 [disjoint pattern heuristic]  These distances could have been precomputed in a database [left as an exercise] 3 2 14 4 1 2 3 6 7 5 87 5 6 8  For example, we could consider two more complex relaxed problems: Can we do better? 14 7 5 2 63 8 64 7 1 5 2 8 3 Several order-of-magnitude speedups have been obtained this way for the 59   h = d1234 + d5678  These distances could have been precomputed [left as an exercise] 3 2 14 4 1 2 3 6 7 5 87 5 6 8 have been obtained this way for the 15- and 24-puzzle (see R&N) On Completeness and Optimality  A* with a consistent heuristic has nice properties: completeness, optimality, no need to revisit states  Theoretical completeness does not mean “practical” completeness if you must wait too 60 pp y long to get a solution (remember the time limit issue)  So, if one can’t design an accurate consistent heuristic, it may be better to settle for a non-admissible heuristic that “works well in practice”, even completeness and optimality are no longer guaranteed 11 Iterative Deepening A* (IDA*)  Idea: Reduce memory requirement of A* by applying cutoff on values of f  Consistent heuristic h  Algorithm IDA*: 61  Algorithm IDA : 1. Initialize cutoff to f(initial-node) 2. Repeat: a. Perform depth-first search by expanding all nodes N such that f(N)  cutoff b. Reset cutoff to smallest value f of non- expanded (leaf) nodes 8-Puzzle f(N) = g(N) + h(N) with h(N) = number of misplaced tiles 62 4 6 Cutoff=4 8-Puzzle f(N) = g(N) + h(N) with h(N) = number of misplaced tiles 63 4 4 6 Cutoff=4 6 8-Puzzle f(N) = g(N) + h(N) with h(N) = number of misplaced tiles 64 4 4 6 Cutoff=4 6 5 8-Puzzle 5 f(N) = g(N) + h(N) with h(N) = number of misplaced tiles 65 4 4 6 Cutoff=4 6 5 8-Puzzle 56 f(N) = g(N) + h(N) with h(N) = number of misplaced tiles 66 4 4 6 Cutoff=4 6 5 12 8-Puzzle f(N) = g(N) + h(N) with h(N) = number of misplaced tiles 67 4 6 Cutoff=5 8-Puzzle f(N) = g(N) + h(N) with h(N) = number of misplaced tiles 68 4 4 6 Cutoff=5 6 8-Puzzle f(N) = g(N) + h(N) with h(N) = number of misplaced tiles 69 4 4 6 Cutoff=5 6 5 8-Puzzle f(N) = g(N) + h(N) with h(N) = number of misplaced tiles 70 4 4 6 Cutoff=5 6 5 7 8-Puzzle f(N) = g(N) + h(N) with h(N) = number of misplaced tiles 71 4 4 6 Cutoff=5 6 5 7 5 8-Puzzle f(N) = g(N) + h(N) with h(N) = number of misplaced tiles 72 4 4 6 Cutoff=5 6 5 7 5 5 13 8-Puzzle f(N) = g(N) + h(N) with h(N) = number of misplaced tiles 73 4 4 6 Cutoff=5 6 5 7 5 5 5 Advantages/Drawbacks of IDA*  Advantages: • Still complete and optimal • Requires less memory than A* • Avoid the overhead to sort the fringe 74 • Avoid the overhead to sort the fringe  Drawbacks: • Can’t avoid revisiting states not on the current path • Available memory is poorly used SMA* (Simplified Memory-bounded A*)  Works like A* until memory is full  Then SMA* drops the node in the fringe with the largest f value and “backs up” this value to its parent  When all children of a node N have been dropped, the smallest backed up value replaces f(N) 75 pp ( )  In this way, SMA* the root of an erased subtree remembers the best path in that subtree  SMA* will regenerate this subtree only if all other nodes in the fringe have greater f values  SMA* generates the best solution path that fits in memory  SMA* can’t completely avoid revisiting states, but it can do a better job at this that IDA* Search problems Blind search Heuristic search: 76 Heuristic search: best-first and A* Construction of heuristics Local searchVariants of A* When to Use Search Techniques? 1) The search space is small, and • No other technique is available, or • Develop a more efficient technique is not worth the effort 77 worth the effort 2) The search space is large, and • No other available technique is available, and • There exist “good” heuristics","libVersion":"0.2.3","langs":""}