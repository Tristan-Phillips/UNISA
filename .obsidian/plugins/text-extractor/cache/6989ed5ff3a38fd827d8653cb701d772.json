{"path":"UNISA/98906 - BSc Science in Computing/COS3751 - Techniques of Artificial Intelligence/Student Notes/2023assignment3tl103_2023.pdf","text":"BAR CODE Deﬁne Tomorrow. university of south africa Tutorial Letter 103/0/2023 Techniques of Artiﬁcial Intelligence COS3751 Year module Computer Science School of Computing CONTENTS Assignment 3 COS3751/103/0/2023 ASSIGNMENT 3 Due Date: 28 August 2023 Total Marks: 83 YEAR MODULE Study material: Chapters 9, and 18. You may skip sections 9.3, and 9.4. You only need to study 18.1, 18.2, and 18.3 Question 1: 16 Marks Convert the following First Order Logic (FOL) sentence to Conjunctive Normal Form (CNF): ∀x(∀y(¬P (y) ∨ Q(x, y)) ⇒ (¬∀yQ(y, x))) Question 2: 36 Marks Consider the following English statements: 1. Anyone who passes their AI exam and who wins the lottery is happy. 2. Anyone who studies hard or is lucky, passes their exams. 3. Jack did not study. 4. Jack is lucky. 5. Anyone who is lucky wins the lottery. (2.1) (8)Provide a vocabulary for the statements. (2.2) (7)Translate the above English sentences to FOL statements using the vocabulary you de- ﬁned above. (2.3) (8)Convert the FOL statements obtained in 2.2 into CNF. (2.4) (13)Use resolution refutation to prove that Jack is happy. Question 3: 10 Marks (3.1) (8)Convert the Boolean function in Table 1 to a decision tree: (3.2) (2)When we construct a decision tree without the beneﬁt of gain values, the order in which we evaluate the variables is important. Why? 2 COS3751/103/0/2023 A B C f(A, B, C) 0 0 0 0 0 0 1 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 1 1 1 1 0 1 1 1 1 0 Table 1: Boolean function table Question 4: 21 Marks Consider the following table which provides examples of weather conditions used to determine whether a old age pensioner is likely to play golf on a given day. Nr Outlook Temperature Windy Crowded Play? 1 Sunny Hot Yes No Yes 2 Sunny Hot No Yes No 3 Overcast Hot Yes Yes Yes 4 Overcast Hot No No Yes 5 Rain Mild Yes No Yes 6 Rain Cool No Yes No 7 Overcast Cool No Yes No 8 Sunny Mild Yes No Yes 9 Sunny Cool Yes No Yes 10 Rain Mild No No Yes 11 Sunny Mild No Yes Yes 12 Overcast Mild Yes Yes Yes 13 Overcast Hot No No Yes 14 Sunny Mild Yes Yes No 15 Rain Cool No Yes No 16 Rain Cool Yes No Yes (4.1) (14)Determine the root node of the decision tree for this table. Show all your calculations during the determination of the root node (all the entropy, remainder and information gain calculations). Use the table on page 4 for your entropy values and calculations. (4.2) (5)Explain brieﬂy what would be done next to determine the complete decision tree. (4.3) (2)Explain the signiﬁcance of any of the listed attributes becoming the root node of the decision tree. 3 Entropy Table (Boolean valued variables) p : Ratio of positive examples. E : Corresponding entropy (−(plog2p + (1 − p)log2(1 − p))). p E 0.00 0.00 0.10 0.47 0.15 0.61 0.20 0.72 0.25 0.81 0.30 0.88 0.35 0.93 0.40 0.97 0.45 0.99 0.50 1.00 0.55 0.99 0.60 0.97 0.65 0.93 0.70 0.88 0.75 0.81 0.80 0.72 0.85 0.61 0.90 0.47 0.95 0.29 1.00 0.00 Example: For a set of 4 positive examples, and 1 negative example (written E[4, 1]), the ratio p = 4 5, or 0.80. The corresponding entropy value is given by the table as 0.72. Always round the ratio to the nearest value on the table, for example: For E[2, 1], the ratio is p = 2 3 ≃ 0.67 ≃ 0.65, which makes the E[2, 1] = 0.93. Copyright ©UNISA 2023 4","libVersion":"0.2.3","langs":""}