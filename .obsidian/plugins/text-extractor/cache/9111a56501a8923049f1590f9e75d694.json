{"path":"Subjects/COS3751 - Techniques of Artificial Intelligence/Telegram Notes/Materials/COS3751_2021_201_ALL_B.pdf","text":"BAR CODE Deﬁne Tomorrow. university of south africa Tutorial Letter 201/1/2021 Techniques of Artiﬁcial Intelligence COS3751 Semester 1 School of Computing This tutorial letter contains assignment 01 COS3751/201/1/2021 ASSIGNMENT 1 Solution Total Marks: 150 Unique Assignment Number: 848630/772589 Study material: Chapters 1 through 4. You may skip sections 4.2 and 4.5. Important: When we use the phrase ‘deﬁne’ (particularly in Question 2), we are looking for a formal deﬁnition using some form of formal notation, and not simply an English description or deﬁnition. For example: ‘Deﬁne the initial state for an agent in Johannesburg’. Answer: In(Johannesburg). ‘Deﬁne the actions available to this agent given that the agent simply moves between major metropolitan areas’. Answer: Actions(In(Johannesburg)) = {Go(Bloemontein), Go(Durban), . . . }. When we want an English deﬁnition we will explicitly ask for it. Question 1: 13 Marks (1.1) (6)Explain the difference between a single and multi-agent environment. An agent solving a problem by itself is a single agent environment✓. The key dis- tinction is if an agent X’s behavior is best described as maximizing a performance measure whose value depends on agent Y’s behavior✓✓. For example in a chess game the opponent agent X is trying to maximize its performance measure, which by the rules of chess minimises agent Y’s performance measure✓. Thus chess is a com- petitive multi agent environment. In multiagent environments communication emerges as a rational behavior✓while non-existent in single agent environments✓. Pg 42-43 (1.2) (4)Explain the difference between a Deterministic and Stochastic environment. In a deterministic environment the next state is completely ✓determined by the current state and the agent’s action ✓. In a stochastic environment one cannot completely ✓determine the next state based solely on the current environment and on the agent’s actions ✓. (1.3) (3)Consider a game of chess. Is this a fully observable, partially observable, or unob- servable environment? Clearly explain your answer. Fully observable ✓. The entire state✓can be observed at each distinct state in the state space✓. Question 2: 22 Marks (2.1) (5)List the 5 components that can be used to deﬁne a problem. 1. Initial state✓ 2. Actions✓ 3. Transition model✓ 2 COS3751/201/1/2021 4. Goal test✓ 5. Path cost✓ (2.2) (2)Differentiate between search space and goal space. The search space is the set of states that have to be searched for a solution✓, whereas a goal space is a set of goal states✓. (2.3) (1)What is the purpose of the explored set? Avoids inﬁnite loops since it holds the list of nodes that have already been explored. ✓ (2.4) (6)List and discuss three types of queues that may be employed in a search. 1. FIFO✓: used in DFS searches, nodes are added in reverse order to ensure that the last node added will be the ﬁrst node to be explored.✓ 2. LIFO✓: Typically used in BFS searches: nodes are added in the order they are generated.✓ 3. Priority queue✓: Nodes are added and sorted based on some key, this ensures that certain states take priority over others during the expansion phase.✓ (2.5) (8)List and explain the measures used to determine problem solving performance. 1. Completeness✓: Will the algorithm ﬁnd a solution if it exists? ✓ 2. Optimality✓: Will the algorithm ﬁnd the best solution (optimal path cost among all solutions)? ✓ 3. Time complexity✓: How long does the algorithm take to ﬁnd a solution? ✓ 4. Space complexity✓: How much memory is needed to perform the search for a solution? ✓ 3 Question 3: 23 Marks Three (3) hikers (Andile, Barbara, and Ch ´e) have just descended down a valley to ﬁnd themselves confronted by a river they cannot get across. After walking downstream for a while they ﬁnd two young boys with a boat and ask them if they would help them get across the river. The boys agree, but inform the hikers that since their boat is so small, it can only hold only the two boys or one of the hikers at a time. We can assume that everyone knows how to row the boat. (3.1) (7)Deﬁne a state using a mathematical notation (pictures or any form of graphical notation will not be accepted). Discuss the appropriateness of your choice, and provide an example to show that it will be suitable to be employed during a search. We can abbreviate the hikers with the labels A, B, and C. The boys we could label b1 and b2. There are many possible answers. We need not distinguish between the boys as it would not make a difference to the solution, however, set theory dictates that sets do not contain duplicates, thus we need to properly indicate that the boys are two unique entities. State {A, B, C, b1, b2}{}(✓for hikers, ✓for boys – ✓if boys are unique) represents the two sides✓of the river and the people on that side✓. We can use a more compact representation {A, B, C, b1, b2}✓, since if a person is not on the one side he/she will be on the other side✓. (3.2) (6)Deﬁne the start and goal states using your representation. Start state: {A, B, C, b1, b2}✓2 There are 3 possible goal states! (I show the state representing the left (beginning) side of the river – the complement is also correct) 1. {} ✓2 2. {bi} with i ∈ {1, 2} ✓ 3. {b1, b2} ✓ (3.3) (2)Deﬁne an appropriate cost function or functions for this problem. Cost function: A cost function is not relevant here✓, since it would not impact which solution would be used✓(if more than one exists). (3.4) (8)Provide a formal deﬁnition of a valid action function for this problem – you need not provide a formal deﬁnition for the operation of the function. Discuss the operation of the function and provide an example to illustrate its use. Successor: move 1 hiker✓, 1 boy✓, or 2 boys ✓to the other side. Mathematically we either remove one or two of the existing elements from our state representation when we move these people to the other side, or we add one or two missing elements to the set when we move these people back to this side✓. One such function could be: move(X , Y )✓with X , Y ∈ {A, B, C, b1, b2, ✓∅✓}, X ̸= Y ✓, where the function would alternately remove or add the function arguments from/to the state set (in the compact representation). 4 COS3751/201/1/2021 Question 4: 12 Marks (4.1) (3)Highlight the differences between a tree and graph search. Remember that a tree is a simple-connected acyclic graph. (That means that there are no loops, and that by deﬁnition there must be nodes that have no children.) A graph, on the other hand may have cycles or loops. The main difference between a tree search is thus that we don’t need to keep track of already explored nodes, since a simple tree cannot have revisited states. Thus, the tree search simple selects a leaf node from the frontier, goal tests, and if it is a goal it returns the path (solution) to that node.✓ Applying a tree search to a graph creates problems since there may be redundant paths and loops. The graph search solves this problem by augmenting the tree search with a ’closed list’ (or explored list). When nodes are generated during the search that are already on the closed list they are not added to the frontier.✓✓ (4.2) (5)How does a Breadth First Search (BFS) differ from the general tree seach algorithm? What is the major reason for implementing the algorithm in this way? Provide an example to aid your discussion. BFS differs from a general tree search most notably during the goal test phase. In a general tree search the goal test phase happens on expansion. Since a BFS has de- graded performace in large search spaces, the goal test takes place during generation. Any suitable example to illustrate this concept is considered. ✓5 (4.3) (4)Consider an example of a sliding-block puzzle game state provided below (Figure 1). How many distinct states are there for puzzles of this sort? How many search nodes? Explain how you reached your answer. A B E I F H C N G J L M O K D Figure 1: Sliding-block puzzle It is interesting to note that half of random start states for the 15 puzzle are not solvable. For the 15-puzzle, we will always have 16! distinct states. However, since only half of the random start states are solvable, we have a possible search space of 16!/2.✓4 Why use 16!? If we start with an empty board we can choose to place any of the 16 tokens (numbers 1 to 15 and space) in the top-left position, from here on we can choose any one of the remaining 15 tokens for the position just to the right of the top-left, and so we carry on: 16 × 15 × 14 × ... × 2 × 1 = 16!. 5 If you’re concerned about the number of distinct states for the example provided above, you will have to consider the number of moves that can be used to solve the puzzle – but that would only yield a possible lower-bound (assuming our search is super- efﬁcient) on the search space. From a space-complexity viewpoint, it will sufﬁce to state that the upper bound of the distinct number of states is 16!. Question 5: 6 Marks Consider the search tree in Figure 2. O M D K I C J H E L F G B N A Figure 2: Search Tree (Iterative Deepening Search (IDS)) (5.1) (6)Show the order in which the nodes will be expanded (from limit 0 to limit 4) given that IDS is used. Assume the goal node is F , and that nodes are expanded from left to right. So expansion means we apply legal actions to a chosen node – this, by deﬁnition, means that our order look somewhat different from what one may expect. Also, note that the IDS is a repeated invocation of the depth limited search. In the depth limited search, once we generate a node, we recursively call the depth limited search on that node. Thus, goal state checking happens immediately after generation for each node. In general, the children of nodes that are expanded are thus goal checked. 1. Limit 0: (No expansion – O is just goal tested) ✓ 2. Limit 1: O ✓(M and E are just goal tested, not expanded) 3. Limit 2: O M E ✓ 4. Limit 3: O M D C E L✓2 5. Limit 4: None, search terminates once L is expanded – once L is expanded, F will be generated and goal tested on the recursive call. ✓ 6 COS3751/201/1/2021 Visually, this is what happens (squares are goal tests and circles are expansions). Limit 0: O Limit 1: O M E Limit 2: O M D C E L B Limit 3: O M D K I C J H E L F Question 6: 16 Marks Consider the graph in Figure 3. (6.1) (4)Explain the concept of Uniform Cost Search (UCS). Provide an example to aid your discussion Uniform cost search is an uninformed search – thus no heuristics!. It calculates the cost of moving from this state to the next (keeping the cost of reaching the current state in mind), and chooses the cheapest (as deﬁned in the problem) next state from the frontier✓✓. It helps to think of it as explained in the textbook: it uses an evaluation function (similar 7 A B C D E F G H 11 20 15 4 24 22 14 12 9 Figure 3: Search Graph (Uniform Cost Search (UCS)) to an informed search such as A ∗): ˆf (n) = ˆg(n) or just ˆg(n). example✓✓ (6.2) (12)Suppose we start at C and the goal is G. Write down the nodes in the order they are expanded. Show your steps and calculations. The step cost between nodes is provided next to the edges in the graph. Since we’ve been talking about uniform cost, we’ll use the UCS algorithm. 1. C is placed on the frontier. (Technically we would put (C,0) on the frontier – see the next step for more detail) Frontier is: (C,0)✓ 2. Choose C (and remove) for expansion: generate the path (C,F, 4), (C,A, 20), and (C,E, 24) (where (C,A,20) means that there is a path from C to A which costs 20), and place on frontier. Frontier is: (C,F,4), (C,A,20), (C,E,24) ✓ 3. Choose (and remove) (C,F,4) (lowest cost, but not goal) – generate (C,F,H,13) and place on the frontier. Frontier is: (C,F,H,13), (C,A,20), (C,E,24) ✓ 8 COS3751/201/1/2021 4. Choose (and remove) (C,F,H,13) (lowest cost, but not goal) – generate (C,F,H,G,25) and place on the frontier. Frontier is: (C,A,20), (C,E,24), (C,F,H,G,25)✓ 5. Choose (and remove) (C,A,20) (lowest cost, but not goal) – generate (C,A,B,31) and place on the frontier. Frontier is: (C,E,24), (C,F,H,G,25),(C,A,B,31)✓ 6. Choose (and remove) (C,E,24) (lowest cost, but not goal) – generate (C,E,D,46) and place on the frontier. Frontier is: (C,F,H,G,25), (C,A,B,31), (C,E,D,46)✓ 7. Choose (and remove) (C,F,H,G,25) (lower cost, and goal) – search terminates.✓ Taking the nodes from the map as they are expanded, you thus have C✓,F✓,H✓,A✓,E✓. Question 7: 4 Marks (7.1) (4)Differentiate between an admissible and a consistent heuristic. An admissible heuristic does not overestimate the actual cost of getting from a node in the search space to the goal✓2. A consistent heuristic is more strict than an admissi- ble one: it obeys the triangle inequality: it is not possible for one side of a triangle to be longer than the sum of the lengths of the other two sides – thus, the heuristic should not estimate a higher cost from a node to a goal than a shorter actual path to the goal from that node.✓2 Question 8: 31 Marks Consider Figure 4 and Table 1 and answer the questions that follow. Table 1 provides the estimated distances from each node to H (thus ˆh for each node). The label next to each edge provides the cost between nodes (thus ˆg). Node Estimated distance to N A 10 B 12 C 4 D 5 E 4 F 7 G 4 H 5 I 4 K 2 L 1 M 1 N 0 Table 1: Distance Table 9 A B C D F G H I L K M E N 7 8 3 10 10 5 8 5 7 6 4 4 3 3 2 5 10 1 Figure 4: A ∗ Search (8.1) (10)Supose the start node is A, and the goal node is N. List the nodes in the order in which they are expanded (not generated). Show your steps (don’t just list the nodes, explain the reason for each choice). Lets start by adding A to the frontier so that the search can kick-off. In actual fact we add A with some additional information so that the search can discriminate between good choices during the search. We’ll add < P n s , cn s >, where P n s is a path than can be explored (from the start node S to node n), and cn s is the cost of getting to n added to the estimate of getting to G (the goal). But we know that for A∗, cn s is ˆf (n) = ˆh(G) + ˆg(n) So, we choose the node in the frontier with the lowest ˆf : < A, (10) >, and we expand it. Expanded thus far: A. Current frontier (ﬁrst item is always chosen next): 1. << A, D >, (8) > 2. << A, C >, (12) > 3. << A, B >, (19) > Now we choose, again, the node from the frontier with the lowest ˆf value: << A, D > , (8) >. This means we have to expand D next. Expanded thus far:A,D So we expand D, and the frontier then looks thus: 10 COS3751/201/1/2021 1. << A, D, K >, (9) > 2. << A, D, L >, (10) > 3. << A, C >, (12) > 4. << A, D, I >, (14) > 5. << A, B >, (19) > The search node with the lowest ˆf is the one with the path through K , so we choose that, and thus we expand K . Expanded thus far:A,D,K Frontier: 1. << A, D, L >, (10) > 2. << A, D, K , M >, (10) 3. << A, C >, (12) > 4. << A, D, I >, (14) > 5. << A, D, K , E >, (16) 6. << A, B >, (19) > Notice that I’ve placed the path to M below the path to L even though they have the same ˆf values. I did this as an internal conﬂict protocol (internal to the algorithm). As long as I am consistent in applying this conﬂict resolution (and it is not grossly wrong) everything should turn out ﬁne. In this case the rule is simple: paths with fewer nodes have priority. So we choose the path through L, and thus expand L. Expanded thus far: A,D,K,L. Frontier: 1. << A, D, K , M >, (10) 2. << A, C >, (12) > 3. << A, D, L, N >, (12) > 4. << A, D, I >, (14) > 5. << A, D, K , E >, (16) 6. << A, B >, (19) > Although we’ve found a path to N, the algorithm ﬁrst tries to ﬁnd a shorter route: it doesn’t exit once it ﬁnds a path to N. Now we get to choose the path through M. Expanded thus far: A,D,K,L,M Frontier: 1. << A, D, K , M, N >, (10) 2. << A, C >, (12) > 11 3. << A, D, I >, (14) > 4. << A, D, K , E >, (16) 5. << A, B >, (19) > Notice that we found a shorter route to N: we thus remove the old route << A, D, L, N > , (12) > from the frontier. Choose the node on the frontier with the lowest ˆf value. In this case it is actually our goal path. However, we have started the process of expansion, although we won’t apply actions to the selected node. So our ﬁnal answer is: A✓2, D✓2, K✓2, L✓2, M✓2 (8.2) (4)When adding nodes to the frontier (during the search), is it enough to simply add nodes from the state space to the frontier? Justify your answer. Simply adding the nodes in the state space may not convey the needed information to solve the problem. It is not always the case, if the search space naturally encodes the solution (or an intermediate solution) then the nodes from the state space should should sufﬁce. For example: if the nodes in the search space for the previous question are simply nodes in the state-space, then the path to follow from start to ﬁnish is not properly encoded in the solution to the problem. We need to have the path to follow (or the route to travel) in order to be able to solve the problem. Thus it is often better to encode partial solutions in the nodes in the search space: in this way they become part of the solution to the problem, and will be used as they are in the frontier.✓✓✓✓ (8.3) (10)Provide the content of the frontier at the time the search terminates. Frontier: 1. << A, D, K , M, N >, (10)✓2 2. << A, C >, (12) >✓2 3. << A, D, I >, (14) >✓2 4. << A, D, K , E >, (16)✓2 5. << A, B >, (19) >✓2 12 COS3751/201/1/2021 (8.4) (7)Is the heuristic being applied consistent? Justify your answer by providing proof from the graph. No✓2. We can easily show that the heuristic is not consistent by providing a single case in which it does not satisfy the triangle inequality✓2. ˆh(A) > ˆg(D) + ˆh(D)✓3 (it is somehow cheaper to reach the goal by moving to D from A and then moving onto the goal than it is estimated in reaching it from A), in other words, the one side of the triangle formed AN (cost = 10), is longer than the sum of the sides AD + DN (cost = 8), which violates the inequality. A D N 5 3 10 Question 9: 23 Marks A magic square is a square of n × n cells, and which has some very interesting properties: all the rows, columns, and diagonals, when summed, adds up to the magic constant. The magic constant is calculated as follows: M = n(n2 + 1) 2 Thus, for n = 3, M = 3(32+1) 2 = 15. The entries in the square are limited to the integers between 1 and n2. For example, a solution for n = 3 is: 4 3 8 9 5 1 2 7 6 Figure 5: Magic Square for n = 3 It is easy to verify that every row, column, and diagonal in the above example adds up to 15. (9.1) (4)Differentiate between a global and local maxima/minima. Local minima/maxima are local solutions in the search space that are optimal solutions to the problem✓2. A global maxima/minima is a solution to the problem that is the best solution in the solution space (no other solution beats it).✓2 13 (9.2) (4)One way of solving a magic square is to randomly populate the entries, and then swap values until a solution is found. Deﬁne an appropriate objective function for this approach. Explain its function and provide and example to illustrate its use. A simple objective function is simply the number of rows, columns, and diagonals that sum to 15. The idea is then to favour moves that have higher values for the objective function – for example if we have a square with an objective function value of 4, chances are that we will only need to make minor adjustments to get to 5,6,7, and then 8. We can deﬁne some auxiliary functions to help deﬁne our objective function: r (x) which returns 1 if row x is 15, c(y) which returns 1 if row y sums to 15, and d and d ′ for each diagonal. r (x) = { 1 if ∑3 j=1 Mxj = 15 0 otherwise c(y) = { 1 if ∑3 k=1 Mky = 15 0 otherwise d = { 1 if ∑3 j=1 Mjj = 15 0 otherwise d ′ = { 1 if ∑3 k=1 Mk(4−k) = 15 0 otherwise Finally we deﬁne our objective function: o(M) = 3∑ j=1 r (j) + 3∑ k=1 c(k) + d + d ′✓4 (1) Another possible solution is to get the absolute value of 15 minus the sum of a row/column/diagonal, and add these together. The objective would then be to min- imise the objective function. r (x) =| 15 − 3∑ j=1 Mxj | c(y) =| 15 − 3∑ k=1 Mky | d =| 15 − 3∑ j=1 Mjj | d ′ =| 15 − 3∑ k=1 Mk (4−k) | Finally we deﬁne our objective function: 14 COS3751/201/1/2021 o(M) = 3∑ j=1 r (j) + 3∑ k=1 c(k) + d + d ′ (2) (9.3) (5)Differentiate between a standard hill-climb search and a simulated annealing search. Would simulated annealing ﬁt the approach in the previous question? Explain your answer. (Hint: think about what simulated annealing is supposed to do with respect to local/global maxima, also think about shoulders and plateaux.) A standard hill-climb attempts to ﬁnd a maximum/minimum by ’climbing/descending the hill’ (moving in the direction of a bigger/smaller objective function result)✓2. It may get stuck at a local max/min. Simulated annealing has some features that try to ’shake things up’ (amongst others it also employs the concept of hardening, or annealing) – in order to avoid getting stuck on plateaux✓2. Simulated annealing works quite well in this case: a random board is chosen at the start, and a random successor is generated, if it is better (using the objective function) it is always accepted, if it is not better, it is accepted with probability e∆E/T : this means the longer we search, the less happy we are to accept ’poor’ moves (this allows us to quickly get out of local minima/maxima at the start).✓. On a side note: remember that SA is supposed to give you a good chance of getting out of local minima/maxima: it does not always succeed. The magic square problem can quite easily be solved using a brute force search. (9.4) (10)Using the approach suggested above, show the ﬁrst 5 states in the search as well as the value of your objective function for each state. (Start off with a randomly populated magic square, and show 4 subsequent “moves”. In each case, show the value the objective function would take for each “move”.) . 9 2 6 3 5 7 1 8 4 Our objective function (the ﬁrst one in the sample solution) is 2. This is our random start, so we perform a random swap of the values on the board. Let’s assume a temperature T = 100.0, and we’ll use a simple linear discounting factor of 0.9 at each interval.✓2 (This doesn’t mean T is linear.) 9 2 4 3 5 7 1 8 6 Here our objective function for the board is 4. This is better than the previous situation (∆E = 2), so we will deﬁnitely use this going forward. T was discounted to 90.0 at the start of the loop.✓2 9 4 2 3 5 7 1 8 6 15 T is discounted to 81.0 Our objective function evaluates to 3, so we have ∆E = −1. At this point we use thus choose this board with probability e∆E/T = e(−1/81.0) = 0.988 (this is an extremely high probability, so for our purposes let’s say we accept the board✓2. Think about it like this: suppose we create 100 cards, each one with a unique number from 1 to 100. We then colour 99 of them red, and one of them white. The cards are placed in a hat, and we randomly draw one: if the card is red, we accept the move, if it is white, we don’t. 3 4 2 9 5 7 1 8 6 T is discounted to 72.9. Our objective function evaluates to 2. This is even worse than before (but we let the annealing process worry about that). ∆E = −1, and we accept the new board with probability e∆E/T = e(−1/72.9) = 0.986, also very high, and (again for illustrative purposes) we accept.✓2 The penultimate state provided here does not follow from the one above, but it is done with a view on the very last state given (particularly to show how the temperature affects the acceptance of boards). Assume we arrived at the following during the search 3 4 8 9 5 1 2 6 7 (The objective function for this board evaluates to 6.)✓2 Suppose now the following random board is: 3 4 8 9 5 1 2 6 7 And that T = 1.4 (after about 40 iterations). The objective function for this board evaluates to 5. That means ∆E = −1, and now we accept this board with probability e−1/1.4 = 0.508. This is basically a coin ﬂip, and from here on things don’t get better for these ’worse’ cases. Copyright ©UNISA 2021 16","libVersion":"0.2.3","langs":""}