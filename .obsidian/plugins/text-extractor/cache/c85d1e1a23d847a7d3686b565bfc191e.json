{"path":"UNISA/98906 - BSc Science in Computing/COS3712 - Computer Graphics/Unsorted/COS3712/Exam pack/Exam pack/Past Exams/cos3712_mj_15memo-Copy.pdf","text":"1 COS3712 May/Jun 2015 [TURN OVER] COS3712 May/June 2015 COMPUTER SCIENCE COMPUTER GRAPHICS Duration: 2 hours Total: 70 marks Examiners: First: Mr L Aron Second: Mr C Dongmo External: Mr JCW Kroeze ........................................................................................................... ........................................................................................................... MEMORANDUM 2 COS3712 May/June 2015 [TURN OVER] QUESTION 1 [12] 1.1 A real-time graphics program can use a single frame buffer for rendering polygons,clearing the buffer, and repeating the process. Why do we usually use two buffers instead? (3) Double buffering is a technique for tricking the eye into seeing smooth animation of rendered scenes. One buffer (the front buffer) is displayed on the screen while the other (the back buffer) is currently being drawn to. ✓During the vertical retrace period the buffers are swapped. Double buffering is needed because of shearing or flickering that occurs when we draw to the colour buffer that is currently being displayed. ✓The front buffer is displayed while the application renders into the back buffer. When the application completes rendering to the back buffer, it requests the graphics display hardware to swap the roles of the buffers, causing the back buffer to now be displayed, and the previous front buffer to become the new back buffer. ✓ 1.2 OpenGL uses a pipeline model to process vertices during rendering. In a typical OpenGL application a vertex will go through a sequence of 6 transformations or change of frames. In each frame the vertex has different coordinates. Name these coordinates and explain the process from one coordinate system to the next. (6) When we model an object, this is usually done in its own reference frame. The object coordinate system. ✓We want to place our object in the world, thus we switch to the world coordinate system. This is done by standard affine transformations✓When we look at our world, we prefere a frame where the observer is in the origin, the view coordinate system. Switching to this one is done by the view transformation. ✓We wish to clip away as much as possible of the world as fast as possible. Clipping is simpler in some standardized clip coordinate system, ✓ so we distort our view volume into e.g. a chopped pyramid, or a cube. This is usually called the clip coordinate system. If the clip coordinate system is not a standard cube with side two, we usually wish to transform our view volume inte that shape. So that we get Normaliced Device Coordinates. ✓This is called projection normalisation, and is sometimes combined with the clip distortion. After the we may do orthographics projection down into 2D. And we end up in a window coordinate system. The last step is to move out on the screen, the view port. So we make a view port transformation to get to the screen coordinate system. ✓ 3 COS3712 May/Jun 2015 [TURN OVER] 1.3 Since clipping to a clip region that is a cube is so easy, graphics systems tranform any scene with its clip window to make the clip window a cube. Name this transformation technique. (1) view normalization. ✓ 1.4 In the graphics pipeline, when a triangle is processed, the (x,y,z) coordinates of the vertices are interpolated across the whole triangle to give the coordinates of each fragment. Name two other things that may commonly be specified at the vertices and then interpolated across the triangle to give a value for each fragment. (2) normals, colors. ✓✓ QUESTION 2 :Transformations and Viewing [12] 2.1 Transformations are often carried out using a homogeneous co-ordinate representation. Give 3 reasons as to why this representation is used? (3) Any 3 reasons When points and vectors are represented using 3-dimensional column matrices one cannot distinguish between a point and a vector, with homogeneous coordinates we can make this distinction. ✓ A matrix multiplication in 3-dimensions cannot represent a change in frames, while this can be done using homogeneous coordinates. ✓ All affine transformations can be represented as matrix multiplications in homogeneous coordinates. ✓ Less arithmetic work is involved when using homogeneous coordinates. ✓ The uniform representation of all affine transformations makes carrying out successive transformations far easier than in 3 dimensional space. ✓ Modern hardware implements homogeneous coordinates operations directly, using parallelism to achieve high speed calculations. ✓ they allow us to express perspective projection as a 4 x 4 projection matrix✓ 4 COS3712 May/June 2015 [TURN OVER] 2.2 Differentiate between Orthographic and perspective projection. (4) In a perspective projection, lines (called projectors) are drawn from the objects to a point called the centre of projection(COP). The projection of the objects is where these lines intersect the projection plane.✔✔ In an orthographic projection, the projectors do not converge to a point but are parallel to one another, in a particular direction - the so called direction of projection. In this case, the COP is assumed to be at an infinite distance. As with a perspective projection, the projection of the objects is where the projectors intersect the projection plane.✔✔ 2.3 A synthetic camera co-ordinate reference frame is given by a view reference point (VRP) a view plane normal (VPN) and a view up vector (VUP). 2.3.1 Draw a diagram to show how these quantities describe the location and orientation of the synthetic camera. (2) 2.3.2 The view up vector is normally resolved to be orthogonal to the view plane normal. Why? (2) Use of the projection allows us to specify any vector not parallel to v, rather than being forced to compute a vector lying in the projection plane. ✔ We use the cross product of VUP and VPN to obtain VUP. ✔ 2.3.3 Consider a camera located at point e, specified in the object frame, and is pointed at a second point a. Express VPN in terms of these two points. (1) vpn = a - e 5 COS3712 May/Jun 2015 [TURN OVER] QUESTION 3 : Hidden surface removal [12] 3.1 Consider the z-buffer algorithm used by OpenGL for hidden surface removal 3.1 Briefly describe the z-buffer algorithm. (3) rasterization is done polygon by polygon✓. For each fragment on the polygon corresponding to the intersection of the polygon with a ray (from the centre of projection) through a pixel we compute the depth from the COP. If depth is greater than depth currently stored in z-buffe✓r it is ignored else z-buffer is updated and colour buffer is updated with new colour for fragment✓. 3.2 Does the z-buffer algorithm operate in object space or image space? (1) Image space✔ 3.3 In the z-buffer algorithm, is shading performed before or after hidden surfaces are eliminated? Explain why. (2) 3.4 Why can’t the standard z-buffer algorithm handle scenes with both opaque and transluc ent objects? Wh a t modifications can be made to the z-buffer algorithm for it to handle this? (3) If all polygons are rendered with the standard z-buffer algorithm, compositing will not be performed correctly, particularly if a translucent polygon is rendered first, and an opaque behind it is rendered later. ✔ However, if we make the z-buffer read-only when rendering translucent polygons, we can prevent the depth information from being updated when rendering translucent objects. ✔In other words, if the depth information allows a pixel to be rendered, it is blended (composited) with the pixel already stored there. If the pixel is part of an opaque polygon, the depth data is updated, but if it is a translucent pixel, the depth data is not updated. ✔ Shading is performed before hidden surface removal. ✔ In the z-buffer algorithm polygons are first rasterized and then for each fragment of the polygon depth values are determined and compared to the z-buffer. ✔ 6 COS3712 May/June 2015 [TURN OVER] 3.2 Briefly describe the algorithm for removing (or “culling\") backfacing polygons. Assume that the normal points out from the visible side of the polygon. (3) v θ n ✓ If θ is the angle between the normal and the viewer ✓then the polygon is facing forward iff -90≤ θ≤90 or cos θ ≥ 0, using dot product n.v ≥ 0✓ QUESTION 4: Lighting and Shading [10] A polygonal mesh comprises many flat polygons, each of which has a well-defined normal. Name and describe three different ways to shade these polygons. • Flat shading (or constant shading): The shading calculation is carried out only once for each polygon, and each point on the polygon is assigned the same shade. Flat shading will show differences in shading among adjacent polygons. We will see stripes, known as Mach bands, along the edges. • Gouraud shading (or smooth shading): The lighting calculation is done at each vertex using the material properties and the vectors , , and . Thus, each vertex will have its own colour that the rasterizer can use to interpolate a shade for each fragment. We define the normal at a vertex to be the normalized average of the normals of the polygons that share the vertex. We implement Gouraud shading either in the application or in the vertex shader. • Phong shading: Instead of interpolating vertex intensities (colours), we interpolate normals across each polygon. We can thus make an independent lighting calculation for each fragment. We implement Phong shading in the fragment shader. QUESTION 5 : Discrete Techniques [8] A fairly simple and cheap (computationally) way of obtaining realistic-looking images is to use texture maps. 7 COS3712 May/Jun 2015 [TURN OVER] 5.1 What are texture maps? (2) Texture mapping uses a pattern (or texture) to determine the color of a fragment. ✓These patterns could be determined by a fixed pattern, such as the regular patterns often used to draw polygons; by a procedural texture-generation method; or through a digitized image. In all cases, we can characterize the image produced by a mapping of a texture to the surface . ✓ 5.2 In OpenGL, what steps are needed in order to apply texture on a polygon.? (3) First the a texture image is formed and placed in texture memory, ✓ then texture coordinates are assigned to each fragment, ✓ finally the texture is applied to each fragment. ✓ 5.3 Describe one difficulty facing the implementer of a graphics package when the map is to be applied on an image? (1) One of the difficulties is that it is not always possible to find mapping functions to complete the mapping from texel to object coordinates. ✓ 5.4 Describe what I might do if I wanted to make my object bumpy, but not change the actual object. (2) Like the texture map that maps a pattern (of colours) to a surface, we can create a mapping that alters the normals in the polygon so the shading model can create the effect of a bumpy surface. This is called a bump map, QUESTION 6 [8] Bresenham derived a line-rasterization algorithm that avoids floating-point arithmetic. 6.1 Explain, using an example, how Bresenham’s algorithm determines how to draw pixels based on the gradient of line. Use a diagram to assist with your explanation. (6) 8 COS3712 May/June 2015 [TURN OVER] ✓✓ Bresenham’s algorithm begins with the point (0, 0) and “illuminates” that pixel. ✓. Rather than keeping track of the y coordinate (which increases by m = ∆y/ ∆x, each time the x increases by one), the algorithm keeps an error bound E at each stage, ✓which represents the negative of the distance from the point where the line exits the pixel to the top edge of the pixel . This value is first set to m − 1, and is incremented by m each time the x coordinate is incremented by one. ✓If E becomes greater than zero, we know that the line has moved upwards one pixel, and that we must increment our y coordinate and readjust the error to represent the distance from the top of the new pixel – which is done by subtracting one from E. ✓ 6.2 Bresenham’s algorithm has become the standard approach used in hardware and software rasterizers as opposed to the more simpler DDA algorithm. Why is this so? (2) The DDA algorithm is efficient and can be coded easily, but it requires a floating-point addition for each pixel generated. Bresenhams algorithm avoids all floating point calculations. 9 COS3712 May/Jun 2015 [TURN OVER] QUESTION 7 [8] 7.1 What is the purpose of the reshape callback function? (1) Reshape callback – to change viewport size when a window size changes. ✓ 7.2 The following two OpenGL statements are used for double duffering: 7.2.1 glutInitDisplayMode(GLUT_DOUBLE); 7.2.2 glutSwapBuffers(); Explain the purpose for each statement. (2) 7.2.1 The use of both front and back (colour) buffers is engaged. ✓ 7.2.2 Here the contents of the back buffer are copied into the front buffer. ✓ 8.3 What is the purpose of the OpenGL glFlush statement? (2) Ensures that all data are rendered asap, ✓ program will work if left out but delays are noticed in busy or networked environments. ✓ 8.4 You are required to add a simple menu, with 4 items, to an OpenGL program. The menu is activated by the right click of the mouse. State which OpenGL statements you would use to do this. (3) glutCreateMenu(); ✓ glutAddMenuEntry(); ✓ glutAttachMenu(GLUT_RIGHT_BUTTON); ✓","libVersion":"0.2.3","langs":""}