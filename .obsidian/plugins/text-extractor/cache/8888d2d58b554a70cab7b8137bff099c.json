{"path":"Subjects/COS3751 - Techniques of Artificial Intelligence/Telegram Notes/techniques-in-artificial-intelligence.pdf","text":"Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 1 | P a g e Contents Ch. 1 – Introduction .................................................................................................................................... 3 What is AI? .............................................................................................................................................. 3 Acting Humanly: The Turing Test Approach ......................................................................................... 3 Thinking Humanly: The Cognitive Modelling Approach........................................................................ 3 Thinking Rationally: The “laws of thought” approach .......................................................................... 4 The Foundations of Artificial Intelligence ............................................................................................ 4 Philosophy ............................................................................................................................................. 4 Mathematics ......................................................................................................................................... 5 Ch.9 - Inference and First-order Logic .................................................................................................... 7 Propositional vs. First-Order Inference ............................................................................................... 7 Inference rules for Quantifiers .............................................................................................................. 7 Reduction to Propositional Inference ................................................................................................... 8 Unification and Lifting ............................................................................................................................ 8 A First-Order Inference Rule ................................................................................................................. 9 Unification ........................................................................................................................................... 10 The Unification Algorithm ................................................................................................................... 11 Storage and Retrieval .......................................................................................................................... 11 Ch. 18 – Learning from Examples ......................................................................................................... 13 Forms of Learning ................................................................................................................................ 13 Supervised Learning ............................................................................................................................ 14 Learning Decision Trees ..................................................................................................................... 15 The decision tree representation ........................................................................................................ 15 Expressiveness of decision trees ......................................................................................................... 16 Inducing Trees from Examples ............................................................................................................ 17 Choosing Attribute Sets ...................................................................................................................... 20 Generalizing and Over-fitting .............................................................................................................. 21 Broadening the applicability of decision trees.................................................................................... 22 Ch. 4 – Beyond Classical Search .......................................................................................................... 24 Local Search Algorithms and Optimization Problems .................................................................... 24 Hill-Climbing Search ............................................................................................................................ 25 Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 2 | P a g e Simulated Annealing ........................................................................................................................... 25 Local Beam Search .............................................................................................................................. 26 Genetic Algorithms ............................................................................................................................. 26 Local Search in Continuous Spaces ................................................................................................. 28 Searching with Nondeterministic Actions ......................................................................................... 30 AND-OR Search Trees .......................................................................................................................... 31 Try, try Again ....................................................................................................................................... 31 Searching With Partial Observations ................................................................................................ 32 Searching with no observation ........................................................................................................... 32 Searching with observations ............................................................................................................... 33 Solving partially observable problems ................................................................................................ 34 An Agent for partially observable environments ................................................................................ 34 Online Search Agents and Unknown Environments ....................................................................... 36 Online search problems ...................................................................................................................... 36 Online search algorithms .................................................................................................................... 38 Online Local Search ............................................................................................................................. 38 Learning in online search .................................................................................................................... 40 Ch. 5 – Adversarial Search ..................................................................................................................... 41 Games .................................................................................................................................................... 41 Optimal Decisions in Games .............................................................................................................. 42 The minimax algorithm ....................................................................................................................... 44 Optimal Decisions in multiplayer games ............................................................................................. 44 Alpha-Beta Pruning .............................................................................................................................. 45 Move Ordering .................................................................................................................................... 47 Imperfect Real Time Decisions .......................................................................................................... 48 Evaluation Functions ........................................................................................................................... 49 Cutting off Search ............................................................................................................................... 50 Forward Pruning ................................................................................................................................. 50 Search versus Lookup.......................................................................................................................... 51 Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 3 | P a g e Ch. 1 – Introduction What is AI? Thinking Humanly Thinking Rationally “The exciting new effort to make computers think.. machines with minds, in the full and literal sense” (Haugeland, 1985) “[The automation of] activities that we associate with human thinking, activities such as decision making, problem solving, learning…” (Bellman, 1978) “The study of mental faculties through the use of computational models.” (Chamiak and McDermott, 1985) The study of the computations that make it possible to perceive, reason, and act” (Winston, 1992) Acting Humanly Acting Rationally “The art of creating machines that perform functions that require intelligence when performed by people” (Kursweil, 1990) “AI . . . is concerned with intelligent behavior in artifacts.” (Nilsson, 1998) “Computational Intelligence is the study of the design of intelligent agents.” (Poole et al., 1998) Acting Humanly: The Turing Test Approach Proposed by Alan Turing (1950) was designed to provide a satisfactory operational definition of intelligence. e computer would need to posse the following capabilities:  Natural Language Processing – To enable it to communicate successfully in English.  Knowledge Representation – To store what it knows or hears.  Automated Reasoning – to use the stored information to answer questions and to draw new conclusions.  Machine Learning – to adapt to new circumstances and to detect and extrapolate patterns. The test deliberately avoided direct physical interaction between the interrogator and the computer, because physical simulation of a person is unnecessary for intelligence. The total Turing Test includes a video signal so that the interrogator can test the subject’s perceptual abilities. To pass the Total Turing Test, the computer will need:  Computer Vision – To perceive objects, and  Robotics – to manipulate and move about Thinking Humanly: The Cognitive Modelling Approach We need to get inside the actual workings of human minds. There are three ways to do this: 1. Introspection – trying to catch out own thoughts as they go by 2. Psychological experiments – observing a person in action 3. Brain imaging – observing the brain in action Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 4 | P a g e The interdisciplinary field of cognitive science brings together computer models from AI and experimental techniques from psychological to construct precise and testable theories of the human mind. Thinking Rationally: The “laws of thought” approach An agent is just something that acts (agent comes from the Latin agere, to do). Computer agents are expected to do more: operate autonomously, perceive their environment, persist over a prolonged time period, adapt to change and create and pursue goals. rational agent is one that acts so as to achieve the best outcome or, when there is uncertainty, the best expected outcome. Making correct inference is sometimes part of being a rational agent, because one way to act rationally is to reason logically to the conclusion that a given action will achieve one’s goals and then to act on that conclusion that a given action will achieve one’s goals. Correct inference is not all of rationality; in some situations, there is no provably correct thing to do, but something must still be done. There are also ways of acting rationally that cannot be said to involve inference. Recoiling from a hot stove is a reflex action that is usually more successful than a slower action taken after deliberation. All the skills needed for the Turing Test allow an agent to act rationally. The Foundations of Artificial Intelligence Philosophy (Russel p. 6) Can formal rules be used to draw valid conclusions?  How does the mind arise from a physical brain?  Where does knowledge come from?  How does knowledge lead to action? One problem with a purely physical conception of the mind is that it seems to leave little room for free will: if the mind is governed entirely by physical laws then it has no more free will than a rock “deciding” to fall to the earth. Descartes was a strong advocate of the power of reasoning in understanding the world, a philosophy now called rationalism, and one that count Aristotle and Leibnitz as members. Descartes was also a proponent of dualism. He held that there is a part of the human mind (or soul or spirit) that is outside of nature. Animals did not possess this. Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 5 | P a g e An alternative to dualism is materialism, which holds that the brain’s operation according to the laws of physics constitutes the mind. Free will is simply the way that the perception of available choices appears to the choosing entity. Given a physical mind that manipulates knowledge, the next problem is to establish the source of knowledge. The empiricism movement, starting with Francois Bacon’s Novum Organum, is characterized by a dictum of John Locke: “Nothing is in the understanding, which was not first in the senses.” David Hume’s A Treatise of Human Nature proposed what is now known as the principle of induction: that general rules are acquired by exposure to repeated associations between their elements. The famous Vienna Circle, led by Rudolf Carnap, developed the doctrine of logical positivism. This doctrine holds that all knowledge can be characterized by logical theories connected, ultimately, to observation sentences that correspond to sensory inputs; thus logical positivism combines rationalism and empiricism. The final element in the philosophy picture of the mind is the connection between knowledge and action. Intelligence requires action as well as reasoning. Only by understanding how actions are justified can we understand how to build an agent whose actions are justifiable (or rational). Aristotle argued that actions are justified by logical connection between goals and knowledge of the action’s outcome. Goal based analysis is useful, but does not say what to do when several actions will achieve the goal or when no action will achieve it completely. Mathematics (Russel p. 8) What are the formal rules to draw valid conclusions?  What can be computed?  How do we reason with uncertain information? 1. The idea of formal logic can be traced back to the philosophers of ancient Greece, but its mathematical development really began with the work of George Boole, who worked out the details of propositional, or Boolean, logic. 2. Gottlob Frege extended Boole’s logic to include objects and relations, creating the first-order logic that is used today. 3. Alfred Tarski introduced a theory of reference that shows how to relate the objects in a logic to objects in the real world. The first nontrivial algorithm is thought to be Euclid’s algorithm for computing greatest common divisors. Godel showed that limits on deduction do exist. His incompleteness theorem showed that in any formal theory as strong as Peano arithmetic (the elementary theory of natural numbers), there are true statements that are undecidable in the sense that they have no proof within the theory. Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 6 | P a g e This fundamental result can also be interpreted as showing that some functions on the integers cannot be represented by an algorithm, they cannot be computed. Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 7 | P a g e Ch.9 - Inference and First-order Logic Propositional vs. First-Order Inference Inference rules for Quantifiers Suppose our knowledge base contains the standard folkloric axiom stating that all greedy kings are evil: ∀𝑥 𝐾𝑖𝑛𝑔(𝑥) ⋀ 𝐺𝑟𝑒𝑒𝑑𝑦(𝑥) ⇒ 𝐸𝑣𝑖𝑙(𝑥) Then it seems quite permissible to infer the following sentences: ∀𝑥 𝐾𝑖𝑛𝑔(𝐽𝑜ℎ𝑛) ⋀ 𝐺𝑟𝑒𝑒𝑑𝑦(𝐽𝑜ℎ𝑛) ⇒ 𝐸𝑣𝑖𝑙(𝐽𝑜ℎ𝑛) ∀𝑥 𝐾𝑖𝑛𝑔(𝑅𝑖𝑐ℎ𝑎𝑟𝑑) ⋀ 𝐺𝑟𝑒𝑒𝑑𝑦(𝑅𝑖𝑐ℎ𝑎𝑟𝑑) ⇒ 𝐸𝑣𝑖𝑙(𝑟𝑖𝑐ℎ𝑎𝑟𝑑) . . . e rule for universal Instantiation (UI for short) says that we can infer any sentence obtained by substituting a ground term (term without variables) for the variable. To write out the inference rule formally, we use the notion of substitution. Let 𝑆𝑈𝐵𝑆𝑇(𝜃, 𝛼) denote the result of applying the substitution 𝜃 to the sentence 𝛼. Then the rule is written: ∀𝑣 𝛼 𝑆𝑈𝐵𝑆𝑇 ({𝑣 𝑔} , 𝛼) For any variable 𝑣 and ground term 𝑔. In the rule for Existential Instantiation, the variable is replaced by a single new constant symbol. The formal statement is as follows: for any sentence 𝛼, variable 𝑣, and constant symbol 𝑘 that does not appear elsewhere in the knowledge base, ∃𝑣 𝛼 𝑆𝑈𝐵𝑆𝑇 ({ 𝑣} , 𝛼) For example: ∃𝑥 𝐶𝑟𝑜𝑤𝑛(𝑥) ∧ 𝑂𝑛𝐻𝑒𝑎𝑑(𝑥, 𝐽𝑜ℎ𝑛) We can infer the sentence: 𝐶𝑟𝑜𝑤𝑛(𝑐1) ∧ 𝑂𝑛𝐻𝑒𝑎𝑑(𝑐1, 𝐽𝑜ℎ𝑛) As long as 𝐶1 does not appear elsewhere in the knowledge base. The existential sentence says there is some object satisfying a condition, and applying the existential instantiation rule just gives a name to that object. Whereas universal instantiation can be applied many times to produce many different consequences, Existential instantiation can be applied once, and then the existentially quantified sentence can be discarded. For Example: We no longer need ∃𝑥 𝐾𝑖𝑙𝑙𝑠(𝑥, 𝑉𝑖𝑐𝑡𝑖𝑚) once we have added the sentence 𝐾𝑖𝑙𝑙(𝑀𝑢𝑟𝑑𝑒𝑟, 𝑉𝑖𝑐𝑡𝑖𝑚). Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 8 | P a g e Strictly speaking, the new knowledge base is not logically equivalent to the old, but it can be shown to be inferentially equivalent, in the sense that it is satisfiable exactly when the original knowledge base is satisfiable. Reduction to Propositional Inference Once we have rules for inferring non-quantified sentences from quantified sentences, it becomes possible to reduce first order inference to propositional inference. 1. Just as an existentially quantified sentence can be replaced by one instantiation, a universally quantified sentence can be replaced by the set of all possible instantiations. For Example: Suppose that our knowledge base contains just the sentences: ∀𝑥 𝐾𝑖𝑛𝑔(𝑥) ⋀ 𝐺𝑟𝑒𝑒𝑑𝑦(𝑥) ⇒ 𝐸𝑣𝑖𝑙(𝑥) 𝐾𝑖𝑛𝑔(𝐽𝑜ℎ𝑛) 𝐺𝑟𝑒𝑒𝑑𝑦(𝐽𝑜ℎ𝑛) 𝐵𝑟𝑜𝑡ℎ𝑒𝑟(𝑅𝑖𝑐ℎ𝑎𝑟𝑑, 𝐽𝑜ℎ𝑛) (9.1) Then we apply UI to the first sentence using all possible ground term substitutions from the vocabulary of the knowledge base, in this case: {𝑥/𝐽𝑜ℎ𝑛} and {𝑥/𝑅𝑖𝑐ℎ𝑎𝑟𝑑}. We obtain: 𝐾𝑖𝑛𝑔(𝐽𝑜ℎ𝑛) ⋀ 𝐺𝑟𝑒𝑒𝑑𝑦(𝐽𝑜ℎ𝑛) ⇒ 𝐸𝑣𝑖𝑙(𝐽𝑜ℎ𝑛) 𝐾𝑖𝑛𝑔(𝑅𝑖𝑐ℎ𝑎𝑟𝑑) ⋀ 𝐺𝑟𝑒𝑒𝑑𝑦(𝑅𝑖𝑐𝑎ℎ𝑟𝑑) ⇒ 𝐸𝑣𝑖𝑙(𝑅𝑖𝑐ℎ𝑎𝑟𝑑) And we discard the universally quantified sentence. The technique of propositionalization can be made completely general, that is, every first-order knowledge base and query can be propositional zed in such a way that entailment is preserved. There is a problem: when the knowledge base includes a function symbol, the set of possible ground-term substitutions is infinite. For Example: If the knowledge base mentions the 𝐹𝑎𝑡ℎ𝑒𝑟 symbol, then infinitely many nested terms such as 𝐹𝑎𝑡ℎ𝑒𝑟(𝑓𝑎𝑡ℎ𝑒𝑟(𝐹𝑎𝑡ℎ𝑒𝑟(𝐽𝑜ℎ𝑛))) can be constructed. Our propositional algorithms will have difficulty with an infinitely large set of sentences. Since any such subset has a maximum depth of nesting among its ground terms, we can find the subset by first generating all the instantiations with constant symbols (𝑅𝑖𝑐ℎ𝑎𝑟𝑑 and 𝐽𝑜ℎ𝑛), then all the terms of depth 1 (𝐹𝑎𝑡ℎ𝑒𝑟(𝑅𝑖𝑐𝑎ℎ𝑟𝑑) and 𝐹𝑎𝑡ℎ𝑒𝑟(𝐽𝑜ℎ𝑛)), then all the terms of depth 2 and so on, until we are able to construct a propositional proof of the entailed sentence. Unification and Lifting The sharp-eyed reader will have noticed that the propositionalization approach is rather inefficient. For example: Given the query 𝐸𝑣𝑖𝑙(𝑥) and the knowledge base in Equation 9.1, it seems perverse to generate sentences such as 𝐾𝑖𝑛𝑔(𝑅𝑖𝑐ℎ𝑎𝑟𝑑) ⋀ 𝐺𝑟𝑒𝑒𝑑𝑦(𝑅𝑖𝑐ℎ𝑎𝑟𝑑) ⇒ 𝐸𝑣𝑖𝑙(𝑅𝑖𝑐ℎ𝑎𝑟𝑑). Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 9 | P a g e Indeed, the inference of 𝐸𝑣𝑖𝑙(𝐽𝑜ℎ𝑛) from the sentences: ∀𝑥 𝐾𝑖𝑛𝑔(𝑥) ⋀ 𝐺𝑟𝑒𝑒𝑑𝑦(𝑥) ⇒ 𝐸𝑣𝑖𝑙(𝑥) 𝐾𝑖𝑛𝑔(𝐽𝑜ℎ𝑛) 𝐺𝑟𝑒𝑒𝑑𝑦(𝐽𝑜ℎ𝑛) seems completely obvious to a human being. We now show how to make it completely obvious to a computer. A First-Order Inference Rule The inference that John is evil works like this: to use the rule that greedy kings are evil, find some 𝑥 such that 𝑥 is a king and 𝑥 is greedy, and then infer that this 𝑥 is evil. More generally: If there is some substitution θ that makes each of the conjuncts of the premise of the implication identical to sentences already in the knowledge base, then we can assert the conclusion of the implication, after applying θ. In this case, the substitution 𝜃 = {𝑥/𝐽𝑜ℎ𝑛} achieves that aim. We can actually make the inference step do more work. Suppose that instead of knowing 𝐺𝑟𝑒𝑒𝑑𝑦(𝐽𝑜ℎ𝑛), we know that 𝑒𝑣𝑒𝑟𝑦𝑜𝑛𝑒 is greedy: ∀𝑦 𝐺𝑟𝑒𝑒𝑑𝑦(𝑦) (9.2) Then we would still like to conclude 𝐸𝑣𝑖𝑙(𝐽𝑜ℎ𝑛). We know that John is a king (given) and John is greedy. What we need for this to work is to find a substitution both for the variables in the implication sentence and for the variables in the sentences that are in the knowledge base. In this case, applying the substitution {𝑥/𝐽𝑜ℎ𝑛, 𝑦/𝐽𝑜ℎ𝑛} to the implication premises 𝐾𝑖𝑛𝑔(𝑥) and 𝐺𝑟𝑒𝑒𝑑𝑦(𝑥) and the knowledge base sentences 𝐾𝑖𝑛𝑔(𝐽𝑜ℎ𝑛) and 𝐺𝑟𝑒𝑒𝑑𝑦(𝑦) will make them identical. This, we can infer the conclusion of the implication. This inference process can be captured as a single rule that we call Generalized Modus Ponens. For atomic sentences 𝑝𝑖, 𝑝𝑖 ′ and 𝑞, where there is a substitution 𝜃 such that 𝑆𝑈𝐵𝑆𝑇(𝜃, 𝑝𝑖 ′) = 𝑆𝑈𝐵𝑆𝑇(𝜃, 𝑝𝑖), for all 𝑖, 𝑝1 ′ , 𝑝2 ′ , … , 𝑝𝑛 ′ , (𝑝1 ∧ p2 ∧ … ∧ pn ⇒ 𝑞) 𝑆𝑈𝐵𝑆𝑇(𝜃, 𝑞) There are 𝑛 + 1 premises to this rule: the 𝑛 atomic sentences 𝑝𝑖′ and the one implication. The conclusion is the result of applying the substitution 𝜃 to the consequent 𝑞. For our Example: 𝑝1 ′ 𝑖𝑠 𝐾𝑖𝑛𝑔(𝐽𝑜ℎ𝑛) 𝑝1 𝑖𝑠 𝐾𝑖𝑛𝑔(𝑥) 𝑝2 ′ 𝑖𝑠 𝐺𝑟𝑒𝑒𝑑𝑦(𝑦) 𝑝2 𝑖𝑠 𝐺𝑟𝑒𝑒𝑑𝑦(𝑥) 𝜃 𝑖𝑠 {𝑥/𝐽𝑜ℎ𝑛, 𝑦/𝐽𝑜ℎ𝑛} 𝑞 𝑖𝑠 𝐸𝑣𝑖𝑙(𝑥) 𝑆𝑈𝐵𝑆𝑇(𝜃, 𝑞) 𝑖𝑠 𝐸𝑣𝑖𝑙(𝐽𝑜ℎ𝑛) Generalized Modus Ponens is a lifter version of Modus Ponens – it raises Modus Ponens from ground (variable-free) propositional logic to first order logic. Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 10 | P a g e Unification Lifted inference rules require finding substitutions that make different logical expressions look identical. This process is called unification and is a key component of all first-order inference algorithms. The UNIFY algorithm takes two sentences and returns a unifier for them if one exists: 𝑈𝑁𝐼𝐹𝑌(𝑝, 𝑞) = 𝜃 where 𝑆𝑈𝐵𝑆𝑇(𝜃, 𝑝) = 𝑆𝑈𝐵𝑆𝑇(𝜃, 𝑞) For Example: Suppose we have a query 𝐴𝑠𝑘𝑉𝑎𝑟𝑠(𝐾𝑛𝑜𝑤𝑠(𝐽𝑜ℎ𝑛, 𝑥)): Whom does John know? Answers to this query can be found by finding all sentences in the knowledge base that unify with 𝐾𝑛𝑜𝑤𝑠(𝐽𝑜ℎ𝑛, 𝑥). Here are the results of unification with 4 different sentences that might be in the knowledge base: UNIFY(Knows(John, x), Knows(John, Jane)) = {x/Jane} UNIFY(Knows(John, x), Knows(y, Bill )) = {x/Bill, y/John} UNIFY(Knows(John, x), Knows(y,Mother (y))) = {y/John, x/Mother (John)} UNIFY(Knows(John, x), Knows(x, Elizabeth)) = fail The last unification fails because 𝑥 cannot take on the values 𝐽𝑜ℎ𝑛 and 𝐸𝑙𝑖𝑧𝑎𝑏𝑒𝑡ℎ at the same time. Now remember that 𝐾𝑛𝑜𝑤𝑠(𝑥, 𝐸𝑙𝑖𝑧𝑎𝑏𝑒𝑡ℎ) means “Everyone knows Elizabeth” so we 𝑠ℎ𝑜𝑢𝑙𝑑 be able to infer that John knows Elizabeth. The problem arises only because the 2 sentences happen to use the same variable name, 𝑥. The problem can be avoided by standardizing apart one of the two sentences being unified, which means renaming its variables to avoid name clashes. For Example: We can rename 𝑥 in 𝐾𝑛𝑜𝑤𝑠(𝑥, 𝐸𝑙𝑖𝑧𝑎𝑏𝑒𝑡ℎ) to 𝑥17 (a new variable name) without changing it meaning. Now the unification will work: UNIFY(Knows(John, x), Knows(x17, Elizabeth)) = {x/Elizabeth, x17/John} . There is one more complication: we said that UNIFY should return a substitution that makes the two arguments look the same. But there could be more than one such unifier. For example: 𝑈𝑁𝐼𝐹𝑌(𝐾𝑛𝑜𝑤𝑠(𝐽𝑜ℎ𝑛, 𝑥), 𝐾𝑛𝑜𝑤𝑠(𝑦, 𝑧)) could return {𝑦/𝐽𝑜ℎ𝑛, 𝑥/𝑧} or {𝑦/𝐽𝑜ℎ𝑛, 𝑥/𝐽𝑜ℎ𝑛, 𝑧/𝐽𝑜ℎ𝑛}. We say that the first unifier is more general than the second, because it places fewer restrictions on the values of the variables. For every unifiable pair of expressions, there is a single most general unifier (or MGU) that is unique to renaming and substitution of variables. There is one expensive step: when matching a variable against a complex term, one must check whether the variable itself occurs inside the term; if it does, the match fails because no consistent unifier can be constructed. Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 11 | P a g e For example: 𝑆(𝑥) can’t unify with 𝑆(𝑆(𝑥)). This so called occur check makes the complexity of the entire algorithm quadratic in the size of the expressions being unified. The Unification Algorithm function 𝑈𝑁𝐼𝐹𝑌(𝑥, 𝑦, 𝜃) returns a substitution to make 𝑥 and 𝑦 identical inputs: 𝑥, a variable, constant, list, or compound expression 𝑦, a variable, constant, list, or compound expression 𝜃, the substitution built up so far (optional, defaults to empty) if 𝜃 = failure then return failure else if 𝑥 = 𝑦 then return 0 else if VARIABLE?(𝑥) then return UNIFY-VAR(𝑥, 𝑦, 𝜃) else if VARIABLE?(𝑦) then return UNIFY-VAR(𝑦, 𝑥, 𝜃) else if COMPOUND?(𝑥) and COMPOUND?(𝑦) then return 𝑈𝑁𝐼𝐹𝑌(𝑥. 𝐴𝑅𝐺𝑆, 𝑦. 𝐴𝑅𝐺𝑆, 𝑈𝑁𝐼𝐹𝑌(𝑥. 𝑂𝑃, 𝑦. 𝑂𝑃, 𝜃)) else if LIST?(𝑥) and LIST(𝑦) then return 𝑈𝑁𝐼𝐹𝑌(𝑥. 𝑅𝐸𝑆𝑇, 𝑦. 𝑅𝐸𝑆𝑇, 𝑈𝑁𝐼𝐹𝑌(𝑥. 𝐹𝐼𝑅𝑆𝑇, 𝑦. 𝐹𝐼𝑅𝑆𝑇, 𝜃)) else return failure function UNIFY-VAR(𝑣𝑎𝑟, 𝑥, 𝜃) returns a substitution if {𝑣𝑎𝑟/𝑣𝑎𝑙} ∈ 𝜃 then return 𝑈𝑁𝐼𝐹𝑌(𝑣𝑎𝑙, 𝑥, 𝜃) else if {𝑥/𝑣𝑎𝑙} ∈ 𝜃 then return 𝑈𝑁𝐼𝐹𝑌(𝑣𝑎𝑟, 𝑣𝑎𝑙, 𝜃) else if 𝑂𝐶𝐶𝑈𝑅 − 𝐶𝐻𝐸𝐶𝐾? (𝑣𝑎𝑟, 𝑥)then return 𝐹𝑎𝑖𝑙𝑢𝑟𝑒 else return add {𝑣𝑎𝑟/𝑥} to 𝜃 The algorithm works by comparing the structures of the inputs, element by element. The substitution 𝜃 that is the argument to UNIFY is built up along the way and is used to make sure that later comparisons are consistent with binding that were established earlier. In a compound expression such as F(A, B), the OP field picks out the function symbol F and the ARGS field picks out the argument list (A, B). Storage and Retrieval STORE(s) stores a sentence 𝑠 into the knowledge base and FETCH(𝑞) returns all unifiers such that the query 𝑞 unifies with some sentence in the knowledge base. The simplest way to implement STORE and FETCH is to keep all the facts in one long list and unify each query against every element of the list. We can make FETCH more efficient by ensuring that unification are attempted only with sentences some chance of unifying. For example: There is no point in trying to unify 𝐾𝑛𝑜𝑤𝑠(𝐽𝑜ℎ𝑛, 𝑥) with 𝐵𝑟𝑜𝑡ℎ𝑒𝑟(𝑅𝑖𝑐ℎ𝑎𝑟𝑑, 𝐽𝑜ℎ𝑛). Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 12 | P a g e We can avoid such unification by indexing the facts in the knowledge base. A simple scheme called predictive indexing puts all the 𝐾𝑛𝑜𝑤𝑠 facts in one bucket and all the 𝐵𝑟𝑜𝑡ℎ𝑒𝑟 facts in another. Predictive indexing is useful when there are many predicate symbols but only a few clauses for each symbol. Sometimes, however, a predicate has many clauses. For example: Suppose that the tax authorities want to keep track of who employs whom, using a predicate 𝐸𝑚𝑝𝑙𝑜𝑦𝑠(𝑥, 𝑦). This would be a very large bucket with perhaps millions of employers and tens of millions of employees. Answering a query such as 𝐸𝑚𝑝𝑙𝑜𝑦𝑠(𝑥, 𝑅𝑖𝑐ℎ𝑎𝑟𝑑) with predicate indexing would require scanning the entire bucket. For this particular query, it would help if the facts were indexed both by predicate and by second argument, perhaps using a combined hash table key. Then we could simply construct the key from the query and retrieve exactly those facts that unify the query. Given a sentence to be stored, it is possible to construct indices for all possible queries that unify with it. For example: For the fact that 𝐸𝑚𝑝𝑙𝑜𝑦𝑠(𝐼𝐵𝑀, 𝑅𝑖𝑐ℎ𝑎𝑟𝑑), the queries are: 𝐸𝑚𝑝𝑙𝑜𝑦𝑠(𝐼𝐵𝑀, 𝑅𝑖𝑐ℎ𝑎𝑟𝑑) Does IBM employ Richard? 𝐸𝑚𝑝𝑙𝑜𝑦𝑠(𝑥, 𝑅𝑖𝑐ℎ𝑎𝑟𝑑) Who employs Richard? 𝐸𝑚𝑝𝑙𝑜𝑦𝑠(𝐼𝐵𝑀, 𝑦) Whom does IBM employ? 𝐸𝑚𝑝𝑙𝑜𝑦𝑠(𝑥, 𝑦) Who employs whom? Figure 9.2 – p. 329 – Subsumption lattice These queries form a subsumption lattice. The lattice has some interesting properties. - The child of any node in the lattice is obtained from its parent by a single substitution - The highest common descendant of any two nodes is the result of applying their MGU - The portion of the lattice above any ground fact can be constructed systematically. - A sentence with repeated constants has a slightly different lattice. For a predicate with 𝑛 arguments, the lattice contains 𝑂(2𝑛) nodes. If function symbols are allowed, the number of nodes is also exponential in the size of the terms in the sentence to be stored. At one point, the benefits of indexing are outweighed by the costs of storing and maintaining all the indices. Summary – p. 357 Exercises – p. 360 Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 13 | P a g e Ch. 18 – Learning from Examples An agent is learning if it improves its performance on future tasks after making observations about the world. Designers cannot anticipate all possible situations, the designers cannot anticipate all changes over time, and sometimes human programmers have no idea how to program solutions themselves. Forms of Learning The improvements, and the techniques used to make them, depend on 4 major factors: - Which component is to be improved - What prior knowledge the agent already has - What representation is used for the data and the component - What feedback is available to learn from. Components to be learned Chapter 2 described several agent designs. The components of these agents include: 1. A direct mapping from conditions on the current state to actions. 2. A means to infer relevant properties of the world from the percept sequence. 3. Information about the way the world evolves and about the result of possible actions the agent can take. 4. Utility information indicating the desirability of world states 5. Action value information indicating the desirability of actions 6. Goals that describe classes of states whose achievement maximizes the agent’s utility Representation and Prior Knowledge This chapter covers inputs that form a factored representation – a vector of attribute values – and outputs that can be either a continuous numerical value or discrete value. There is another way to look at the various types of learning. We say that learning a (possibly incorrect) general function or rule from specific input-output pairs is called inductive learning. We can also do analytical or deductive learning: going from a known general rule to a new rule that is logically entailed. Feedback to learn from There are 3 types of feedback that determines the three main types of learning: 1. In unsupervised learning the agents learns patterns in the input even though no explicit feedback is supplied. The common unsupervised learning task is clustering: detecting potentially useful clusters of input examples. 2. In reinforcement learning the agent learns from a series of reinforcements, rewards or punishments. 3. In Supervised learning the agent observes some example input-output pairs and learns a function that maps from input to output. In practice, these distinctions are not always so crisp. In semi-supervised learning we are given a few labelled examples and must make what we can of a large collection of unlabelled examples. Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 14 | P a g e Supervised Learning The task of supervised learning is this: Given a training set of 𝑁 example input-output pairs (𝑥1, 𝑦1), (𝑥2, 𝑦2), . . . (𝑥𝑁, 𝑦𝑁) Where each 𝑦𝑖 was generated by an unknown function 𝑦 = 𝑓(𝑥), Discover that a function ℎ that approximates the true function 𝑓. 𝑥 and 𝑦 can be any values, they need not be numbers. The function ℎ is a hypothesis. Learning is a search through the space of possible hypotheses for one that will perform well, even on the new examples. To measure the accuracy of a hypothesis, we give it a test set of examples that are distinct from the training set. We say a hypothesis generalizes well if it correctly predicts the value of 𝑦 for novel examples. Sometimes the function 𝑓 is stochastic – it is not strictly a function of 𝑥, and what we have to learn is that a conditional probability distribution, 𝑷(𝑌|𝑥). When the output 𝑦 is one of a finite set of values, the learning problem is called classification, and is called Boolean or Binary classification if there are only two values. When 𝑦 is a number, the learning problem is called regression. We don’t know that 𝑓 is, but we will approximate it with a function ℎ selected from a hypotheses space, 𝐻, which for this example we will take to be the set of polynomials, such as 𝑥5 + 3𝑥2 + 2. Figure 18.1.a shows some data with an exact fit by a straight line. The line is called a consistent hypothesis because it agrees with all the data. Figure 18.1.b shows a high-degree polynomial that is also consistent with the same data. Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 15 | P a g e This illustrates a fundamental problem with inductive learning: how do we choose from among multiple consistent hypotheses? One answer is to prefer the simplest hypotheses consistent with the data. This principle is called Ockham’s Razor. Figure 18.1.c shows a 2nd data set. There is no consistent straight line, it requires a degree 6 polynomial for an exact fit. In general, there is a trade-off between complex hypotheses that can fit the training data well and simpler hypotheses that may generalize better. Figure 18.1.d expands the hypothesis space 𝐻 to allow polynomials over both 𝑥 and sin (𝑥), and find that the data in (c) can be fitted exactly by a simple function of the form 𝑎𝑥 + 𝑏 + 𝑐 sin (𝑥). We say that a learning problem is realizable if the hypothesis space contains the true function. Supervised Learning can be done by choosing the hypothesis ℎ* that is most probable given the data: ℎ* = 𝑎𝑟𝑔𝑚𝑎𝑥 𝑃(ℎ|𝑑𝑎𝑡𝑎) ℎ ∈ 𝐻 By Bayes’ rule this is equivalent to ℎ* = 𝑎𝑟𝑔𝑚𝑎𝑥 𝑃(ℎ|𝑑𝑎𝑡𝑎)𝑃(ℎ) ℎ ∈ 𝐻 Then we can say that the prior probability 𝑃(ℎ) is high for a degree-1 or -2 polynomial, lower for a degree-7 polynomial, and especially low for degree-7 polynomials with large, sharp spikes. There is a trade-off between the expressiveness of a hypothesis space and the complexity of finding a good hypothesis within that space. Learning Decision Trees The decision tree representation decision tree represents a function that takes as input a vector of attribute values and returns a “decision” – a single output value. Each internal node in the tree corresponds to a test of the value of one of the input attributes, 𝐴𝑖 and the branches from the node are labelled with the possible values of the attribute, 𝐴𝑖 = 𝑣𝑖𝑘. Each leaf node in the tree specifies a value returned by the functions. For Example We will now build a decision tree to decide whether to wait for a table at a restaurant. The aim here is to learn a definition for the goal predicate 𝑊𝑖𝑙𝑙 𝑊𝑎𝑖𝑡. First we list the attributes that we will consider as part of the input: Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 16 | P a g e Expressiveness of decision trees A Boolean decision tree is logically equivalent to the assertion that the goal attribute is true if and only if the input attributes satisfy one of the paths leading to a leaf with value 𝑡𝑟𝑢𝑒. Writing this out in propositional logic, we have: 𝐺𝑜𝑎𝑙 ⇔ (𝑃𝑎𝑡ℎ1 V 𝑃𝑎𝑡ℎ2 V . . . ) where each 𝑃𝑎𝑡ℎ is a conjunction of attribute-value tests required to follow that path. For Example The rightmost path in Figure 18.2 is 𝑃𝑎𝑡ℎ = (𝑃𝑎𝑡𝑟𝑜𝑛𝑠 = 𝐹𝑢𝑙𝑙 ∧ WaitEstimate = 0 − 10) Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 17 | P a g e Inducing Trees from Examples An example for a Boolean decision tree consists of an (𝒙, 𝑦) pair, where 𝒙 is a vector of values for the input attributes, and 𝑦 is a single Boolean output value. The positive examples are the ones where the goal 𝑊𝑖𝑙𝑙𝑊𝑎𝑖𝑡 is true (𝑥1, 𝑥3. . . ); The negative examples are the ones in which it is false. The DECISION-TREE-LEARNING algorithm adopts a greedy divide-and-conquer strategy: always test the most important attribute first. By “most important attribute” we mean the the one that makes the most difference to the classification. Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 18 | P a g e In general, after the 1st attribute set splits the examples, each outcome is a new decision tree learning problem itself, with fewer examples and one less attribute. There are 4 cases to consider for these recursive problems: 1. If the remaining examples are all positive (or all negative), then we are done: we can 𝑌𝑒𝑠 or 𝑁𝑜. 2. If there are some positive and some negative examples, then choose the best attribute to split them. 3. If there are no examples left, it means that no example has been observed for this combination of attribute values, and we return a default value calculated from the plurality classification of all the examples that were used in constructing the node’s parent. These are passed along in the variable 𝑝𝑎𝑟𝑒𝑛𝑡_𝑒𝑥𝑎𝑚𝑝𝑙𝑒𝑠. 4. If there are no attributes left, but not positive or negative examples, it means that these examples have exactly the same description, but different classifications. This can happen because there is an error or noise in the data; because the domain is nondeterministic; or because we can’t observe an attribute that would distinguish the examples. Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 19 | P a g e Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 20 | P a g e We can evaluate the accuracy of a learning algorithm with a learning curve. We learn a hypothesis ℎ with the training set and measure its accuracy with the test set. We do this starting with a training set of size 1, and increasing one at a time up to the size 99. For each size we actually repeat the process of randomly splitting 20 times. The curve shows that as the training set grows, the accuracy increases. (For this reason, the learning curves are also called happy graphs). Choosing Attribute Sets A perfect attribute divides the examples into sets, each of which are all positive or negative and thus will be leaves of the tree. All we need is a formal measure of “fairly good” and “really useless” and we can implement the IMPORTANCE function. We will use the notion of information gain, which is defined in terms of entropy, the fundamental quantity in information theory. Entropy is a measure of the uncertainty of a random variable; acquisition of information corresponds to a reduction in entropy. In general, the entropy of a random variable 𝑉 with values 𝑣𝑘, each with probability 𝑃(𝑣𝑘), is defined as Entropy: 𝐻(𝑉) = ∑ 𝑃(𝑣𝑘) log2 1 𝑃(𝑣𝑘) = − ∑ 𝑃(𝑣𝑘) log2 𝑃(𝑣𝑘) 𝑘𝑘 For Example We can check that the entropy of a fair coin flip is indeed 1 bit: 𝐻(𝑓𝑎𝑖𝑟) = −(0.5 log2 0.5 + 0.5 log2 0.5) = 1 If the coin is leaded to give 99% heads, we get 𝐻(𝑙𝑜𝑎𝑑𝑒𝑑) = −(0.99 log2 0.99 + 0.01 log2 0.1) ≈ 0.08 Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 21 | P a g e If a training set contains 𝑝 positive examples and 𝑛 negative examples, then the entropy of the goal attribute on the whole set is 𝐻(𝑔𝑜𝑎𝑙) = 𝐵 ( 𝑝 𝑝 + 𝑛) An attribute A with 𝑑 distinct values divides the training set 𝐸 into subsets 𝐸1, . . . , 𝐸𝑑. Each subset 𝐸𝑘 has 𝑝𝑘 positive examples and 𝑛𝑘 negative examples, so if we go along that branch, we will need an additional 𝐵(𝑝𝑘/(𝑝𝑘 + 𝑛𝑘)) bits of information to answer the question. A randomly chosen example from the training set has the 𝑘th value for the attribute with probability (𝑃𝑘 + 𝑛𝑘)/(𝑝 + 𝑛), so the expected entropy remaining after testing attribute A is: 𝑅𝑒𝑚𝑎𝑖𝑛𝑑𝑒𝑟(𝐴) = ∑ 𝑝𝑘 + 𝑛𝑘 𝑝 + 𝑛 𝐵( 𝑝𝑘 𝑝𝑘 + 𝑛𝑘 𝑑 𝑘=1 ) The information gain from the attribute test on A is the expected reduction in entropy: 𝐺𝑎𝑖𝑛(𝐴) = 𝐵 ( 𝑝 𝑝 + 𝑛) − 𝑅𝑒𝑚𝑎𝑖𝑛𝑑𝑒𝑟(𝐴) In fact, 𝐺𝑎𝑖𝑛(𝐴) is just what we need to implement the IMPORANTCE function. For Example Returning to the attributes considered in Figure 18.4, we have 𝐺𝑎𝑖𝑛(𝑝𝑎𝑡𝑟𝑜𝑛𝑠) = 1 − [ 2 12 𝐵 ( 0) + 4 12 𝐵 (4) + 6 12 𝐵 (2)] ≈ 0.541 𝐵𝑖𝑡𝑠 𝐺𝑎𝑖𝑛(𝑇𝑦𝑝𝑒) = 1 − [ 2 12 𝐵 ( 1) + 2 12 𝐵 ( 1) + 4 12 𝐵 (2) + 4 12 𝐵 ( 2)] = 0 𝐵𝑖𝑡𝑠 Confirming that our intuition that 𝑃𝑎𝑡𝑟𝑜𝑛𝑠 is a better attribute to split on. Generalizing and Over-fitting On some problems,. The DECISION-TREE LEARNING algorithm will generate a large tree when there is actually no pattern to be found. This problem is called over-fitting. A general phenomenon, over-fitting occurs with all types of learners, even when the target function is not at all random. Over-fitting becomes more likely as the hypothesis space and the number of input attributes grows, and less likely as we increase the number of training examples. For decision trees, a technique called decision tree pruning combats over-fitting. Pruning works by eliminating nodes that are not clearly relevant. If the test appears to be irrelevant, detecting only noise in the data, then we eliminate the test, replacing it with a leaf node. How do we detect that a node is testing an irrelevant attribute? Suppose we are at a node consisting of 𝑝 positive and 𝑛 negative examples. If the attribute is irrelevant, we would expect that it would split the examples into subsets that each have roughly the same proportion of positive examples as the whole set, 𝑝/(𝑝 + 𝑛), and so the information gain will be close to zero. How large a gain should we require in order to split on particular attribute? Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 22 | P a g e We can answer this question by using a statistical significance test. Such a test begins by assuming that there are no underlying pattern (the so called null-hypothesis). Then the actual data are analysed to calculate the extent to which they deviate from a perfect absence of pattern. If a degree of deviation is statistically unlikely (usually taken to mean a 5% probability or less), then that is considered to be good evidence for the presence of a significant pattern in the data. The probabilities are calculated from standard distributions of the amount of deviation one would expect to see random sampling. In this case the null hypothesis is that the attribute is irrelevant and that the information gain for and infinitely large sample would be zero. We need to calculate the probability that, under the null hypothesis, a sample of size 𝑣 = 𝑛 + 𝑝 would exhibit the observed deviation from the expected distribution of positive and negative examples. We can measure the actual numbers of positive and negative examples in each subset, 𝑝𝑘 and 𝑛𝑘, with the expected numbers, 𝑝̂𝑘 and 𝑛̂𝑘, assuming true irrelevance: 𝑝̂𝑘 = 𝑝 × 𝑝𝑘 + 𝑛𝑘 𝑝 + 𝑛 𝑛̂𝑘 × 𝑝𝑘 + 𝑛𝑘 𝑝 + 𝑛 A convenient measure of the total deviation is given by: △= ∑ (𝑝𝑘 − 𝑝̂𝑘)2 𝑝̂𝑘 + (𝑛𝑘 − 𝑛̂𝑘)2 𝑛̂𝑘 𝑑 𝑘=1 Under the null hypothesis, the value of △ is distributed according to the 𝑋2 (chi Squared) distribution with 𝑣 − 1 degrees of freedom. Exercise 18.8 asks you to extend the DECISION-TREE-LEARNING algorithm to implement this form of pruning, which is known as 𝑋2 pruning. Broadening the applicability of decision trees In order to extend decision tree induction to a wider variety of problem, a number of issues must be addressed. We will briefly mention several, suggesting that a full understanding is best obtained by doing the associated exercises: - Missing Data – In many domains, not all the attribute values will be known for every example. This gives arise to two problems: 1st, given a complete decision tree, how should one classify an example that is missing one of the test attributes? 2nd, how should one modify the information-gain formula? - Multivalued Attributes – When an attribute has many possible values, the information gain measure gives and inappropriate indication of the attribute’s usefulness. One solution is to use the gain ratio (exercise 18.10). Another possibility is to allow a Boolean test of the form 𝐴 = 𝑣𝑘 that is, picking out just one of the possible values for an attribute, leaving the remaining values to possibly be tested later in the tree. - Continuous and Integer-Valued input attributes – These kinds of attributes such as 𝐻𝑒𝑖𝑔ℎ𝑡 and 𝑊𝑒𝑖𝑔ℎ𝑡 have an infinite set of possible values. Rather than generate infinitely many branches, decision tree learning algorithms typically find the split point that gives the highest information gain. Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 23 | P a g e For Example At a given node in the tree, it might be the case that testing on 𝑊𝑒𝑖𝑔ℎ𝑡 > 160 given the most information. Efficient methods exist for finding good split point: start by sorting the values of the attribute, and then consider only split points that are between two examples in sorted order that have different classifications, while keeping track of the running totals of positive and gative examples on each side of the split point. Continuous-valued output attributes – if we are trying to predict a numerical output value, then we need a regression tree rather than a classification tree. This tree has at each leaf a linear function of some subset of numerical attributes, rather than a single value. For Example The branch for two-bedroom apartments might end with a linear function of square footage, number of bathrooms, and average income for the neighbourhood. The learning algorithm must decide when to stop splitting and begin applying linear regression over the attributes. Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 24 | P a g e Ch. 4 – Beyond Classical Search Local Search Algorithms and Optimization Problems When a goal is found, the path to that goal also becomes the solution to the problem. I many problems, however, the path to the goal is irrelevant. Local Search algorithms operate using a single current node (rather than multiple paths) and generally move only to neighbours of that node. Although local search algorithms are not systematic, they have two key advantages 1. They use very little memory – usually a constant amount 2. They can often find reasonable solution in large or infinite state spaces for which systematic algorithms are unsuitable. Local search algorithms are useful for solving pure optimization problems, in which the aim is to find the best state according to an objective function. First consider the state space landscape. A landscape has both “location” (state) and “elevation) defined by the heuristic cost function or objective function). Aim is to find the lowest valley – a global minimum. If elevation corresponds to an object function, then the aim is to find the highest peak – a global maximum. complete local search algorithm always finds a goal if one exists; an optimal algorithm always finds a global minimum/maximum. Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 25 | P a g e Hill-Climbing Search (Hill Climbing Search – Steepest Ascent version) It is simply a loop that continually moves in the direction of increasing value. It terminates when it reaches a peak. Hill-Climbing search is sometimes called greedy local search because it grabs a good neighbour state without thinking about where to go next. Hill-Climbing Search often gets stuck for the following reasons: - Local Maxima: A local maximum is a peak that is higher than each of its neighbouring states, but lower than the global maximum. - Ridges: These result in a sequence of local maxima that is very difficult for greedy algorithms to navigate. - Plateaux: A plateau is a flat area of the state-space landscape. When an algorithm reaches a plateau, is it a good idea to allow sideways movement in the hope that the plateau is really a shoulder? The answer is usually yes, but if we always allow sideways movement where there are no uphill moves, an infinite loop will occur. Many Variations of hill-climbing have been invented: 1. Stochastic hill climbing chooses at random from among the uphill moves; the probability of selection can vary with the steepness of the uphill move. 2. First choice hill climbing implements stochastic hill climbing by generating successors randomly until one is generated that is better than the current state. 3. Random-restart hill climbing conducts a series of hill climbing searches from randomly generated initial states, until a goal is found. mulated Annealing It seems reasonable to combine hill climbing with a random walk in some way that yields both efficiency and completeness. Simulated annealing is such an algorithm. To explain this we switch our point of view from hill climbing to gradient descent and imagine the task of getting a Ping-Pong ball into the deepest crevice in a bumpy surface. If we just let the ball roll, it will come to rest at a local minimum. If we shake the surface, we can bounce the ball out of the local minimum. The trick is to shake just hard enough to bounce the ball out of a local minimum, but not hard enough to dislodge it from the global minimum. The simulated-annealing solution is to start by shaking hard, and then gradually reduce the intensity. Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 26 | P a g e Local Beam Search The local beam search algorithm keeps track of 𝑘 states rather than just one. It begins with 𝑘 randomly generated states. At each step, all the successors of all 𝑘 states are generated. If anyone is a goal, the algorithm halts, otherwise, it selects the 𝑘 best successors from the complete list. In a random restart search, each search process runs independently of the others. In a local beam search, useful information is passed among the parallel search threads. Local beam search can suffer from a lack of diversity among the 𝑘 states. A variant called stochastic beam search, analogous to stochastic hill climbing, help alleviate this problem. Instead of choosing the best 𝑘 from the pool of candidate successors, stochasitc beam search chooses 𝑘 successors at random, with the probability of choosing a given successor being an increasing function of its value. Genetic Algorithms genetic algorithm (or GA) is a variant of stochastic beam search in which successor states are generated by combining two parent states rather than by modifying a single state. Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 27 | P a g e GAs begin with a set of 𝑘 randomly generated states, called the population. Each state, or individual, is represented as a string over a finite alphabet – most commonly, a string of 1s and 0s. For Example An 8-queens state must specify the positions of 8 queens, each in a column of 8 squares, and so requires 8 × log2 8 = 24 bits. In (b), each state is rated by the objective function, the fitness function. A fitness function should return higher values for better states, so, for the 8-queens problem, we use the number of nonattacking pairs of queens, which has a value of 28 for a solution. In (c) two pairs are selected at random from reproduction, in accordance with the probabilities in (b). Notice that the individual is selected twice and one not at all. For each pair to be mated, a crossover point is chosen randomly from the positions of the string. In (d) the offspring themselves are created by crossing over the parent strings at the crossover point. For example The 1st child of the 1st pair gets the 1st three digits from the 1st parent and remaining digits from the 2nd parent, whereas the 2nd child gets the first three digits from the 2nd parent and the rest from the 1st parent. Finally, in (e) each location is subject to random mutation with a small independent probability. Like stochastic mean search, genetic algorithms combine an uphill tendency with random exploration and exchange of information among parallel search treads. The primary advantage of GAs comes from the crossover operations. The theory of genetic algorithms explains how this works using the idea of a schema, which is a substring in which some of the positions can be left unspecified. For Example Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 28 | P a g e The schema 2468***** describes all 8-queens states in which the first 3 queens are in positions 2, 4 and 6, respectively. Strings that match the schema (such as 24613578) are called instances of the schema. Local Search in Continuous Spaces We begin with an example. For Example: Suppose we want to place three new airports anywhere in Romania, such that the sum of the squared distances from each city on the map to its nearest airport is minimized. The state space is then defined by the coordinates of the airports: (𝑥1, 𝑦1), (𝑥2, 𝑦2) and (𝑥3, 𝑦3). This is a six dimensional space; we also say that states are defined by 6 variables. Let 𝐶𝑖 be the set of cities whose closest airport is airport 𝑖. Then, in the neighbourhood of the current state, where the 𝐶𝑖s remain constant, we have: Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 29 | P a g e 𝑓(𝑥1, 𝑦1, 𝑥2, 𝑦2, 𝑥3𝑦3) = ∑ ∑ (𝑥𝑖 − 𝑥𝑐)2 + (𝑦𝑖 − 𝑦𝑐)2 𝑐∈𝐶𝑖 3 𝑖=1 4.1 This expression is correct locally, but nor globally because the sets 𝐶𝑖 are (discontinuous) functions of the state. One way to avoid continuous problems is simply to discretize the neighbourhood of each state. For Example: We can only move one airport at a time in either the 𝑥 or 𝑦 direction by a fixed amount ±𝛿. With 6 variables, this gives 12 possible successors for each state. We can then apply any of the local search algorithms described previously. We could also apply sophisticated hill climbing and simulated annealing directly, without discretizing the space. These algorithms choose successors randomly, which can be done by generating random vectors of length 𝛿. Many methods attempt to use the gradient of the landscape to find the maximum. The gradient of the objective function is a vector ∇𝑓 that gives the magnitude and direction of the steepest slope. For our problem, we have: ∇𝑓 = ( 𝜗𝑓 𝜗𝑥1 , 𝜗𝑓 𝜗𝑦1 , 𝜗𝑓 𝜗𝑥2 , 𝜗𝑓 𝜗𝑦2 , 𝜗𝑓 𝜗𝑥3 , 𝜗𝑓 𝜗𝑦3) In some cases, we can find a maximum by solving the equations ∇𝑓 = 0. In many cases, however, this equation cannot be solved in closed form. For example, with 3 airports, the expression for the gradient depends on what cities are closest to each airport in the current state. This means we can compute the gradient locally but not globally; For example: 𝜗𝑓 𝜗𝑥1 = 2 ∑ (𝑥𝑖 − 𝑥𝑐) 𝑐∈𝐶1 Given a locally correct expression for the gradient, we can perform steepest-ascent hill climbing by updating the current state according to the formula: 𝒙 ← 𝒙 = 𝛼∇𝑓(𝒙) Where 𝛼 is a small constant often called the step size. In other cases, the objective function might not be available in differentiable form at all. For example, the value of a particular set of airport locations might be determined by running some large-scale economic simulation package. In those cases, we can calculate a so called empirical gradient by evaluating the response to small increments and decrements in each coordinate. Empirical gradient search is the same as steepest ascent hill climbing in a discretized version of the state space. Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 30 | P a g e There are a huge variety of methods for adjusting 𝛼. The basic problem is that, if 𝛼 is too small, too many steps are needed; if it’s too big, the search could overshoot the maximum. The technique of line search tries to overcome this dilemma by extending the current gradient direction – usually by repeatedly doubling 𝛼 – until 𝑓 starts to decrease again. For many problems the most effective algorithm is the venerable Newton-Raphson method. This is a general technique for solving equations of the form 𝑔(𝑥) = 0. It works by computing a new estimate for the root 𝑥 according to Newton’s Formula: 𝑥 ← 𝑥 − 𝑔(𝑥) 𝑔′(𝑥) To find the max or min of 𝑓, we need to find 𝑥 such that the gradient is 0. Thus, 𝑔(𝑥) in Newton’s formula becomes ∇𝑓(𝑥), and the update equation can be written in matrix-vector form as: 𝒙 ← 𝒙 − 𝑯𝑓 −1(𝒙)∇𝑓(𝒙) Where 𝑯𝑓(𝒙) is the Hessian matrix of second derivatives, whose elements 𝐻𝑖𝑗 are given by 𝜕2𝑓 𝜕𝑥𝑖𝜕𝑥𝑗 For Example: For our airport example, we can see from Equation (4.2) that 𝐻𝑓 (𝑥) is particularly simple: the off- diagonal elements are zero and the diagonal elements for airport 𝑖 are just twice the number of cities in 𝐶𝑖. A moment’s calculation shows that one step of the update moves airport 𝑖 directly to the centroid of 𝐶𝑖, which is the minimum of the local expression for f from Equation (4.1). A final topic, with which a passing acquaintance is useful, is constrained optimization. An optimization problem is constrained if solutions must satisfy some hard constraints on the values of the variables. The difficulty of constrained optimization problems depends on the nature of the constraints and the objective function. The best-known category of that is linear programming problems, in which constraints must be linear inequalities forming a convex set and the objective function is also linear. Linear Programming is a special case of the more general problem of convex optimization, which allows the constraint region to be any convex region and the objective to by any function that is convex within the constraints region. Searching with Nondeterministic Actions The future percepts cannot be determined in advance and the agent’s future actions will depend on those future percepts. So the solution to a problem is not a sequence but a contingency plan (also known as a strategy) that specifies what to do depending on what percepts are received. Instead of defining the transition model by a RESULT function that returns a single state, we use a RESULTS function that returns a set of possible outcome states. [𝑆𝑢𝑐𝑘, 𝐢𝐟 𝑆𝑡𝑎𝑡𝑒 = 5 𝐭𝐡𝐞𝐧 [𝑅𝑖𝑔ℎ𝑡, 𝑆𝑢𝑐𝑘] 𝐞𝐥𝐬𝐞 [. . . ]] Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 31 | P a g e Solutions for nondeterministic problems can contain nested if-then-else statements; this means that they are trees rather than sequences. AND-OR Search Trees In a deterministic environment, the only branching is introduced by the agent’s own choices in each state. We call these nodes OR nodes. In a nondeterministic environment, branching is also introduced by the environment’s choice of outcome for each action. We call these nodes AND nodes. One may also consider a somewhat different agent design, in which the agent can act before it has found a guaranteed plan and deals with some contingencies only as they arise during execution. This type of interleaving of search and execution is also useful for exploration problems and for game playing. Try, try Again There is a cyclic solution, which is to keep trying until it works. We can express this solution by adding a label to denote some portion of the plan and using that label later instead of repeating the plan itself. [𝑆𝑢𝑐𝑘, 𝐿1: 𝑅𝑖𝑔ℎ𝑡, 𝐢𝐟 𝑆𝑡𝑎𝑡𝑒 = 5 𝐭𝐡𝐞𝐧 𝐿1 𝐞𝐥𝐬𝐞 𝑆𝑢𝑐𝑘] A better syntax for the looping part of this plan would be “ while State = 5 do Right”. Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 32 | P a g e Searching With Partial Observations The key concept required for solving partially observable problems is the belief state, representing the agent’s current belief about the possible physical states it might be in, given the sequence of actions and percepts up to that point. Searching with no observation When the agent’s percepts provide no information at all, we have what is called a sensorless problem or sometimes a conformant problem. Sensorless agents can be surprisingly useful, primarily because they don’t rely on sensors working properly. To solve sensorless problems, we search in the space of belief states, rather than physical states. Notice that in belief-state space, the problem is fully observable because the agent always knows its own belief state. It is instructive to see how the belief state search problem is constructed. Suppose the underlying physical problem 𝑃 is defined by ACTIONSp, RESULTp, GOAL − TESTp and STEP − COSTp. Then we define the corresponding sensorless problems as follows: - Belief states – The entire belief state space contains every possible set of physical states. If 𝑃 has 𝑁 states, then the sensorless problem has up to 2𝑁 states. - Initial State – Typically the set of all states in 𝑃, although in some cases the agent will have more knowledge than this. - Actions – Suppose the agent is in belief state 𝑏 = {𝑠1, 𝑠2} but ACTIONSp(s1) ≠ ACTIONSp(s2); then the agent is unsure of which actions are legal. If we assume that illegal actions have no effect on the environment, then it is safe to take the union of all the actions in any of the physical states in the current belief state 𝑏: ACTIONS(b) = ⋃ ACTIONSp(s) s∈b On the other hand, if an illegal action might be the end of the world, it is safer to allow only the intersection, that is, the set of actions that is legal in all states. - Transition model – The agent doesn’t know which state in the belief state is the right one; so as far as it knows, it might get to any of the states resulting from applying the action to one of the physical states in the belief state. For deterministic action, the set of states that might be reached is 𝑏′ = 𝑅𝐸𝑆𝑈𝐿𝑇(𝑏, 𝑎) = {𝑠′: 𝑠′ = 𝑅𝐸𝑆𝑈𝐿𝑇𝑝(𝑠, 𝑎)𝑎𝑛𝑑 𝑠 ∈ 𝑏} 4.4 With deterministic actions, 𝑏′ is never larger than 𝑏. With nondeterminism, we have 𝑏′ = 𝑅𝐸𝑆𝑈𝐿𝑇(𝑏, 𝑎) = {𝑠′: 𝑠′ ∈ 𝑅𝐸𝑆𝑈𝐿𝑇𝑝(𝑠, 𝑎)𝑎𝑛𝑑 𝑠 ∈ 𝑏} 4.4 = ⋃ 𝑅𝐸𝑆𝑈𝐿𝑇𝑆𝑝(𝑠, 𝑎) 𝑠∈𝑏 Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 33 | P a g e Which may be larger than 𝑏. The process of generating the new belief state after the action is called the prediction step; the notation 𝑏′ = 𝑃𝑅𝐸𝐷𝐼𝐶𝑇𝑝(𝑏, 𝑎) will come in handy. - Goal Test – The agent wants a plan that is sure to work, which means that a belief state satisfies the goal only if 𝑎𝑙𝑙 the physical states in it satisfy 𝐺𝑂𝐴𝐿 − 𝑇𝐸𝑆𝑇𝑝. The agent may have accidentally achieved its goal earlier, but it won’t know that it has done so. - Path Cost – This is also tricky. If the same action can have different costs in different states, then the cost of taking an action in a given belief state could be one of several values. For now we assume the cost of an action is the same in all states. The preceding definitions enable the automatic construction of the belief state problem formulation form the definition of the underlying physical problem. Once this is done, we can apply any of the search algorithms of Chapter 3. Even with this improvement, however, sensorless problem-solving as we have described it is seldom feasible in practice. The difficulty is not so much the vastness of the belief state space; in most cases the branching factor and solution length in the belief state space and physical state space are not so different. The real difficulty lies with the size of each belief state. One solution is to represent the belief state by some more compact description. Another approach is to avoid the standard search algorithms, which treat belief states as black boxes. Instead, we can look inside the belief states and develop incremental belief-state search algorithms that build up the solution one physical state at a time. For Example: Suppose the initial belief state is {1, 2, 3, 4, 5, 6, 7, 8} and we have to find an action sequence that works in all 8 states. We can do this by 1st finding a solution that works for state 1: then we check if it works for state 2; if not we go back and find a different solution for state 1. The main advantage of the incremental approach is that it is typically able to detect failure quickly, when a belief state is unsolvable. Searching with observations For a general partially observable problem, we have to specify how the environment generates percepts for the agent. The formal problem specification includes a PERCEPT(s) function that returns the percept received in a given state. When observations are partial, it will usually be the case that several states could have produced any given percept. The ACTIONS, STEP-COST and GOAL-TEST are constructed from the underlying physical problem just as for sensorless problems, but the transition model is a bit more complicated. We can think of transitions from one belief state to the next for a particular action as occurring in three stages: Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 34 | P a g e - The prediction stage is the same as for sensorless problems: given the action 𝑎 in a belief state 𝑏, the predicted belief state is 𝑏̂ = 𝑃𝑅𝐸𝐷𝐼𝐶𝑇(𝑏, 𝑎). - The observation prediction state determines the set of percepts 𝑜 that could be observed in the predicted belief state: 𝑃𝑂𝑆𝑆𝐼𝐵𝐿𝐸 − 𝑃𝐸𝑅𝐶𝐸𝑃𝑇𝑆(𝑏̂) = {𝑜: 𝑜 = 𝑃𝐸𝑅𝐶𝐸𝑃𝑇𝑆(𝑠) 𝑎𝑛𝑑 𝑠 ∈ 𝑏̂} - The update stage determines, for each possible percept, the belief state that would result from the percept. The new belief state 𝑏0 is just the set of states in 𝑏̂ is just the set of states in 𝑏̂ that could have produced the percept: 𝑏0 = 𝑈𝑃𝐷𝐴𝑇𝐸(𝑏̂, 𝑜) = {𝑠: 𝑜 = 𝑃𝐸𝑅𝐶𝐸𝑃𝑇(𝑠) 𝑎𝑛𝑑 𝑠 ∈ 𝑏̂} Notice that each updated belief state 𝑏0 can be no larger than the predicted belief state 𝑏̂; Observations can only help reduce uncertainty compared to the sensorless case. Moreover, for deterministic sensing, the belief states for the different possible percepts will be disjoint, forming a partition of the original predicted belief state. Putting these three stages together, we obtain the possible belief states resulting from a given action and the subsequent possible percepts: 𝑅𝐸𝑆𝑈𝐿𝑇𝑆(𝑏, 𝑎) = {𝑏𝑜: 𝑏𝑜 = 𝑈𝑃𝐷𝐴𝑇𝐸(𝑃𝑅𝐸𝐷𝐼𝐶𝑇(𝑏, 𝑎), 𝑜) 𝑎𝑛𝑑 𝑜 ∈ 𝑃𝑂𝑆𝑆𝐼𝐵𝐿𝐸 − 𝑃𝐸𝑅𝐶𝐸𝑃𝑇𝑆(𝑃𝑅𝐸𝐷𝐼𝐶𝑇(𝑏, 𝑎))} 4.5 Solving partially observable problems The preceding section showed how to derive the RESULTS function for a nondeterministic belief- state problem from an underlying physical problem and the PERCEPT function. Given such a formulation, the AND-OR search algorithm can be applied directly to derive a solution. Figure 4.16 – p. 145 – The 1st level of the AND-OR search tree for a problem in the local- sensing vacuum work; Notice that, because we supplied a belief state problem to the AND-OR search algorithm, it returned a conditional plan that tests the belief state rather than the actual state. In a partially observable environment the agent won’t be able to execute a solution that requires testing the actual state. An Agent for partially observable environments The design of a problem-solving agent for a partially observable environments is quite similar to the simple problem solving agent in Figure 3.1: The agent formulates a problem, calls a search algorithm to solve it and executes the solution. There are two main differences: 1. The solution to a problem will be conditional plan rather than a sequence; if the first step is an if-then-else expression, the agent will need to test the condition in the if-part and execute the then-part of the else-part accordingly. 2. The agent will need to maintain its belief state as it performs actions and receives percepts. Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 35 | P a g e This process resembles the prediction-observation-update process in Equation 4.5 but is actually simpler because the percept is given by the environment rather than calculated by the agent. Given an initial belief state 𝑏, an action 𝑎, and a percept, 𝑜, the new belief state is: 𝑏′ = 𝑈𝑃𝐷𝐴𝑇𝐸(𝑃𝑅𝐸𝐷𝐼𝐶𝑇(𝑏, 𝑎), 𝑜) 4.6 In partially observable environments, which include the cast majority of real-world environments, maintaining one’s belief state is a core function of any intelligent system. The function goes under various names, including monitoring, filtering and state estimation. Equation 4.6 is called a recursive state estimator because it computes the new belief state from the previous one rather than by examining the entire percept sequence. For Example: The example concerns a robot with the task of localization: Working out where it is, given a map of the world and a sequence of percepts and actions. The robot is placed in a maze. The robot is equipped with 4 sonar sensors that tell whether there is an obstacle in each of the 4 compass directions. We assume the sensors give perfectly correct date, and that the robot has a correct map of the environment. But unfortunately the robot’s navigational system is broken, so when it executes a 𝑀𝑜𝑣𝑒 action, it moves randomly to one of the adjacent squares. The robots task is to determine its current location. Its initial belief state 𝑏 consists of the set of all locations. Then the robot receives the percept 𝑁𝑆𝑊, meaning there are obstacles to the north, west and south and does an update using the equation 𝑏𝑜 = 𝑈𝑃𝐷𝐴𝑇𝐸(𝑏). Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 36 | P a g e Online Search Agents and Unknown Environments Offline search algorithms compute a complete solution before setting foot in the real world and then execute the solution. Online search agent interleaves computation and action: 1st it takes an action, then observes the environment and computes the next action. Online search is a necessary idea for unknown environments, where the agent does not know what states exist or what its actions do. In this state of ignorance, the agent faces an exploration problem and must use its actions as experiments in order to learn enough to make deliberation worthwhile. Online search problems An online search problem must be solved by an agent executing actions, rather than by pure computation. We assume a deterministic and fully observable environment, but we stipulate that the agent knows only the following: - ACTIONS(s), which returns a list of actions allowed in state s; - The STEP_COST function 𝑐(𝑠, 𝑎, 𝑠′) – note that this cannot be used until the agent knows that 𝑠′ is the outcome - GOAL-TEST(s) Note in particular that the agent cannot determine 𝑅𝐸𝑆𝑈𝐿𝑇(𝑠, 𝑎) except by actually being in 𝑠 and doing 𝑎. For example: In the maze problem from figure 4.19, the agent does not know that going 𝑢𝑝 from (1, 1) leads to (1,2); nor, having done that, does it know that going 𝑑𝑜𝑤𝑛 will take it back to (1, 1). Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 37 | P a g e Finally, the agent might have access to an admissible heuristic function ℎ(𝑠) that estimates the distance from the current state to a goal state. Typically, the agent’s objective is to reach a goal state while minimizing the cost. The cost is the total path cost of the path that the agent actually travels. It is common to compare this cost with the path costs of the path the agent would follow if it know the search space in advance. In the language of online algorithms, this is called the competitive ratio; we would like it to be as small as possible. Our claim is that no algorithm can avoid dead ends in all state spaces. Consider the two dead end state spaces in the figure above. To an online search algorithm, states 𝑆 and 𝐴, look identical, so it must make the same decision in both. Therefore, it will fail in one of them. This is an example of an adversary argument we can imagine an adversary constructing the state space while the agent explores it and putting the goals and dead ends wherever it chooses. Dead ends are real difficult for robot exploration. To make progress, we simply assume that the state space is safely explorable – that is, some goal state is reachable from every reachable state. Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 38 | P a g e Online search algorithms After each action, an online agent receives a percept telling it what state it has reached; from this information, it can augment its map of the environment. The current map is decided where to go next. An online depth-first search agent is shown in Figure 4.21. This agent stores its map in a table, 𝑅𝐸𝑆𝑈𝐿𝑇[𝑠, 𝑎], that record the state resulting from executing action 𝑎 and 𝑠. Whenever an action from the current state has not been explored, the agent tries that action. The difficulty comes when the agent has tried all the actions in a state. In offline depth-first search, the state is imply dropped from the queue; in an online search, the agent has to backtrack physically. In depth-first search, this means going back to the state from which the agent most recently entered the current state. To achieve that, the algorithm keeps a table that list, for each state, the predecessor states to which the agent has not yet backtracked. Online Local Search Like depth-first search, hill-climbing search has the property of locality in its node expansions. In fact, it keeps just one current state in memory, hill climbing search is already an online search algorithm. Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 39 | P a g e Instead of random restarts, one might consider using a random walk to explore the environment. A random walk simply selects at random one of the available actions from the current state; preference can be given to actions that have not yet been tried. Random walks will eventually find a goal, provided that the space is finite. On the other hand, the process may be very slow. Augmenting hill climbing with memory rather than randomness turns out to be a more effective approach. The basic idea is to store a ‘current best estimate’ 𝐻(𝑠) of the cost to reach the goal from each state that has been visited. 𝐻(𝑠) starts out being just the heuristic estimate ℎ(𝑠) and is updated as the agent gains experience in the state space. In (a), the agent seems to be stuck in a flat local minimum at the shaded side. Rather than staying where it is, the agent should follow what seems to be the best path to the goal given the current cost estimates for its neighbours. The estimated cost to reach the goal through a neighbour 𝑠′is the cost to get to 𝑠′ plus the estimated cost to get to a goal from there – that is, 𝑐(𝑠, 𝑎, 𝑠′) + 𝐻(𝑠′). In the example there are two actions, with estimated costs 1 + 9 and 1 + 2, so it seems to move right. Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 40 | P a g e Learning in online search st the agent learns to map the environment – more precisely, the outcome of each action in each state. 2nd, the local search agents acquire more accurate estimates of the cost of each state by using local updating rules. Summary – p. 154 Exercises – p. 158 Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 41 | P a g e Ch. 5 – Adversarial Search Games Ch. 2 introduced multiagent environments, in which each agent needs to consider the actions of other agents and how they affect their own welfare. The unpredictability of these other agents can introduce contingencies into the agent’s problem-solving process, as discussed in ch. 4. In this chapter we cover competitive environments, in which the agents’ goals are in conflict, giving rise to adversarial search problems – often known as games. Mathematical game theory, a branch of economics, vies any multiagent environment as a game, provided that the impact of each agent on the others is “significant”, regardless of whether the agents are cooperative or competitive. In AI, the most games are of a rather specialised kind – what game theorists call deterministic, turn- taking, two player, zero-sum games of perfect information. In our terminology, this means deterministic, fully observable environments in two agents act alternately and in which the utility values at the end of the game are always equal and opposite. We begin with a definition of the optimal move and an algorithm for finding it. We then look at techniques for choosing a good move when time is limited. Pruning allows us to ignore portions of the search tree that make no difference to the final choice, and heuristic evaluation functions allow us to approximate the true utility of a state without doing a complete search. First we consider games with two players, whom we call MAX and MIN. MAX moves 1st, and then they take turns moving until the game is over. At the end of the game, points are awarded to the winning player and penalties are given to the loser. A game can be formally defined as a kind of search problem with the following elements: - 𝑆0: The initial state, which specifies how the game is set up at the start. - PLAYER(s): Defines which player has the move in a state. - ACTION(s): returns the set of legal moves in a state. - RESULT(s, a): The Transitional model, which defines the result of a move. - TERMINAL-TEST(s): A terminal test, which is true when the game is over and false otherwise. State where the game has ended are called terminal states. - UTILITY(s, p): a Utility Function (a.k.a. an objective function or a payoff function), defines the final numeric value for a game that ends in terminal state 𝑠 for a player 𝑝. In chess, the outcome is a win, loss, or draw with values +1, 0, or 1. A zero-sum game is defined as one where the total payoff to all players is the same for every instance of the game. Chess is a zero-sum because every game has payoff of either 0 +1, 1 + 0, or 1 + 1. “Constant sum” would have been a better term. The initial state, ACTIONS functions, and RESULT function define the game tree for the game, a tree where the nodes are game states and the edges are moves. Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 42 | P a g e From the initial state, MAX has 9 possible moves. Play alternatives between MAX’s placing an X and MIN’s placing an O until we reach leaf nodes corresponding to terminal states such that that one player has three in a row or all the squares are filled. The number of each leaf node indicates the utility value of the terminal state from the point of view of MAX; high values are assumed to be good for MAX and bad for MIN. We use the term search tree for a tree that is superimposed on the full game, and examines enough nodes to allow a player to determine what move to make. Optimal Decisions in Games In a normal search problem, the optimal solution would be a sequence of actions leading to a goal state, a terminal state that is a win. In adversarial search, MIN has something to say about it. MAX therefore must find a contingent strategy, which specifies MAX’s move in the initial state, then MAX’s moves in the states resulting from every possible response by MIN, then MAX’s move s in the states resulting from every possible response by MIN to those moves, and so on. This is exactly analogous to the AND-OR search algorithm. Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 43 | P a g e The possible moves for MAX at the root node are labelled 𝑎1, 𝑎2 and 𝑎3. The possible replies to 𝑎1 for MIN are 𝑏1, 𝑏2, 𝑏3 and so on. This particular game ends after one move each by MAX and MIN. (In game parlance, we say that this tree is one move deep, consisting of two half-moves, each of which is called a ply). Given a game tree, the optimal strategy can be determined from the minimax value of each node, which we write as MINIMAX(𝑛). The minimax value of a node is the utility (for MAX) of being in the corresponding sate, assuming that both players play optimally from there to the end of the game. Obviously, the minimax value of a terminal state is just its utility. Furthermore, given a choice, MAX prefers to move to a state of maximum value, whereas MIN prefers a state of minimum value. So we have the following: MINIMAX(𝑠) = { 𝑈𝑇𝐼𝐿𝐼𝑇𝑌(𝑠) 𝑖𝑓 𝑇𝐸𝑅𝑀𝐼𝑁𝐴𝐿 − 𝑇𝐸𝑆𝑇(𝑠) 𝑚𝑎𝑥𝑎∈𝐴𝑐𝑡𝑖𝑜𝑛𝑠(𝑠)𝑀𝐼𝑁𝐼𝑀𝐴𝑋(𝑅𝐸𝑆𝑈𝐿𝑇(𝑠, 𝑎)) 𝑖𝑓 𝑃𝐿𝐴𝑌𝐸𝑅(𝑠) = 𝑀𝐴𝑋 min𝑎∈𝐴𝑐𝑡𝑖𝑜𝑛(𝑠) 𝑀𝐼𝑁𝐼𝑀𝐴𝑋(𝑅𝐸𝑆𝑈𝐿𝑇(𝑠, 𝑎)) 𝑖𝑓 𝑃𝐿𝐴𝑌𝐸𝑅(𝑠) = 𝑀𝐼𝑁 For Example: Let us apply these definitions to the game tree in Figure 5.2. The terminal nodes on the bottom level get their utility values from the game’s UTILITY function. The 1st MIN node, labelled 𝐵, has three successor states with values 3, 12, and 8, so its minimax value is 3. Similarly, the other two MON nodes have minimax value 2. The root node is a MAX node; its successor states have minimax values 3, 2 and 2; so it has a minimax value of 3. We can also identify the minimax decision at the root; action 𝑎1 is the optimal choice for MAX because it leads to the state with the highest minimax value. This definition of optimal play for MAX assumes that MIN also plays optimally, it maximizes the worst-case outcome for MAX. Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 44 | P a g e The minimax algorithm This algorithm computes the minimax decision from the current state. It uses a simple recursive computation of the minimax values of each successor state, directly implementing the defining equations. The recursion proceeds all the way down to the leaves of the tree, and then the minimax values are backed up through the tree as the recursion unwinds. For Example: In figure 5.2, the algorithm 1st recurses down to the three bottom left nodes and uses the UTILITY function on them to discover that their values are 3, 12, and 8, respectively. Then it takes the minimum of these values, 3, and returns it as the backed up value of B. The minimax algorithm performs a complete depth-first exploration of the game tree. If the maximum depth of the tree is 𝑚 and there are 𝑏 legal moves at each point, then the time complexity of the minimum algorithm is 𝑂(𝑏𝑚). The space complexity is 𝑂(𝑏𝑚) for an algorithm that generates all actions at once, or 𝑂(𝑚) for an algorithm that generates actions one at a time. Optimal Decisions in multiplayer games st, we need to replace the single value for each node with a vector of values. For Example: In a three player game with players A, B and C, a vector 〈𝑣𝐴, 𝑣𝐵, 𝑣𝐶〉 is associated with each node. Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 45 | P a g e For terminal states, this vector gives the utility of the state from each player’s viewpoint. The simplest way to implement this is to have the UTILITY function return a vector of utilities. Consider the node marked X in the game tree in Figure 5.4. In that state, player 𝐶 chooses what to do. The two choices lead to terminal states with utility vectors 〈𝑣𝐴 = 1, 𝑣𝐵 = 2, 𝑣𝐶 = 6〉 and 〈𝑣𝐴 = 4, 𝑣𝐵 = 2, 𝑣𝐶 = 3〉. Since 6 is bigger than 3, 𝐶 should choose the first move. This means that if state 𝑋 is reached, subsequent play will lead to a terminal state with utilities 〈𝑣𝐴 = 1, 𝑣𝐵 = 2, 𝑣𝐶 = 6〉. Hence the backed up value of 𝑋 is this vector. The backed-up value of a node 𝑛 is always the utility vector of the successor state with the highest value for the player choosing at 𝑛. Multiplayer games usually involve alliances, whether formal or informal, among the players. Alliances are made and broken as the game proceeds. Alpha-Beta Pruning The problem with minimax is that the number of game states it has to examine is exponential in the depth of the tree. Unfortunately, we can’t eliminate the exponent, but we can effectively cut it in half. The trick is that it is possible to compute the correct minimax decision without looking at every node in the game tree. That is, we can borrow the idea of pruning from chapter 3 to eliminate large parts of the tree from consideration. The particular technique we examine is called alpha-beta pruning. When applied to a standard minimax tree, it returns the same move as minimax would, but prunes away branches that cannot possible influence the final decision. Consider Figure 5.2. The steps are explained in Figure 5.5. The outcome is that we can identify the minimax decision without ever evaluating two of the leaf nodes. Another way to look at this is as a simplification of the formula for MINIMAX. Let the two unevaluated successors of node 𝐶 in Figure 5.5 have values 𝑥 and 𝑦. Then the valye of the root node is given by Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 46 | P a g e 𝑀𝐼𝑁𝐼𝑀𝐴𝑋(𝑟𝑜𝑜𝑡) = max(𝑚𝑖𝑛(3, 12, 8), min(2, 𝑥, 𝑦) , min(14, 5, 2)) = max (3, min(2, 𝑥, 𝑦) , 2) = max(3, 𝑧, 2) 𝑤ℎ𝑒𝑟𝑒 𝑧 = min(2, 𝑥, 𝑦) ≤ 2 = 3 In other words, the value of the root and hence the minimax decision are independent of the values of the pruned leaves 𝑥 and 𝑦. Alpha beta pruning can be applied to trees of any depth, and it is often possible to prune entire subtrees rather than just leaves. The general principle is this: consider a node 𝑛 somewhere in the tree (Figure 5.6), such that Player has a choice of moving to that node. If Player has a better choice either at the parent code of 𝑛 or at any choice point further up, then 𝑛 will never be reached in actual play. So once we have found out enough about 𝑛 to reach this conclusion, we can prune it. Alpha beta pruning gets its name from the following two parameters that describe bounds on the backed up values that appear anywhere along the path: 𝛼 = the value of the best (i.e. highest value) choice we have found so far at any choice point along the path for MAX. 𝛽 = The value of the best (i.e. lowest-value) choice we have found so far at any choice point along the path for MIN. Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 47 | P a g e Move Ordering The effectiveness of alpha-beta pruning is highly dependent on the order in which the states are examined. For example: In Figure 5.5 E and F, we could not prune any successors of 𝐷 at all because the worst successors were generated 1st. If the 3rd successor of 𝐷 had been generated 1st, we would have been able to prune the other two. This suggests that it might be worthwhile to try to examine 1st the successors that are likely to be best. If this can be done, then it turns out that aplha-beta needs to examine only 𝑂(𝑏𝑚/2) nodes to pick the best move, instead of 𝑂(𝑏𝑚) for minimax. This means that the effective branching factor becomes √𝑏 instead of 𝑏. Adding dynamic move-ordering schemes, such as trying 1st the moves that were found to be best in the past, brings us quite close to the theoretical limit. As we saw in chapter 3, iterative deepening on an exponential game tree adds only a constant fraction to the total search time, which can be more than made up from better move ordering. The best moves are often called killer moves and to try them1st is called the killer move heuristic. In Chapter 3, we notes that repeated states in the search tree can cause an exponential increase in search cost. In many games, repeated states occur frequently because of transpositions – different permutations of the move sequence that end up in the same position. Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 48 | P a g e For example: If white has one move, 𝑎1, that can be answered by black with 𝑏1 and an unrelated move 𝑎2 on the other side of the board that can be answered by 𝑏2, then the sequences 𝑎1, 𝑏1, 𝑎2, 𝑏2 and 𝑎2, 𝑏2, 𝑎1, 𝑏1 both end up in the same position It is worthwhile to store the evaluation of the resulting position in a hash table the 1st time it is encountered so that we don’t have to recompute it on subsequent occurrences. The hash table of previously seen positions is traditionally called a transposition table; it is essentially identical to the explored list in GRAPH-SEARCH. Imperfect Real Time Decisions The minimax algorithm generates the entire game search space, whereas the alpha-beta algorithm allows us to prune large parts of it. However, alpha beta still has to search all the way to terminal states for at least a portion of the search space. The suggestion is to alter minimax or alpha beta in two ways: - Replace the utility function by a heuristic evaluation function EVAL, which estimates the position’s utility Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 49 | P a g e - Replace the terminal test by a cut-off test that decides when to apply EVAL. That gives us the following heuristic minimax for state 𝑠 and maximum depth 𝑑: 𝐻 − 𝑀𝐼𝑁𝐼𝑀𝐴𝑋(𝑠, 𝑑) = { 𝐸𝑉𝐴𝐿(𝑠)𝑖𝑓 𝐶𝑈𝑇𝑂𝐹𝐹 − 𝑇𝐸𝑆𝑇(𝑠, 𝑑) 𝑚𝑎𝑥𝑎∈𝐴𝑐𝑡𝑖𝑜𝑛(𝑠)𝐻 − 𝑀𝐼𝑁𝐼𝑀𝐴𝑋(𝑅𝐸𝑆𝑈𝐿𝑇𝑆(𝑠, 𝑎), 𝑑 + 1) 𝑖𝑓 𝑃𝐿𝐴𝑌𝐸𝑅(𝑠) = 𝑀𝐴𝑋 𝑚𝑖𝑛𝑎∈𝐴𝑐𝑡𝑖𝑜𝑛(𝑠)𝐻 − 𝑀𝐼𝑁𝐼𝑀𝐴𝑋(𝑅𝐸𝑆𝑈𝐿𝑇𝑆(𝑠, 𝑎), 𝑑 + 1) 𝑖𝑓 𝑃𝐿𝐴𝑌𝐸𝑅(𝑠) = 𝑀𝐼𝑁 Evaluation Functions An evaluation function returns an estimate of the expected utility of the game from a given position, just as the heuristic functions of Chapter 3 return an estimate of the Distance to the goal. How exactly do we design good evaluation functions? 1. The evaluation function should order the terminal states in the same way as the true utility function: states that are wins must evaluate better than draws, which in turn must be better than losses. 2. The computation must not take too long. 3. For nonterminal states, the evaluation function should be strongly correlated with the actual chances of winning. If the search must be cut off at nonterminal states then the algorithm will necessarily be uncertain about the final outcomes of those states. This type of uncertainty is induced by computational, rather than informational, limitations. Given the limited amount of computation that the evaluation function is allowed to do for a given state, the best it can do is make a guess about the final outcome. Most evaluation functions work by calculating various features of the state. The features, taken together, define various categories or equivalence classes of states: the states in each category have the same values for all the features. For example: One category contains all two pawn vs. one pawn endgames. Any given category, generally speaking, will contain some states that leads to wins, some that lead to draws and some that lead to losses. The evaluation function cannot know which states are which, but it can return a single value that reflects the proportion of states with each outcome. For example: Suppose our experience suggests that 72% of the states encountered in the two pawns vs. one pawn category lead to a win (utility +1); 20% to a loss (0), and 8% to a draw (1/2). Then a reasonable evaluation for states in the category is the expected value: (0.72 × 1) + (0.2 × 0) + (0.08 × 1/2) = 0.76. In practice, this kind of analysis requires too many categories and hence too much experience to estimate all the probabilities of winning. Instead, most evaluation functions compute separate numerical contributions from each feature and then combine them to find the total value. For Example: Introductory chess books give an approximate material value for each piece: each pawn is worth 1, a knight or bishop is worth 3, a rook 5, and the queen 9. Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 50 | P a g e A secure advantage equivalent to a pawn gives a substantial likelihood of winning and a secure advantage equivalent to three pawns should give almost a certain victory. Mathematically this kind of evaluation function is called a weighted linear function because it can be expressed as: 𝐸𝑉𝐴𝐿(𝑠) = 𝑤1𝑓1(𝑠) + 𝑤2𝑓2(𝑠) + ⋯ + 𝑤𝑛𝑓𝑛(𝑠) = ∑ 𝑤𝑖𝑓𝑖(𝑠) 𝑛 𝑖=1 Where each 𝑤𝑖 is a weight and each 𝑓𝑖 is a feature of the position. Cutting off Search The next step is to modify ALPHA-BETA-SEARCH so that it will call the heuristic EVAL function when it is appropriate to cut off the search. We replace the two lines in Figure 5.7 that mention TERMINAL-TEST with the following line: IF CUTOFF-TEST(state, depth) then return EVAL(state) We must also arrange for some bookkeeping so that the current depth is increased on each recursive call. The most straightforward approach to controlling the amount of search is to set a fixed depth limit so that CUT-OFF(state, depth) returns true for all depth greater than some fixed depth d. A more robust approach is to apply iterative deepening. When time runs out, the program returns the move selected by the deepest completed search. As a bonus, iterative deepening also helps with move ordering. A more sophisticated cut-off test is needed. The evaluation function should be applied only to positions that are quiescent – that is, unlikely to exhibit wild swings in value in the near future. Non-quiescent positions can be expanded further until quiescent positions are reached. This extra search is called quiescent search; sometimes it is restricted to consider only certain types of moves, such as capture moves, that will quickly resolve the uncertainties in the position. The horizon effect is more difficult to eliminate. It arises when the program is facing an opponent’s move that causes serious damage and is ultimately unavoidable, but it can be temporarily avoided by delaying tactics. One strategy to mitigate the horizon effect is the singular extension, a move that is “clearly better” than all other moves in a given position. Once discovered anywhere in the tree in the course of a search, this singular move is remembered. When the search reaches the normal depth limit, the algorithm checks to see if the singular extension is a legal move. Forward Pruning It is also possible to do forward pruning, meaning that some moves at a given node are pruned immediately without further consideration. One approach to forward pruning is beam search: on each ply, consider only a “beam” of the 𝑛 best moves (according to the evaluation function) rather than considering all possible moves. Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 51 | P a g e This approach is rather dangerous because there is no guarantee that the best move will not be pruned away. The PROBCUT, or probabilistic cut, algorithm is a forward-pruning version of alpha beta search that uses statistics gained from prior experience to lessen the chance that the best move will be pruned. Alpha-beta search prunes any node that is provable outside the current (𝛼, 𝛽) window. PROCUT also prunes nodes that are probably outside the window. It computes this probability by doing a shallow search to compute the backed-up value 𝑣 of a node and then using past experience to estimate how likely it is that a score of 𝑣 at depth 𝑑 in the tree would be outside (𝛼, 𝛽). Search versus Lookup Many game-playing programs use table lookup rather than search for the opening and ending of games. For the openings, the computer is mostly relying on the expertise of humans. Computers can also gather statistics from a database of previously played games to see which opening sequences most often lead to a win. Near the end game there are again fewer possible positions, and thus more chance to do a lookup. Here it is the computer that has the expertise: computer analysis of endgames goes far beyond anything achieved by humans. A computer can completely solve the endgame by producing a policy, which is a mapping from every possible state to the best move in that state. Retrograde minimax search: reverse the rules to do unmoves rather than moves. Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 52 | P a g e Ch. 6 – Constraint Satisfaction Problems Chapters 3 and 4 explored the idea that problems can be solved by searching in a space of states. These states can be evaluated by domain-specific heuristics and tested to see whether they are goal states. From the point of view of the search algorithm, each state is atomic, or indivisible – a black box with no internal structure. We use a factored representation for each state: a set of variables, each of which has a value. A problem is solved when each variable has a value that satisfies all the constraints on the variable. A problem described this way is called a constraint satisfaction problem, or a CSP. Defining Constraint Satisfaction Problems A CSP consists of three components, X, D and C: X is the set of variables {𝑋1, 𝑋2, … 𝑋𝑛} D is the set of domains, {𝐷1, 𝐷2 … 𝐷𝑛}, one for each variable. C is the set of constraints that specify allowable combinations of values. Each domain 𝐷𝑖 consists of a set of allowable values, {𝑣1, … 𝑣𝑘} for variable 𝑋𝑖. Each constraint 𝐶𝑖 consists of a pair 〈𝑆𝑐𝑜𝑝𝑒, 𝑟𝑒𝑙〉, where scope is a tuple of variables that participate in the constraint, and rel is a relation that defines the values that those variables can take on. For Example: If 𝑋1 and 𝑋2 both have the domain {𝐴, 𝐵}, then the constraint saying the two variables must have different values can be written as 〈(𝑋1, 𝑋2), [(𝐴, 𝐵), (𝐵, 𝐴)]〉 or as 〈(𝑋1, 𝑋2), 𝑋1 ≠ 𝑋2〉. To solve a CSP, we need to define a state space and the notion of a solution. Each state in a CSP is defined by an assignment of values to some or all of the variables, {𝑋𝑖 = 𝑣𝑖, 𝑋𝑗 = 𝑣𝑗, … }. An assignment that does not violate any constraints is called a consistent or legal assignment. A complete assignment is one in which every variable is assigned, and a solution to a CSP is a consistent, complete assignment. A partial assignment is one that assigns values to only some of the variables. Example Problem: Map Coloring (p. 203) Example Problem: Job-Shop Scheduling (p. 204) Variations on the SCP Formalism The simplest kind of CSP involves variables that have discrete, finite domains. Map-colouring problems and scheduling with time limits are both of this kind. A discrete domain can be infinite, such as the set of integers or strings. With infinite domains, it is no longer possible to describe constraints by enumerating all allowed combinations of values. Instead a constraint language must be used that understands constraints such as 𝑇1 + 𝑑1 ≤ 𝑇2 directly, without enumerating the set of pairs of allowable values for (𝑇1, 𝑇2). Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465 53 | P a g e Special solution algorithms exist for linear constraints on integer variables, that is, constraints, such as the one just given, in which each variable appears only in linear form. It can be shown that no algorithm exists for solving general nonlinear constraints on integer variables. p. 207 Downloaded by Xolani Mazibuko (mazi76erx@gmail.com) lOMoARcPSD|4629465","libVersion":"0.2.3","langs":""}