{"path":"Subjects/INF3703 - Databases II/Unsorted/INF3703/studocnotes.pdf","text":"Database Consistency: • Consistent database state All data integrity constraints are met – • Every transaction must begin with the database in a consistent state. • All transactions are controlled and executed by the DBMS to guarantee DB integrity Transaction Properties: • Atomicity All operations of a transaction to be completed. All requests – must be successfully completed, otherwise transaction aborted. • Consistency – indicates the permanence of the DB’s consistent state. Transaction keeps database in consistent state, else it is aborted • Isolation - Data used during the execution of a transaction cannot be used while another transaction is using the data • Durability Once transaction changes are done and committed, they – cannot be undone or lost Serialisability: • Property in which selected order of concurrent transaction operations create same DB state as individual transaction execution. • Important in multiuser and distributed DB’s in which transactions are frequently concurrently executed. CONCURRENCY CONTROL What is concurrency control in a database: • Coordinates simultaneous execution of transactions in a multiuser DB system • Ensures serialisability of transactions in a multiuser DB environment. • Important as simultaneous execution of transactions over shared DB can create data integrity and consistency problems. Lost updates, Uncommitted data, inconsistent retrievals: • Lost updates: 2 Concurrent transactions are working on the same data, one update is lost (overwritten by the other transaction) • Uncommitted data: First transaction is rolled back after a second transactions has accessed uncommitted data. • Inconsistent Retrievals: Transaction accesses data before and after one or more transactions finish working with such data. CONCURRENCY CONTROL WITH LOCKING METHODS: What is an exclusive lock, and under what circumstances is it granted? • Exclusive lock exists when access is reserved specifically for the transaction that locked the object. Must be used when potential for conflict exists. • Issued when a transaction requests permission to update a data item and no locks are held on that data. • Exclusive lock not allows other transactions to access DB. Locking Granularity: • Database-level – Entire DB is locked • Table-level Entire table is locked – • Page-Level Entire diskpage locked – • Row-level Entire row is locked, allows different rows of the same table – to be accessed. • Field-level allows access to same row, but different fields. – Lock Types: • Binary lock Either locked or unlocked, no other transaction can use data – item • Exclusive lock Access is reserved specifically for transaction that locked – the object o Issued when transaction requests permission to update item and no locks are held on that data • Shared Lock Allows other read-only transactions to access the DB – Two Phase Locking: • Growing Phase: Transaction acquires all required locks without unlocking data. Once locks have been acquired, transaction is in it’s locked. • Shrinking Phase: Occurs when transaction releases all locks and cannot obtain a new lock. Deadlocks: • Occurs when two transactions wait indefinitely for each other to unlock data. 3 basic techniques to control deadlocks: • Prevention: Transaction requesting a new lock is aborted when possibility of deadlock occurs. All changes are rolled back, and locks released. • Detection: Periodically tests DB for deadlocks. If found, “victim” transaction is aborted. And other transaction continues. • Avoidance: Transaction must obtain all locks needed before being executed. Concurrency control with time stamping methods • A Technique used in scheduling concurrent transactions that assign a global unique time stamp to each transaction • Two properties: o Monotonicity always increases – o Uniqueness No -time stamps are the same – two Concurrency control with optimistic methods: • Optimistic approach does not require locking or time stamping techniques, instead transaction executed without restrictions until committed. Based on assumption majority of DB operations don’t conflict. Each transaction moves through phases: 1. Read Phase: transaction reads DB, executes needed computations, and updates private copy of Database. Changes recorded a temporary in update file, not updated by remaining transactions. 2. Validation: transaction is validated to ensure that the changes made will not affect integrity and consistency of the DB. Positive test, transaction goes to write phase, negative test, transaction is restarted, changes discarded. 3. Write phase: Changes permanently applied to the DB. Database Recovery management: Database backups can be performed at different levels: • Full backup: Full backup or dump of the DB. • Differential backup: Only objects that were updated or modified since last backup • Transaction log backup: backs up only the transaction log operations that are not reflected in a previous backup copy of DB. Critical events: • Hardware / Software failure • Human caused incidents • Natural disasters Transaction Recovery: What is database recovery? • Process of restoring a database to a previous consistent state, restoring lost data, accidentally deleted, corrupted, or made inaccessible. Four important concepts affecting recovery process when using data in the transaction log Transaction log contains data for DB recovery purposes. • Write-ahead protocol ensures transaction logs are always written before any database data is actually updated. • Redundant transaction logs: Ensures a physical disk failure will not impair the DBMS’s ability to recover data. • Database buffers: Temp storage areas in primary memory used to speed up disk operations. When updates are carried out, it actually updates the copy of data in the buffer, then all buffer changes are written to physical disk in a single operation. • Database checkpoints: operations in which DBMS writes all updated buffers in memory to disk. Also registered in the transaction log. As a result, physical database and transaction log will be in sync. Transaction recovery procedures make use of deferred write and write-through techniques: • Deferred write: Transaction operations do not immediately update the physical database. Only transaction log is updated. Then the database is physically updated only with data from committed transactions, with info from transaction log. • Write-through: DB is immediately updated by transaction operations during transaction execution. Even before transaction reaches commit point. Chapter 11 – Database Performance Tuning and Query Optimization End users interacting with the DBMS through queries process: • End user application generates a query • Query sent to DBMS • DBMS executes query, • Resulting data set sent back to end user. Performance Tuning client and server: Database performance tuning: • Set of activities to reduce response time of DB system SQL vs DBMS performance tuning • SQL performance tuning Activities to help generate a SQL query that – returns correct answers in the least amount of time, using minimum amount of resources at server end. • DBMS performance tuning Activates – ensuring client’s requests are addressed as quickly as possible while making optimum use of resources. • SQL done on client side (Query optimisation) • DBMS done on server side (caching, optimiser modes) Data cache vs SQL cache: • Data cache, or buffer cache, shared reserved memory area that stores the most recently accessed data blocks in RAM • SQL cache is a shared, reserved memory area that stores processed version of most recent executed SQL statements. DBMS Architecture: Typical DBMS processes • Listener – Listens for client’s requests, and handles processing of the SQL requests to other DBMS processes. Request passed to appropriate user process • User – DBMS creates a user process to manage each client session. When logged on to DBMS, user process is assigned. • Scheduler – Organises concurrent execution of SQL requests • Lock Manager - manages all locks placed on database objects, including disk pages. • Optimiser – Process analyses SQL queries and finds most efficient way to access data. Database statistics: • Refer to number of measurements about database objects, such as number of rows, or number of disk blocks used. • Provide a snapshot of database characteristics • They are important, they play a role in query optimisation where DBMS will use them in query processing to improve efficiency. Query Processing: DBMS processes queries in 3 phases: • Parsing: DBMS parses the query and chooses most efficient access / execution plan. • Execution: DBMS executes the SQL query using chosen execution plan. • Fetching: DBMS fetches data and sends result back to the client. Matching rows are retrieved, sorted, grouped, and aggregated, and returned to client Data sparsity: The number of different values a column can have. Optimiser choices: • Rule-based optimiser: Uses pre-set rules and points determining best approach to execute a query. Assigning fixed costs to each SQL operation • Cost-based optimiser: Uses sophisticated algorithms based on statistics about objects being accessed determining best approach to query execution. Query Optimisation: • Automatic Query optimisation: o DBMS finds the most cost effective access path without user intervention o Clearly more desirable from the end point of view, but users’ increases the overhead imposed on the DBMS • Manual Query optimisation: o Requires the optimisation be selected and scheduled by the end user or programmer • Static: o Algorithms and statistics determining the optimiser o Done at Compilation • Dynamic o Predefined rules the optimiser must follow o Done at execution Indexes: • Hash index o Ordered list of hash values • B-tree index o Ordered data structure organised as upside-down tree. • Bitmap index o Uses bit array to represent the existence of a value or condition. Chapter 12 – Distributed Database Management Systems DDBMS Advantages and Disadvantages: • Advantages: o Data located near site of greatest demand o Faster data access and processing o Growth facilitation o Improved communications o Reducing operating costs o Less danger of single point failure. o Processor Independence • Disadvantages: o Complexity of management and control o Technological difficulty o Security o Lack of standards o Increased storage and infrastructure requirements o Increased training costs. Characteristics of DDBMS 1. Application Interface 2. Validation 3. Transformation 4. Query optimisation 5. Mapping 6. I/O interface 7. Formatting 8. Security 9. Backup and recovery 10. DB administration 11. Concurrency control 12. Transaction management Fully Distributed DBMS must perform all functions of a centralised DBMS: 1. Receive request of application or end user 2. Validate, analyse, and decompose request. Might include mathematical and logical operations 3. Map request’s logical to physical data components 4. Decompose request into several disk I/O operations 5. Search for, locate, read, and validate data 6. Ensure DB consistency, security, and integrity 7. Validate data for the conditions, if any, specified by the request. 8. Present selected data in required format. DDBMS Components: • Network Hardware and software • Communications media • Computer workstations or remote devices. Distributed Database Transparency Features The minimum desirable DDBMS transparency features are: • Distribution transparency: Allows distributed DB to look like single logical DB to end user • Transaction transparency: Allows transaction to update data at more than one network site. Ensures the transaction will be either completed entirely, or aborted. Maintaining DB integrity • Failure transparency: Ensures system continues to operate in event of a node or network failure. Another network node will pick up lost functions. • Performance transparency: Allows system to perform as if it were a centralised DBMS and doesn’t suffer any performance degradation due to use on a network. Most cost-effective path to access remote data. • Heterogeneity transparency: Allows integration of several different local DBMSs (relational, network, and hierarchical) under a common, or global, schema. Three levels of distribution transparency are recognised: • Fragmentation transparency: Highest level, end user or programmer does not need to know a DB is partitione d. SELECT * FROM EMPLOYEE WHERE EMP_DOB = ’10-DEC-1988’; • Location transparency: When end user or programmer must specify the database fragment names but does not need to specify where those fragments are located SELECT * FROM A WHERE EMP_DOB = ’10-DEC-1988’ UNION SELECT * FROM B WHERE EMP_DOB = ’10-DEC-1988’ UNION SELECT * FROM C WHERE EMP_DOB = ’10-DEC-1988’; • Local mapping transparency: End user or programmer must specify both fragment names and their locations. SELECT * FROM A NODE SAF WHERE EMP_DOB = ’10-DEC-1988’ UNION SELECT * FROM B NODE NGA WHERE EMP_DOB = ’10-DEC-1988’ UNION SELECT * FROM A NODE ANG WHERE EMP_DOB = ’10-DEC-1988’; Transaction Transparency: • Is a DDBMS property that ensures DB transactions will maintain the distributed DB’s integrity and consistency, and the transaction will be completed only when all DB sites involved in the transaction complete their part of the transaction. Distributed Requests and Transactions: Different type of DB requests and transactions: • Remote request: DDBMS feature allowing a single SQL statement to access data in a single remote DP • Remote transaction: Feature allows a transaction (Several requests) to access data in a single remote DP. • Distributed request: Database request allowing a single SQL statement to access data in several remote DPs in a distributed DB. • Distributed transaction: Database transaction accesses data in several remote DPs in a distributed database. Distributed Database Design: Data fragmentation, replication, and allocation 1. How to partition into fragments: DB a. Allows you to break a single object into two or more segments b. Strategies: i. Horizontal fragmentation: refers to division into fragments of rows. Each fragment is stored at a different node, and each fragment has unique rows but same columns ii. Vertical fragmentation: Same concept but each fragment has unique columns (except key column, duplicated) iii. Mixed fragmentation: combination of both the above strategies 2. Which fragments to replicate Storage of duplicated DB fragments at – multiple sites on a DDBMS. 3. Where to locate those fragments and replicas data allocation strategies – are as follows: a. Centralised data allocation Entire DB stored at one site – b. Partitioned data allocation DB divided into two or more – disjointed parts (fragments) and stored at two or more sites. c. Replicated data allocation copies of one or more database – fragments are stored at several sites. Example of horizontal fragmentation: The CUSTOMER table partitioned horizontally by city (CUS_CITY attributes): Example fragment for Pretoria: Fragment: C1 Location: Pretoria Node: PTA CUS_NUM CUS_NAME CUS_ADDRESS CUS_CITY CUS_ZIP CUS_SUBSDATE 18384 John Smith 123 Free Street Pretoria 8000 18 Jan 2010 21653 Jane Doe 45 Loop Street Pretoria 8000 26 Oct 2016 The CAP Theorem Discuss the 3 properties of the CAP theorem: • Consistency: In a distributed DB, all nodes should see same data at the same time, which means the replicas should be immediately updated. However, involves dealing with latency and network partitioning delays • Availability: Simply speaking, requests are always fulfilled by the system. No received request is ever lost. • Partition Tolerance: System continues to operate even in the even of a node failure. Fragment Name Location Condition Node Name C1 Pretoria CUS_CITY = ‘PTA’ PTA C2 Florida CUS_CITY = ‘FL’ FL C3 Durban CUS_CITY = ‘DB’ DB C4 Cape Town CUS_CITY = ‘CT’ CT C.J. Date’s 12 Commandments for distributed Database: Chapter 13 – Business Intelligence and Data Warehouses Business Intelligence: • Provides framework for: o Collecting and storing operational data o Aggregating the operational data into decision support data o Analysing decision support data to generate information o Presenting such information to the end user to support business decisions. o Making Business decisions o Monitoring results to evaluate outcomes of business decisions o Predicting future behaviours and outcomes with a high degree of accuracy • Reporting styles: o Advanced reporting o Monitoring and alerting o Advanced data analytics • Benefits: o Integrating architecture o Common user interface for data reporting and analysis o Common data repository fosters single version of company data o Improved organisational performance Decision support data vs Operational data from Data analyst POV: • Time span, operational data covers a short time frame, DSS covers a longer time frame • Granularity (Level of aggregation), DSS must be presented at different levels of aggregation • Dimensionality, Operational data focuses on representing individual transactions rather than effects of transactions over time. The data warehouse: • Integrated, subject-oriented, time-variant, non-volatile collection of data that provides support for decision making. o Integrated: Centralised, consolidated database integrating data derived from entire organisation and from multiple sources with diverse formats. o Subject-Oriented: Data warehouse data arranged and optimised to provide answers to questions from diverse functional areas within a company. Organised and summarised by topic. o Time-variant: In contrast to operational data, focusing on current transactions, warehouse data represents the flow of data through time. Even contains projected data generated through statistical and other models. o Non-volatile: Once data enters the data warehouse, it is never removed. Represents company history, operational data, representing near-term history is always added. Data is never deleted and new data is continually added. Data marts: Compare and contrast a data warehouse and a data mart: • Data mart is a small, single-subject data warehouse subset providing decision support to a small group of people. Data mart could be created from data extracted from a larger data warehouse • Only difference between warehouse and mart is the size and scope of problem being solved. Problem definitions and data requirements are essentially the same for both. Some organisations prefer a data mart because: • Lower cost and shorter implementation time (6 month 12 months, vs 1-– 3 years) • Company employees more likely to embrace minor changes • People at different organisational levels will likely require data with different summarisation, aggregation, and presentation formats. • Data marts can serve as a test vehicle for organisations exploring the potential benefits of a data warehouse. Star Schemas: • Star schema is a data modelling technique used to map multidimensional decision support data into a relational database. Represents data using a central table known as a fact table. • Components: Facts, dimensions, attributes, and attribute hierarchies • Technique allows model data optimised for business decision making. Data analyst usually looks at facts through dimension’s attributes. Was designed to optimise data query operations rather than data update operations Common techniques to improve performance: • Normalising dimensional tables • Maintaining multiple fact tables to represent different aggregation levels • De-normalising fact tables • Partitioning and replicating tables. Chapter 14 Big Data Analytics and NoS – QL Big Data • Set of data displaying Volume, Velocity and Variety to an extent that makes it unusable for management by a relational DBMS. The 3V Characteristics • Volume quantity of data to be stored. Systems either need to scale-– up or down. • Velocity speed at which data enters the system & rate of data being – processed. Data streams require processed and filtered to determine which data is feasibly kept. • Variety the variation in structure of the data to be stored. Big data is – mostly semi or unstructured thus not suitable for relational DB. Data must be captured format it naturally exists. Other Vs • Variability the change in the meaning of the data (based on context) – • Veracity refers to the trustworthiness of data – • Value (viability) the degree to which the data can be analysed to – provide meaningful information • Visualization the ability to graphically present the vast amounts of data – understandably Stream Processing: • Focuses on input processing, and requires analysis of data stream as it enters the system. Feedback loop processing • The analysis of data to produce actionable results • Capturing data, processing it, then acting on returned information • Provides immediate results requires analysis of large amounts of data within seconds for results to be delivered in real time • Used to help organisations sift through data for faster decision making & tactical decisions. key component of data analytics– • Feedback loop processing: Hadoop: • High Volume • Write-once, read-many • Streaming access • Fault tolerance Chapter 15 Database Connectivity and Web Technologies – Database Connectivity Refers to the mechanisms where applications connect & communicate with data repositories. 5 database interfaces: 1. Native SQL connectivity (vendor provided) 2. Microsoft’s Open Database Connectivity (ODBC), Data Access Objects (DAO), and Remote Data Objects (RDO) 3. Microsoft’s Object Linking and Embedding for Database (OLE-DB) 4. Microsoft’s ActiveX Data Objects (ADO.NET) 5. Oracle’s Java Database Connectivity (JDBC) 3 Main components of ODBC: • High-level ODBC API through which application programs access ODBC functionality. • A driver manager that oversees managing all database connections. • An ODBC driver that communicates directly to the DBMS. ODBC: Middleware that provides DB access to API to Windows applications DAO: OO-application programming interface used to access MS Access, FileMaker Pro RDO: Higher level OO application interface used to access remote DB servers. They use lower level DAO & ODBC for direct access to databases. DAO & RDO object interface provides more functionality than ODBC. ODBC, DAO, and RDO are implemented as shared code that is dynamically linked to the Windows operating environment through DLLs Main objects in OLE- DB • Consumers: request & use data by invoking methods exposed by data provider objects • Providers: manage the connection with data source & provide data to consumers o Data: provide data to other processors. Database vendors create data provider objects that expose the functionality of the underlying data source. o Service: provide additional functionality to consumers. Located between data provider & consumer, requests data from data provider, transforms the data, then provides the transformed data to the data consumer ADO.NET: • Consolidates all data access functionality under one integrated object model. • Data access component of Microsoft’s .NET application development framework. Oracle’s Java Database Connectivity • An API that allows a Java program to interact with a wide range of data sources: relational DB, tabular data sources, spreadsheets • Allows a Java program to establish connection with data sources, prepare & send the SQL code to the DB server & process the result sets Advantage of JDBC: • It allows a company to leverage its existing investments in technology and personnel training. • Allows programmer to use SQL skills to manipulate company’s databases Advantage of JDBC over DB connectivity that shares components & functionalities • Requires no configuration on the client’s side • JDBC driver is automatically downloaded & installed as part of the Java applet download. It is web-based, applications can connect to a DB directly using a URL Web to DB Middleware: A database server-side extension program (web- -database middleware.) to 1. Client browser sends a page request to the web server. 2. Web server receives & passes request to WTDB middleware for processing. 3. Requested page contains some type of scripting language to enable DB interaction. The web server passes the script to WTDB middleware. 4. The WTDB middleware reads, validates, & executes the script. It connects to the and passes the query using connectivity layer. DB DB 5. The server executes the query & passes the result back to the WTDB DB middleware. 6. The WTDB middleware compiles the result set, dynamically generates an HTML-formatted page including data retrieved from the DB, sends it to the web server. 7. The web server returns the just-created HTML page, which now includes the query result, to the client browser. The client browser displays the page on the local computer. Cloud Computing • Cloud computing is a computing model that provides ubiquitous, on-demand access to a shared pool of configurable resources that can be rapidly provisioned with minimal effort. • Cloud computing can be called a “game changer” as it eliminates financial and technological barriers so organizations can leverage database technologies in their business processes with minimal effort & cost. • Cloud services have the potential to turn basic IT services into “commodity” & to enable a revolution that could change not only the way that companies do business, but the IT business itself. Cloud Implementation Types • Public Cloud: built by third parties to sell services to general public. Most common type. (Google Application Engine, Office 365) • Private Cloud: built by an organisation for the sole use of servicing its own needs (Private company) • Community Cloud: built for and by a specific group/organisation that share common trades (agencies, government) Types of Cloud services: • SaaS: offers turnkey applications that run in the cloud • PaaS: offers capability to build & deploy consumer-created applications using the provider’s cloud tools, languages & interfaces • IaaS: offers consumers the ability to provision their own resources on demand Advantages & Disadvantages of Cloud Services Advantages: Low initial cost of entry when compared to building in-house. • Scalability easy to add/remove resources on demand. – • Support for mobile computing • Ubiquitous access access from anywhere, any time. – • High reliability & performance – • Fast provisioning resources can be provisioned in a matter of minutes with – minimal effort. • Managed infrastructure Disadvantages: Issues of security, privacy and compliance • Hidden costs of implementation and operation (bandwidth & data migration) • Data migration is a difficult and lengthy process. • Complex licensing schemes and complicated SLAs. • Loss of ownership and control no longer in complete control of their data. – • Organisation culture – end-users tend to be resistant to change. • Difficult integration with internal IT systems. XML: • Metalanguage used to represent and manipulate data elements • Permits manipulation, facilitates exchange, and structured documents • Chapter 16 – Database Administration and Security Desirable DBA skills: Managerial skills: Broad business understanding • Coordination skills • Analytical skills • Conflict resolution skills • Communication skills • Negotiation skills Technical skills: Broad data-processing background and up- -date knowledge of database to technologies • Understanding of SDLC • Structured methodologies (data flow diagrams, structure charts, programming languages) • Knowledge of DBLC • Database modelling and design skills (conceptual, logical, physical) • Operational skills (database implementation, data dictionary management) Support services: • Correct • Efficient • Compliant DBMS Operations: • System support • Performance monitoring and tuning • Backup and recovery • Security auditing and monitoring The importance of database backup and recovery: When data is not readily available, companies face potentially ruinous losses. Data backup & recovery procedures are critical in all database installations. • The DBA must also ensure that data can be fully recovered in case of data loss or loss of database integrity. Losses can be partial or total, backup & recovery procedures are the cheapest database insurance you can buy Measures to put in place to ensure backup & recovery 1. Periodic data and application backups 2. Proper backup identification 3. Convenient and safe backup storage’ 4. Physical protection of both hardware & software 5. Personal access control to the software of a DB installation 6. Insurance coverage for the data in the DB The DBA’s Technical Function • Evaluating, selecting & installing the DBMS & related utilities • Designing & implementing DBs & applications • Testing & evaluating DBs & applications • Training & supporting users • Maintaining the DBMS, utilities & applications DBA Design & Implementation Technical Services The DBA provides data-modelling and design services to end users • The DBA works with application programmers to ensure the quality and integrity of database design and ensuring transactions are correct, efficient and compliant. • The DBA must provide oversight and assistance in determination and creation of storage space, data loading, conversion and database migration services, as well as the generation, compilation and storage of the application’s access plan. • The DBA must develop, test, and implement operational procedures required by the new system before the application comes online. Fine-tuning and reconfiguring of the DBMS may also be required. Database Security • Goals o Confidentiality Data protected against unauthorised access. – o Integrity keep data consistent, free of errors – o Availability Accessibility of data whenever required by authorised – users • Vulnerabilities: o Technical o Managerial o Cultural o Procedural • Breach: o Integrity is preserved or corrupted • Define each user to the database o Assign passwords to each user o Define user groups o Assign access privileges o Control physical access • View definition - define data views to protect and control scope of the data • DBMS access control – controlled by placing limits on the use of DBMS query and reporting tools. DBMS usage monitoring –audit the use of data in the database Success factors in the development and implementation of a successful data administration strategy: • Management commitment – The commitment of top-level management is necessary to enforce the use of standards, procedures, planning, and controls. • Thorough analysis of the company situation • End-user involvement • Defined standards • Training • A small pilot project","libVersion":"0.2.3","langs":""}