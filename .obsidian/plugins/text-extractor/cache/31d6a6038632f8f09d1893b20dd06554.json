{"path":"Subjects/COS3712 - Computer Graphics/Unsorted/Web/Summary_info_2.pdf.pdf","text":"Summary Interactive Computer Graphics Page 1 of 38 Chapter 1 Computer graphics is concerned with all aspects of producing pictures or images using a computer. 1.1 Applications of Computer Graphics The development of computer graphics has been driven both by the needs of the user community and by advances in hardware and software. The application of computer graphics can be divided into four, possibly overlapping, areas: 1. Display of information: Some examples include: ÔÇ∑ Maps are used to display celestial and geographical information. ÔÇ∑ Statistical plots are generated to aid the viewer in determining information in a set of data. ÔÇ∑ Medical imaging technologies, such as CT, MRI, ultrasound, and PET. 2. Design: Professions such as engineering and architecture use computer-aided design (CAD) tools to create interactive technical drawings. 3. Simulation and animation: Graphical flight simulators have proved both to increase safety and to reduce training expenses. Computer graphics are also used for animation in the television, motion-pictures, and advertising industries. Virtual reality (VR) technology allows the viewer to act as part of a computer-generated scene. 4. User interfaces: Our interaction with computers has become dominated by a visual paradigm that includes windows, icons, menus, and a pointing device, such as a mouse. 1.2 A Graphics System The six major components of a basic graphics system are: 1. Input devices; 2. Central Processing Unit (CPU); 3. Graphics Processing Unit (GPU); 4. Memory; 5. Frame buffer; 6. Output devices. In a graphics program, we can obtain the measure (what the device returns) of an input device in three distinct modes: 1. Request mode: The measure of the device is not returned to the program until the device is triggered. A trigger of a device is a physical input on the device with which the user can signal the computer. 2. Sample-mode: As soon as the function call of a function that expects device input is encountered, the measure is returned. 3. Event-mode: Each time that a device is triggered, an event is generated and the device measure, including the identifier for that device, is placed in an event queue. Periodically the queue is polled, and foreach (if any) event in the queue, the program can look at the event‚Äôs type and then decide what to do. Both request- and sample-mode input APIs require that the user identify which device is to provide the input, and, are thus not sufficient for modern computing environments. Summary Interactive Computer Graphics Page 2 of 38 Raster based graphics system: The image we see on the output device is an array ‚Äì the raster‚Äì of pixels produced by the graphics system. Each pixel corresponds to a unique location in the image. Collectively, the pixels are stored in a part of memory called the frame buffer. Resolution refers to the number of pixels in the frame buffer, and it determines the detail that you can see in the image. The depth, or precision of the frame buffer, defined as the number of bits used per pixel, determines properties such as how many colours can be represented on a given system. In full- colour (also known as true-colour or RGB-colour) systems, there is at least 24 bit per pixel. The frame buffer is actually a collection of buffers: Colour buffers hold the coloured pixels that are displayed; depth buffers hold information needed for creating images from three-dimensional data; and other special purpose buffers, such as accumulation buffers, etc... Rasterization (or scan conversion) is the process of converting geometric entities to pixel colours and locations in the frame buffer. Non-interlaced display: The pixels are displayed row by row, or scan line by scan line at the refresh rate. I.e. all the rows are refreshed. Interlaced display: Odd rows and even rows are refreshed alternatively. 1.3 Images: Physical and Synthetic Computer-generated images are synthetic or artificial in the sense that the objects being imaged do not exist physically. Two basic entities must be part of any image-formation process, be it mathematical or physical: ÔÇ∑ Object: It exists in space independent of any image-formation process and of any viewer. We define a synthetic object by specifying the positions in space, called vertices, of various geometric primitives (points, lines, and polygons) that, when put together, approximate the object. CAD systems make it easy for a user to build synthetic objects. ÔÇ∑ Viewer: It is the viewer that forms the image of our objects. Viewers placed at different positions, will see different images of the same object. Projection (image formation): The process by which the specification of an object is combined with the specification of a viewer to produce a two-dimensional image. Visible light has wavelengths in the range of 350 to 780 nanometres (nm). Distinct frequencies within this range are visible as distinct colours. Wavelengths in the middle of the range, around 520 nm, are seen as green; those near 450 nm are seen as blue; and those near 650 nm are seen as red. Light sources can emit light either as a set of discrete frequencies or continuously. A particular light source is characterized by the intensity of light that it emits at each frequency and by that light‚Äôs directionality. An ideal point source emits energy from a single location at one or more frequencies equally in all directions. Light from the source strikes various surfaces of an object; the details of the interaction between light and the surfaces of the object determine how much light is reflected, and hence the colour(s) of the object as perceived by the viewer. Summary Interactive Computer Graphics Page 3 of 38 1.4 Imaging Systems A pinhole camera is a box with a small hole in the centre of one side of the box. The hole must be small enough to ensure that only a single ray of light, emanating from a point, can enter it. The film is placed inside the box, at a distance Ôøø from the pinhole. If we orient the camera along the Ôøø-axis, with the pinhole at the origin of the coordinate system, the projection of the point (Ôøø, Ôøø, Ôøø) is ÔøΩùë•,Ôøø , Ôøø ÔøΩ = ÔøΩ‚àíùë•ùëë , ‚àí ùë¶ùëë , ‚àíùëëÔøΩ Ôøø Ôøø Ôøø Ôøø Ôøø Infinite depth of field: Every point within the field of view is in focus. 1.5 The Synthetic Camera Model Synthetic-camera model: A paradigm in which we look at the creation of a computer-generated image as being similar to forming an image using an optical system, such as a camera. To create artificial images, we need to identify a few basic principles: ÔÇ∑ The specification of the objects is independent of the specification of the viewer. ÔÇ∑ We can compute the image of an object using simple geometric calculations. We find the image of a point on an object on the virtual image plane, called the projection plane, by drawing a line, called a projector, from the point to the centre of the lens, called the centre of projection (COP).The image of the point is located where the projector passes through the projection plane. ÔÇ∑ The size of the image is limited. Objects outside the field of view should not appear in the resulting image. We place a clipping rectangle, or clipping window, in the projection plane. Projection plane COP Projector Clipping window Summary Interactive Computer Graphics Page 4 of 38 1.6 The Programmer‚Äôs Interface The interface between an application program and a graphics system can be specified through a set of functions that resides in a graphics library. These specifications are called the application programming interface (API). The application programmer sees only the API and is thus shielded from the details of both the hardware and the software implementation of the graphics library. If we are to follow the synthetic-camera model, we need functions in the API to specify the following: ÔÇ∑ Objects: The geometry of an object is usually defined by sets of vertices. ÔÇ∑ A viewer: We can define a viewer or camera by specifying a number of parameters, including: position, orientation, focal length, and the size of the projection plane. ÔÇ∑ Light sources: Light sources are defined by their location, strength, colour, and directionality. ÔÇ∑ Material properties: Material properties are characteristics, or attributes, of the objects, and such properties are specified through a series of function calls at the time that each object is defined. Wire-frame image: Only the edges of polygons (outline) are rendered using line segments. The modelling-rendering paradigm: The modelling of the scene is separated from the production of the image, or the rendering of the scene. Thus, we might implement the modeler and the renderer with different software and hardware. This paradigm has become popular as a method for generating computer games and images over the Internet. Models, including the geometric objects, lights, cameras, and material properties, are placed in a data structure called a scene graph that is passed to a renderer or game engine. 1.7 Graphics Architectures Early graphics architectures: Early graphics systems used general-purpose computers that could process only a single instruction at a time. Its display included the necessary circuitry to generate a line segment connecting two points. The job of the host computer was to run the application program that computes and sends endpoint data of the line segments in the image to the display at a rate high enough to avoid flicker on the display. Display processors: The earliest attempts to build special-purpose graphics systems were concerned primarily with relieving the general-purpose computer from the task of refreshing the display continuously. The instructions to generate the image could be assembled once in the host and sent to the display processor where they we restored in the display processor‚Äôs local memory. Thus, the host is freed for other tasks while the display processor continuously refreshes the display. Pipeline architecture: A pipeline comprises a series of interconnected components, each optimized to perform a specific, or set, of operations on data moving through the pipeline. A pipeline can provide significant performance increase when the same sequence of concurrent operations must be performed on many, or large, data sets. That is exactly what we do in computer graphics, where large sets of vertices and pixels must be processed. Latency is the time it takes a single data item to pass through the pipeline. Throughput is the rate at which data flows through the pipeline. Summary Interactive Computer Graphics Page 5 of 38 The graphics pipeline: We start with a (possibly enormous) set of vertices which defines the geometry of the scene. We must process all these vertices in a similar manner to form an image in the frame buffer. There are four major steps in the imaging process: 1. Vertex processing: Each vertex is processed independently. The two major functions of this block are to carry out coordinate transformations and to compute a colour for each vertex. Per-vertex lighting calculations can be performed in this box. 2. Clipping and primitive assembly: Sets of vertices are assembled into primitives, such as line segments and polygons, before clipping can take place. In the synthetic camera model, a clipping volume represents the field of view of an optical system. The projections of objects in this volume appear in the image; those that are outside do not (clipped out); and those that straddle the edges of the clipping volume are partly visible. Clipping must be done on a primitive-by-primitive basis rather than on a vertex-by-vertex basis. The output of this stage is a set of primitives whose projections should appear in the image. 3. Rasterization: The primitives that emerge from the clipper are still represented in terms of their vertices and must be converted to pixels in the frame buffer. The output of the rasterizer is a set of fragments foreach primitive. A fragment can be thought of as a potential pixel that carries with it information, including its colour, location, and depth. 4. Fragment processing: It takes the fragments generated by the rasterizer and updates the pixels in the frame buffer. Hidden-surface removal, texture mapping, bump mapping, and alpha blending can be applied here. Per-fragment lighting calculations can also be performed in this box. 1.8 Programmable Pipelines For many years, although the application program could set many parameters, the basic operations available within the pipeline were fixed (fixed-function pipeline). Recently, both the vertex processor and the fragment processor are programmable by the application program. The main advantage of this is that many of the techniques that formerly could not be done in real time, because they were not part of the fixed-function pipeline, can now be done in real time. Vertex programs can alter the location or colour of each vertex as it flows through the pipeline: allowing for a variety of light- material models or creating new projections. Fragment programs allow us to uses textures in new ways (bump-mapping) and to implement other parts of the pipeline, such as lighting, on a per- fragment basis. These vertex- and fragment-programs, are commonly known as shaders, or shader programs. 1.9 Performance Characteristics The overall performance of a graphics system is characterized by how fast we can move geometric entities through the pipeline and by how many pixels per second we can alter in the frame buffer. Consequently, the fastest graphics workstations are characterized by geometric pipelines at the front end and parallel bit processors at the back end. Physically based techniques, such as ray tracing and radiosity, can create photorealistic images with great fidelity, but usually not in real time. Summary Interactive Computer Graphics Page 6 of 38 Chapter 2 Graphics Programming 2.1 The Sierpinski Gasket Immediate mode graphics: As vertices are generated by the application, they are sent directly to the graphics processor for rendering on the display. One consequence of immediate mode is that there is no memory of the geometric data. Thus, if we want to redisplay the scene, we would have to go through the entire creation and display process again (and every time are display is required). Retained mode graphics: We compute all the geometric data first and store it in some data structure. We then display the scene by sending all the stored data to the graphics processor at once. This approach avoids the overhead of sending small amounts of data to the graphics processor for each vertex we generate, but at the cost of having to store all the data. Because the data are stored, we can redisplay the scene, by resending the stored data without having to regenerating it. Current GPUs allow us to store the generated data directly on the GPU, thus avoiding the bottleneck caused by transferring the data from the CPU to the GPU each time we wish to redisplay the scene. 2.2 Programming Two-Dimensional Applications Two-dimensional systems are regarded as a special case of three-dimensional systems. Mathematically, we view the two-dimensional plane, or a simple two-dimensional curved surface, as a subspace of a three-dimensional space. We can represent the two-dimensional point Ôøø = (Ôøø, Ôøø) as Ôøø = (Ôøø, Ôøø, 0) in the three-dimensional world. In WebGL, vertices specified as two- or three- dimensional entities are internally represented in the same manner. In WebGL terms: ÔÇ∑ A vertex is a position in space; we use two-, three- and four-dimensional spaces in computer graphics. We use vertices to specify the atomic geometric primitives that are recognized by our graphics system. ÔÇ∑ A point is the simplest geometric primitive, and is usually specified by a single vertex. Clip coordinate system: Can be visualized as a cube centered at the origin whose diagonal goes from (-1, -1, -1) to (1, 1, 1). Objects outside this cube will be eliminated, or clipped, and cannot appear on the display. The vertex shader uses transformations to convert geometric data specified in some coordinate system to a representation in clip coordinates and outputs this information to the rasterizer. 2.3 The WebGL Application Programming Interface A graphics system performs multiple tasks to produce output and handle user input. An API for interfacing with this system can contain hundreds of individual functions. These functions can be divided into seven major groups: 1. Primitive functions: Define the low-level objects or atomic entities that our system can display. WebGL supports only points, line segments, and triangles. 2. Attribute functions: Govern the way that a primitive appears on the display. Summary Interactive Computer Graphics Page 7 of 38 3. Viewing functions: Allow us to specify various views. WebGL does not provide any viewing functions, but relies on the use of transformations in the shaders to provide the desired view. 4. Transformation functions: Allow us to carry out transformations of objects, such as rotation, translation, and scaling. In WebGL, we carry out transformations by forming transformation matrices in our applications, and then applying then either in the application or in the shaders. 5. Input functions: Deals with input devices. 6. Control functions: Enable us to communicate with the window system, to initialize our programs, and to deal with any errors that take place during the execution of our programs. 7. Query functions: Allow us to obtain information about the operating environment, camera parameters, values in the frame buffer, etc. We can think of the entire graphics system as a state machine. Applications provide input that change the state of the machine or cause the machine to produce a visible output. From the perspective of the API, graphics functions are of two types: those that specify primitives that flow through a pipeline inside the state machine and those that either change the state inside the machine or return state information. One important consequence of the state machine view is that most parameters are persistent; their values remain unchanged until we explicitly change them. WebGL functions are in a single library called GL. Shaders are written in the WebGL Shading Language (GLSL), which has a separate specification from WebGL, although the functions to interface the shaders with the application are part of the WebGL API. To interface with the window system and to get input from external devices into our programs, we need to use some other library (GLX for X Window System, wgl for Windows, agl for Macintosh, GLUT (WebGL Utility Toolkit) is a simple cross-platform library). The WebGL Extension Wrangler (GLEW) library is used with cross-platform libraries, such a GLUT, to removes operating system dependencies. WebGL makes heavy use of defined constants to increase code readability and avoid the use of magic numbers. Functions that transfer data to the shaders have the following notation: glSomeFunction*(); where the * can be interpreted as either nt or ntv, where n signifies the number of dimensions (1, 2, 3, 4, or Matrix); t denotes the datatype, such as integer(i), float (f), or double (d); and v, if present, indicates that the variables are specified through a pointer to an array, rather than through an argument list. The units used to specify vertex positions in the application program are referred to as vertex coordinates, object coordinates, or world coordinates, and can be arbitrarily chosen by the programmer to suite the application. Units on the display device are called window coordinates, screen coordinates, physical-device coordinates or just device coordinates. At some point, the values in vertex coordinates must be mapped to window coordinates, but this is automatically done by the graphics system as part of the rendering process. The user needs to specify only a few parameters. This allows for device-independent graphics; freeing application programmers from worrying about the details of input and output devices. Summary Interactive Computer Graphics Page 8 of 38 2.4 Primitives an Attributes To ensure that a filled polygon is render correctly, it must have a well-defined interior. A polygon has a well-defined interior if it satisfies the following three properties: ÔÇ∑ Simple: In two dimensions, as long as no two edges of a polygon cross each other, we have a simple polygon. ÔÇ∑ Convex: An object is convex if all points on the line segment between any two points inside the object, or on its boundary, are inside the object (i.e. the line segment never intersects the edges of the object). Convex objects include triangles, tetrahedral, rectangles, parallelepipeds, circles, and spheres. ÔÇ∑ Flat (planar). All the vertices that specify the polygon lie in the same plane. As long as the three vertices of a triangle are not collinear, its interior is well defined and the triangle is simple, flat, and convex. Consequently, triangles are easy to render, and for these reasons triangles are the only fillable geometric entity that WebGL recognizes. We can separate primitives into two classes: geometric primitives and image, or raster primitives. The basic WebGL geometric primitives are specified by sets of vertices. All WebGL geometric primitives are variants of points, line segments, and triangular polygons. ÔÇ∑ Points (GL_POINTS): A point can be displayed as a single pixel or a small group of pixels. Use glPointSize() to set the current point size (in pixels). ÔÇ∑ Lines: Use glLineWidth() the set the current line width (in pixels). o Line segments (GL_LINES): Successive pairs of vertices are interpreted as the endpoints of individual line segments. o Line strip or polyline (GL_LINE_STRIP): Successive vertices are connected. o Line loop (GL_LINE_LOOP): Successive vertices are connected, and a line segment is drawn from the final vertex to the first, thus creating a closed path. ÔÇ∑ Polygons (triangles): Use glPolygonMode() to tell the renderer to generate only the edges or just points for the vertices, instead of fill (the default). o Triangles (GL_TRIANGLES): Each successive group of three vertices specifies anew triangle. o Triangle strip (GL_TRIANGLE_STRIP): Each additional vertex is combined with the previous two vertices to define a new triangle. o Triangle fan (GL_TRIANGLE_FAN): It is based on one fixed point. The next two points determine the first triangle, and subsequent triangles are formed from one new point, the previous point, and the first (fixed) point. Summary Interactive Computer Graphics Page 9 of 38 Triangulation is the process of approximating a general geometric object by subdividing it into a set of triangles. Every set of vertices can be triangulated. Triangulation is a special case of the more general problem of tessellation, which divides a polygon into a polygonal mesh, not all of which need be triangles. Text in computer graphics is problematic. There are two forms of text: ÔÇ∑ Stroke text: stroke text is constructed as are other geometric objects. We use vertices to specify line segments or curves that outline each character. The advantage of stroke text is that it can be defined to have all the detail of any other object and it can be manipulated by standard transformations and viewed like any other graphical primitive. ÔÇ∑ Raster text: Characters are defined as rectangles of bits called bit blocks. Each block defines a single character by the pattern of 0 and1 bits in the block. A raster character can be placed in the frame buffer rapidly by a bit-block-transfer (bit blt) operation. Increasing the size of raster text characters cause it to appear blocky. WebGL does not have a text primitive. Attributes are properties that describe how an object should be rendered. Available attributes depend on the type of object. For example, line segments can have colour, thickness, and pattern (solid, dashed, or dotted). 2.5 Colour Additive colour model: The three primary colours (Red, Green, Blue) add together to give the perceived colour. (CRT monitors and projectors are examples of additive colour systems). With additive colour, primaries add light to an initially black display, yielding the desired colour. Subtractive colour model: Here we start with a white surface, such as a sheet of paper. Coloured pigments remove colour components from light that is striking the surface. If we assume that white light hits the surface, a particular point will appear red if all components of the incoming light are absorbed by the surface except for wavelengths in the red part of the spectrum, which is reflected. In subtractive systems, the primaries are usually the complementary colours: cyan, magenta, and yellow (CMY). Industrial printers are examples of subtractive colour systems. Colour cube: We can view a colour as a point in a colour solid. We draw the solid using a coordinate system corresponding to the three primaries. The distance along a coordinate axis represents the amount of the corresponding primary in the colour. We can represent any colour that we can produce with this set of primaries as a point in the cube. In a RGB system, each pixel might consist of 24 bits (3 bytes): 1 byte for each of red, green, and blue. The specification of RGB colours is based on the colour cube. Thus, specify colour components as numbers between 0.0 and 1.0, where 1.0 denotes the maximum (or saturated value) of the corresponding primary and 0.0 denotes a zero value of that primary. RGBA is an extension of the RGB model, where the fourth colour (A, or alpha) is treated by WebGL as either an opacity or transparency value. Transparency and opacity are complements of each other: an opaque object lets no light through, while a transparent object passes all light. Opacity values range from 0.0 (fully transparent) to 1.0 (fully opaque). Alpha blending is disabled by default. Summary Interactive Computer Graphics Page 10 of 38 Indexed colour: Early graphics systems had frame buffers that were limited in depth: for example, each pixel was only 8 bits deep. Instead of subdividing a pixel‚Äôs bits into groups, and treat them as RGB values (which will result in a very restricted set of colours), with Indexed colours the limited- depth pixel is interpreted as an integer value which index into a colour-lookup table. The user program can fill the entries (rows) of the table with the desired colours. A Problem with indexed colours is that when we work with dynamic images that must be shaded, usually we need more colours than are provided by colour-index mode. Historically, colour-index mode was important because it required less memory for the frame buffer; however, the cost and density of memory is no longer an issue. 2.6 Viewing A fundamental concept that emerges from the synthetic-camera model is that the specification of the objects in our scene is completely independent of our specification of the camera. The simplest and WebGL‚Äôs default view is the orthographic projection. All projectors are parallel, and the centre of projection is replaced by a direction of projection. Furthermore, all projectors are perpendicular(orthogonal) to the projection plane. The orthographic projection takes a point (Ôøø, Ôøø, Ôøø) and projects it into the point (Ôøø, Ôøø, 0). 2.7 Control Functions A window, or display window, is an operating-system managed rectangular area of the screen in which we can display our images. A window has a height and width. Because the window displays the contents of the frame buffer, positions in the window are measured in window or screen coordinates, where the units are pixels. Note that references to positions in a window are usually relative to the top-left corner (which is regarded as position (0, 0)). The aspect ratio of a rectangle is the ratio of the rectangle‚Äôs width to its height. If the aspect ratio of the viewing (clipping) rectangle, specified by camera parameters, is not the same as the aspect ratio of the window, objects appear distorted on the screen. A viewport is a rectangular area of the display window in which our images are rendered. By default, it is the entire window, but it can be set to any smaller size in pixels via the function void glViewport(GLint x, GLint y, GLsizei w, GLsizei h) where (x, y) is the lower-left corner of the viewport (measured relative to the lower-left corner of the window) and w and h give the width and height, respectively. For a given window, we can adjust the height and width of the viewport to match the aspect ratio of the clipping rectangle, thus preventing any object distortion in the image. Events are changes that are detected by the operating system and include such actions as a user pressing a key on the keyboard, the user clicking a mouse button or moving the mouse. When an event occurs, it is placed in an event-queue. The event queue can be examined by an application program or by the operating system. We can associate callback functions with specific types of events. Event processing gives us interactive control in our programs. With GLUT, we can execute the function glutMainLoop() to begin an event-processing loop. All our programs must have at least a display callback function which is invoked when the application program or the operating system determines that the graphics in a window need to be redrawn. Summary Interactive Computer Graphics Page 11 of 38 2.8 The Gasket Program A vertex-array object (VAO) allows us to bundle data associated with a vertex array. Use of multiple vertex-array objects will make it easy to switch among different vertex arrays. You can have at most one current (bound) VAO at a time. A buffer object allows us to store data directly on the GPU. Include glFlush();at the end of the display callback function to ensure that all the data are rendered as soon as possible. Every application, no matter how simple, must provide both a vertex- and a fragment-shader (there are no default shaders). Each shader is a complete C-line program with main() as its entry point. The vertex shader is executed foreach vertex that is passed through the pipeline. In general, a vertex shader will transform the representation of a vertex location from whatever coordinate system in which it is specified to a representation in clip coordinates for the rasterizer. The fragment shader is executed foreach fragment generated by the rasterizer. At a minimum, each execution of the fragment shader must output a colour for the fragment. 2.9 Polygons and Recursion Recursive subdivision is a powerful technique that can be used to subdivide a polygon into a set of smaller polygons. 2.10 The Three-Dimensional Gasket By default, primitives are drawn in the order in which they appear in the array buffer. As a primitive is being rendered, all its fragments are unconditionally placed into the frame buffer covering any previously drawn objects, even if those objects are actually located closer to the viewer. Algorithms for ordering objects so that they are drawn correctly are called visible-surface algorithms or hidden- surface-removal algorithms, depending on how we look at the problem. The hidden-surface-removal algorithm supported by WebGL is called the z-buffer algorithm. The z-buffer is one of the buffers that make up the frame buffer. You use glEnable(GL_DEPT_TEST) to enable the z-buffer algorithm. Chapter 3 Instead of calling the display callback function directly, rather invoke the glutPostRedisplay() function which sets an internal flag indicating that the display needs to be redrawn. At the end of each event-loop iteration, if the flag is set, the display callback is invoked and the flag is unset. This method prevents the display from being redrawn multiple times in a single pass through the event loop. Two types of events are associated with the pointing device (mouse): ÔÇ∑ Mouse events occur when one of the mouse buttons is either depressed (mouse down event) or released (mouse up event). ÔÇ∑ Move events are generated when the mouse is moved with one of the buttons depressed. If the mouse is moved without a button being held down, this event is called a passive move event. Summary Interactive Computer Graphics Page 12 of 38 Reshape events occur when the user resizes the window, usually by dragging a corner of the window to a new location. This is an example of a window event. Unlike most other callbacks, there is a default reshape callback that simply changes the viewport to the new window size. Keyboard events can be generated when the mouse is in the window and one of the keys is depressed or released. The idle callback is invoked when there are no other events. A typical use of the idle callback is to continue to generate graphical primitives through a display function while nothing else is happening. Another is to produce an animated display. An application program operates asynchronously from the automatic display of the contents of the frame buffer, and can cause changes to the frame buffer at any time. Hence, a redisplay of the frame buffer can occur while its contents are still being altered by the application and the user will see only a partially drawn display. This distortion can be severe, especially if objects are constantly moving around in the scene. A common solution is double-buffering. The hardware has two frame buffers: one, called the front buffer, is the one that is displayed, the other, called the back buffer, is then available for constructing what we would like to display next. Once the drawing is complete, we swap the front and back buffers. We then clear the new back buffer and can start drawing into it. With double-buffering we use glutSwapBuffers() instead of glFlush()at the end of the display callback function. GLUT provides pop-up menus that we can use with the mouse to create sophisticated interactive applications. GLUT also supports hierarchical (cascading) menu entries. Using menus involves taking a few simple steps: ÔÇ∑ Define callback function(s) that specify the actions corresponding to each entry in the menu. ÔÇ∑ Create a menu; register its callback; and add menu entries and/or submenus. This step must be repeated foreach submenu, and once for the top-level menu. ÔÇ∑ Attach the top-level menu to a particular mouse button. Summary Interactive Computer Graphics Page 13 of 38 Chapter 4: Geometric Objects and Transformations 4.1 Scalars, Points, and Vectors See tutorial letter 103 for a summary on linear algebra. 4.2 Three-Dimensional Primitives A full range of three-dimensional objects cannot be supported on existing graphics systems, except by approximate methods. Three features characterize three-dimensional objects that fit well with existing graphics hardware and software: 1. The objects are described by their surfaces and can be thought of as being hollow. Since a surface is a two- rather than a three-dimensional entity, we need only two-dimensional primitives to model three-dimensional objects. 2. The objects can be specified through a set of vertices in three dimensions. 3. The objects either are composed of, or can be approximated by simple, flat, and convex polygons. Most graphics systems are optimized for the processing of points, line segments, and triangles. Even if our modelling system provides curved objects, we assume that a triangle mesh approximation is used for implementation. 4.3 Coordinate Systems and Frames In three dimensions, the representation of a point and a vector is the same: Ôøø ÔøΩùë¶ÔøΩ Ôøø Homogeneous coordinates avoid this difficulty by using a four-dimensional representation for both points and vectors in three dimensions: The fourth component of a vector is set to 0: Ôøø Ôøø ÔøΩùëßÔøΩ, 0 whereas the fourth component of a point is set to 1: Ôøø Ôøø ÔøΩùëßÔøΩ 1 Some advantages of using homogeneous coordinates include: ÔÇ∑ All affine (line-preserving) transformations can be represented as matrix multiplications. ÔÇ∑ We can carry out operations on points and vectors using their homogeneous-coordinate representations and ordinary matrix algebra. ÔÇ∑ The uniform representation of all affine transformations makes carrying out successive transformations (concatenation) far easier than in three-dimensional space. ÔÇ∑ Although we have to work in four dimensions to solve three-dimensional problems when we use homogeneous-coordinate representations, less arithmetic work is involved. ÔÇ∑ Modern hardware implements homogeneous-coordinate operations directly, using parallelism to achieve high-speed calculations. Summary Interactive Computer Graphics Page 14 of 38 4.4 Frames in webGL In versions of WebGL with a fixed-function pipeline and immediate-mode rendering, six frames were specified in the pipeline. With programmable shaders, we have a great deal of flexibility to add additional frames or avoid some traditional frames. The following is the usual order in which the frames occur in the pipeline: 1. Object (or model) coordinates: In most applications, we tend to specify or use an object with a convenient size, orientation, and location in its own frame called the model or object frame. 2. World (or application) coordinates: A scene may comprise many objects. The application program generally applies a sequence of transformations to each object to size, orient, and position it within a frame that is appropriate for the particular application. This application frame is called the world frame, and the values are in world coordinates. 3. Eye (or camera) coordinates: Virtually all graphics systems use a frame whose origin is the centre of the camera‚Äôs ‚Äúlens‚Äù and whose axes are aligned with the sides of the camera. This frame is called the camera frame or eye frame. 4. Clip coordinates: Once objects are in eye coordinates, WebGL must check whether they lie within the view volume. If an object does not, it is clipped from the scene prior to rasterization. WebGL can carry out this process most efficiently if it first carries out a projection transformation that brings all potentially visible objects into a cube centered at the origin in clip coordinates. 5. Normalized device coordinates: At this stage, vertices are still represented in homogeneous coordinates. The division by the Ôøø (fourth) component, called perspective division, yields three-dimensional representations in normalized device coordinates. 6. Window coordinates: The final transformation takes a position in normalized device coordinates and, taking into account the viewport, creates a three-dimensional representation in window coordinates. Window coordinates are measured in units of pixels on the display but retain depth information. 7. Screen coordinates: If we remove the depth coordinate, we are working with two- dimensional screen coordinates. Because there is an affine transformation that corresponds to each change of frame, there are 4 √ó 4 matrices that represent the transformation from model coordinates to world coordinates and from world coordinates to eye coordinates. These transformations usually are concatenated together into the model-view transformation, which is specified by the model-view matrix. 4.5 Matrix and Vector Classes The authors of the book provide javascript types that you could use in your application. These can be found in MV.js. The package contains the definitions of mat2, mat3, mat4, vec2, vec3, and vec4 types. These classes mirror those available in the GLSL language. Matrix classes are for 2 √ó 2, 3 √ó 3, and 4 √ó 4 matrices whereas the vector types are for 2-, 3- and 4-element arrays. Standard matrix and vector operations are also implemented. 4.6 Modelling a Coloured Cube Summary Interactive Computer Graphics Page 15 of 38 We describe geometric objects through a set of vertex specifications. The data specifying the location of the vertices (geometry) can be stored as a simple list or array - the vertex list. We have to be careful about the order in which we specify our vertices when we are defining a three-dimensional polygon. The order is important because each polygon has two sides. Our graphics systems can display either or both of them. We call a face outward facing if the vertices are traversed in a counter-clockwise order when the face is viewed from the outside. This method is also known as the right-hand rule because if you orient the fingers of your right hand in the direction the vertices are traversed, the thumb points outward. By specifying front and back carefully, we will be able to eliminate (or cull) faces that are not visible or to use different attributes to display front and back faces. 4.7 Affine Transformations A transformation is a function that takes a point (or vector) and maps it into another point (or vector). When we work with homogeneous coordinates, any affine transformation can be represented by a 4 √ó 4 matrix that can be applied to a point or vector by pre-multiplication: Ôøø = ÔøøÔøø. All affine transformations preserve lines. Common affine transformations include rotation, translation, scaling, shearing, or any combination of these. 4.8 Translation, Rotation, and Scaling Translation is an operation that displaces points by a fixed distance in a given direction. Rotation is an operation that rotates points by a fixed angle about a point or line. In a right-handed system, when we draw the ùë•- and ùë¶-axes in the standard way, the positive ùëß-axis comes out of the ‚Äúpage‚Äù. If we look down the positive ùëß-axis towards the origin, the positive direction of rotation (positive angle of rotation) is counter-clockwise. This definition applies to both the ùë•- and ùë¶-axes as well. Rotation and translation are known as rigid-body transformations. No combination of rotations and translations can alter the shape or volume of an object; they can alter only the object‚Äôs location and orientation. Scaling is an affine non-rigid-body transformation by which we can make an object bigger or smaller. Scaling has a fixed point: a point that is unaffected by the transformation. A negative scaling factor gives us reflection about the fixed point, in the specific scaling direction. ÔÇ∑ Uniform scaling: The scaling factor in all directions is identical. The shape of the scaled object is preserved. ÔÇ∑ Non-uniform scaling: The scaling factor of each direction need not be identical. The shape of the scaled object is distorted. Summary Interactive Computer Graphics Page 16 of 38 4.9 Transformations in Homogenous Coordinates Translation matrix: Inverse translation matrix: Scaling matrix (fixed point at origin): Inverse scaling matrix (fixed point at origin): Rotation about the Ôøø-axis by an angle Ôøø: Rotation about the Ôøø-axis by an angle Ôøø: Summary Interactive Computer Graphics Page 17 of 38 Rotation about the Ôøø-axis by an angle Ôøø: Suppose that we let Ôøø denote any of our three rotation matrices, the inverse rotation matrix is: ùëÖ‚àí1(ùúÉ) = ùëÖ(‚àíùúÉ) = ùëÖùëá(ùúÉ) We can construct any desired rotation matrix, with a fixed point at the origin as a product of individual rotations about the three axes: Ôøø = ÔøøÔøøÔøøÔøøÔøøÔøø. Using the fact that the transpose of a product is the product of the transposes in the reverse order, we see that for any rotation matrix, ùëÖ‚àí1 = ùëÖùëá. Summary Interactive Computer Graphics Page 18 of 38 4.10 Concatenation of Transformations We can create transformation matrices for more complex affine transformations by multiplying together, or concatenating, sequences of the basic transformation matrices. This strategy is preferable to attempting to define an arbitrary transformation directly. To rotate an object about an arbitrary fixed point, say ùê©ùëì, we first translate the object such that ùê©ùëì coincides with the origin: ùëá(‚àíùê©ùëì); we then apply the rotation: ùëÖ(ùúÉ); and finally move the object back such that ùê©ùëì is again at its original position: ùëá(ùê©ùëì). Thus, concatenating the matrices together, we obtain the single matrix: Notice the ‚Äúreverse‚Äù order in which the matrices are multiplied. Matrix multiplication, in general, is not a commutative operation, thus, the order in which we apply transformations is critical! Objects are usually defined in their own frames, with the origin at the centre of mass and the sides aligned with the model frame axes. To place an instance of such an object in a scene, we apply an affine transformation ‚Äì the instance transformation ‚Äì to the prototype to obtain the desired size, orientation, and location. The instance transformation is constructed in the following order: first, we scale the object to the desired size; then we orient it with a rotation matrix; finally, we translate it to the desired location. Hence, the instance transformation is of the form Ôøø = ÔøøÔøøÔøø. To rotate an object by an angle Ôøø about an arbitrary axis, we carry out at most two rotations to align the axis of rotation with, say the Ôøø-axis; then rotate by Ôøø about the Ôøø-axis; and finally, we undo the two rotations that did the aligning. Thus, our final rotation matrix will be of the form: ùëÖ = ùëÖ‚àí1ùëÖ‚àí1ùëÖ (ùúÉ)ùëÖ ùëÖ . Ôøø Ôøø Ôøø Ôøø Ôøø 4.11 Transformation Matrices in GL In a modern implementation of WebGL, the application programmer not only can choose which frames to use (model-, world-, and eye-frame), but also where to carry out the transformations between frames (application or vertex shader). Although very few state variables are predefined in WebGL, once we specify various attributes and matrices, they effectively define the state of the system and hence how vertices are processed. The two transformations we will use most often are: 1. Model-view transformation: The model-view matrix is an affine transformation matrix that brings representations of geometric objects from application or model frames to the camera frame. 2. Projection transformation. The projection matrix is usually not affine and is responsible for carrying out both the desired projection and also changes the representation to clip coordinates. In my opinion, the remainder of this section (except for the example) applies to older versions of WebGL. Summary Interactive Computer Graphics Page 19 of 38 4.12 Spinning of the Cube In a given application, a variable may change in a variety of ways. When we send vertex attributes to a shader, these attributes can be different foreach vertex in a primitive. We may also want parameters that will remain the same for all vertices during a draw. Such variables are called uniform qualified variables. Summary Interactive Computer Graphics Page 20 of 38 Chapter 5: Viewing 5.1 Classical and Computer viewing Projectors meet at the centre of projection (COP). The COP corresponds to the centre of the lens in the camera or in the eye, and in a computer-graphics system, it is the origin of the camera frame for perspective views. The projection surface is a plane, and the projectors are straight lines. If we move the COP to infinity, the projectors become parallel and the COP can be replaced by a direction of projection (DOP). Views with a finite COP are called perspective views; views with a COP at infinity (i.e. a DOP) are called parallel view. The class of projections produced by parallel and perspective systems is known as planar geometric projections because the projection surface is a plane and the projectors are lines. Both perspective and parallel projections preserve lines; they do not, in general, preserve angles. Classical views: ÔÇ∑ Parallel projections: o Orthographic projection: In all orthographic (or orthogonal) views, the projectors are perpendicular to the projection plane. In a multi-view orthographic projection, we make multiple projections, in each case with the projection plane parallel to one of the principal faces of the object. The importance of this type of view is that it preserves both distances and angles. It is well suited for working drawings. o Axonometric projections: In axonometric views, the projectors are still orthogonal to the projection plane, but the projection plane can have any orientation with respect to the object. ÔÇß Isometric view: The projection plane is placed symmetrically with respect to the three principal faces that meet at a corner of a rectangular object. ÔÇß Diametric view: The projection place is placed symmetrically with respect to two of the principal faces of a rectangular object. ÔÇß Trimetric view: The projection plane can have any orientation with respect to the object (the general case). Although parallel lines are preserved in the image, angles are not. Axonometric views are used extensively in architecture and mechanical design. o Oblique projections: It is the most general parallel view. We obtain an oblique projection by allowing the projectors to make an arbitrary angle with the projection plane. Angles in planes parallel to the projection plane are preserved. ÔÇ∑ Perspective projections: All perspective views are characterized by diminution of size: the farther an object is moved from the viewer, the smaller its image becomes. We cannot make measurements from a perspective view. Hence, perspective views are used by applications where it is important to achieve natural-looking images. The classical perspective views are usually known as one-, two-, and three-point perspective. The one, two, and three prefixes refer to the number of vanishing points (points at which lines of perspective meet). Three-point Two-point One-point Summary Interactive Computer Graphics Page 21 of 38 5.2 Viewing with a Computer In computer graphics, the desired view can be achieved by applying a sequence of transformations on each object in the scene. Every transformation is equivalent to a change of frames. Of the frames that are used in WebGL, three are important in the viewing process: the object frame, the camera frame, and the clip coordinate frame. Viewing can be divided into two fundamental operations: ÔÇ∑ First, we must position and orient the camera. This transformation is performed by the model-view transformation. ÔÇ∑ The second step is the application of the projection transformation (parallel or perspective). Objects within the specified clipping volume are placed into the same cube in clip coordinates. Hidden-surface removal occurs after the fragment shader. Consequently, although an object might be blocked from the camera by other objects, even with hidden-surface removal enabled, the rasterizer will still generate fragments for blocked objects within the clipping volume. 5.3 Positioning of the Camera At any given time, the model-view matrix encapsulates the relationship between the camera frame and the object frame. As its name suggests, the model-view transformation is the concatenation of two transformations: ÔÇ∑ A modelling transformation that takes instances of objects in object coordinates and brings them into the world frame. For each object in a scene, we construct a transformation matrix, called the instance transformation, that will scale, orient, and translate the object to the desired location in the scene. This matrix can be derived by concatenating a series of basic transformation matrices. ÔÇ∑ The viewing transformation transforms world coordinates to camera coordinates. We consider three ways to construct this transformation matrix. The first method for constructing the view transformation is by concatenating a carefully selected series of affine transformations. We can think of the camera as being fixed at the origin, pointing down the negative Ôøø-axis. Thus, we transform (translate, rotate, etc.) the scene relative to the camera frame. For example, if we want to move farther away from an object located directly in front of the camera, we move the scene down the negative Ôøø-axis (i.e. translate by a negative Ôøø-value). In the second approach, we specify the camera frame with respect to the world frame and construct the matrix, called the view-orientation matrix, that will take us from world coordinates to camera coordinates. In order to define the camera frame, we require three parameters to be specified: ÔÇ∑ View-Reference Point (VRP): Specifies the location of the COP, given in world coordinates. ÔÇ∑ View-Plane Normal (VPN): Also known as Ôøø, specifies the normal to the projection plane. ÔÇ∑ View-up vector (VUP): Specifies what direction is up from the camera‚Äôs perspective. This vector need not be perpendicular to ùëõ. We project the VUP vector onto the view plane to obtain the up-direction vector, ùë£, which is orthogonal to ùëõ. We then use the cross product (v√ó ùëõ) to obtain a third orthogonal direction ùë¢. This Summary Interactive Computer Graphics Page 22 of 38 new orthogonal coordinate system usually is referred to as either the viewing-coordinate system or the Ôøø-Ôøø-Ôøø system. With the addition of the VRP, we have the desired camera frame. The third method, called the look-at function, is similar to our second approach: it differs only in the way we specify the VPN. We specify a point, ùêû, called the eye point, which has exactly the same meaning as the VRP described above. Next, we define a point, ùêö, called the at point, at which the camera is pointing. Together, these points determine the VPN (ùë£ùëùùëõ = ùêö ‚àí ùêû). The specification of VUP and the derivation of the camera frame is the same as above. The second part of the viewing process, often called the normalization transformation, involves specifying and applying a specific projection matrix (parallel or perspective). 5.4 Parallel Projections Projectors are parallel and point in a direction of projection (DOP). Projection is a technique that takes the specification of points in three dimensions and maps them to points on a two-dimensional projection surface. Such a transformation is not invertible, because all points along a projector map into the same points on the projection surface. Orthogonal or orthographic projections are a special case of parallel projections, in which the projectors are perpendicular to the projection plane. Projection normalization is a technique that converts all projections into simple orthogonal projections by distorting the objects such that the orthogonal projection of the distorted objects is the same as the desired projection of the original objects. This is done by applying a matrix called the normalization matrix, also known as the projection matrix. Conceptually, the normalization matrix should be defined such that it transforms (distorts) the specified view volume to coincide exactly with the canonical (default) view volume. Consequently, vertices are transformed such that vertices within the specified view volume are transformed to vertices within the canonical view volume, and vertices outside the specified view volume are transformed to vertices outside the canonical view volume. The canonical view volume is the cube defined by the planes ùë• = ¬±1, ùë¶ = ¬±1, ùëß = ¬±1. Two advantages of employing projection normalization are: ÔÇ∑ Both perspective and parallel views can be supported by the same pipeline; ÔÇ∑ The clipping process is simplified because the sides of the canonical view volume are aligned with the coordinate axes. The shape of the viewing volume for an orthogonal projection is aright-parallelepiped. Thus, the projection normalization process for an orthographical projection requires two steps: 1. Perform a translation to move the centre of the specified view volume to the centre of the canonical view volume (the origin). 2. Scale the sides of the specified view volume such that they have a length of 2. Summary Interactive Computer Graphics Page 23 of 38 5.5 Perspective Projections A point in space (ùë•, ùë¶, ùëß) is projected along a projector into the point ÔøΩùë•ùëù,ùë¶ùëù, ùëßùëùÔøΩ. All projectors pass through the COP (origin), and, because the projection plane is perpendicular to the Ôøø-axis, ÔøøÔøø = Ôøø. Because the camera is pointing in the negative Ôøø-direction, Ôøø is negative. From the top view shown in the figure above, we see that two similar triangles are formed. Hence ÔøøÔøø = Ôøø Using the side view: Ôøø ‚áí ùë•ùëù Ôøø = Ôøø Ôøø/Ôøø ÔøøÔøø = Ôøø Ôøø ‚áí ùë¶ùëù Ôøø = Ôøø Ôøø/Ôøø The division by Ôøø describes nonuniform foreshortening: The images of objects farther from the centre of projection are reduced in size(diminution) compared to the images of objects closer to the COP. Although this perspective transformation preserves lines, it is not affine. It is also irreversible: we cannot recover a point from its projection. We can extend our use of homogeneous coordinates to handle projections. When we introduced homogeneous coordinates, we represented a point in three dimensions (Ôøø, Ôøø, Ôøø) by the point (Ôøø, Ôøø, Ôøø, 1) in four dimensions. Suppose that, instead, we replace (Ôøø, Ôøø, Ôøø) by the four-dimensional point As long as ùë§ ‚â† 0, we can recover the three-dimensional point from its four-dimensional representation by dividing the first three components by ùë§; a process known as perspective division. By allowing ùë§ to change, we can represent a larger class of transformations, including perspective projections. Consider the matrix The matrix Ôøø transforms the point [Ôøø, Ôøø, Ôøø, 1]Ôøø to the point [Ôøø, Ôøø, Ôøø, Ôøø/Ôøø]Ôøø. By performing perspective division (i.e. divide the first three components by the fourth), we obtain Summary Interactive Computer Graphics Page 24 of 38 Hence, matrix Ôøø can be used to perform a simple perspective projection. We apply the projection matrix after the model-view matrix, but remember that we must perform a perspective division at the end. 5.6 Perspective Projections with WebGL The shape of the view volume for a perspective projection is a frustum (truncated pyramid). Frustum() and Perspective() are two APIs that can be used to specify a perspective projection matrix. 5.7 Perspective-Projection Matrices A perspective-normalization transformation converts a perspective projection to an orthogonal projection. 5.8 Hidden-Surface Removal The graphics system must be careful about which surfaces it displays in a three-dimensional scene. Algorithms that remove those surfaces that should not be visible to the viewer are called hidden- surface-removal algorithms, and algorithms that determine which surfaces are visible to the viewer are called visible-surface algorithms. Hidden-surface-removal algorithms can be divided into two broad classes: ÔÇ∑ Object-space algorithms attempt to order the surfaces of the objects in the scene such that rendering surfaces in a particular order provides the correct image. This class of algorithms does not work well with pipeline architectures in which objects are passed down the pipeline in an arbitrary order. The graphics system must have all the objects available so it can sort them into the correct back-to-front order. ÔÇ∑ Image-space algorithms work as part of the projection process and seek to determine the relationship among object points on each projector. The z-buffer algorithm is an image-space algorithm that fits in well with the rendering pipeline. As primitives are rasterized, we keep track of the distance from the COP or the projection plane to the closest point on each projector that has already been rendered. We update this information as successive primitives are projected and filled. Ultimately, we display only the closest point on each projector. The algorithm requires a depth buffer, or z-buffer, to store the necessary depth Summary Interactive Computer Graphics Page 25 of 38 information as primitives are rasterized. The z-buffer forms part of the frame buffer and has the same spatial resolution as the colour buffer. Major advantages of this algorithm are that its complexity is proportional to the number of fragments generated by the rasterizer and that it can be implemented with a small number of additional calculations over what we have to do to project and display polygons without hidden- surface removal. Culling: for a convex object, such as the cube, faces whose normals point away from the viewer are never visible and can be eliminated or culled before the rasterization process commences. 5.9 Displaying Meshes A mesh is a set of polygons that share vertices and edges. We use meshes to display, for example, height data. Height data determine surface, such as terrain, through either function that gives the heights above a reference value, such as elevations above sea level, or through samples taken at various points on the surface. For the sake of efficiency and simplicity, we should strive to organize this data in a way that will allow us to drawn it using a combination of triangle strips and triangle fans. 5.10 Projections and Shadows Shadows are important components of realistic images and give many visual clues to the spatial relationships among the objects in a scene. Shadows require a light source to be present. If the only light source is at the centre of projection, a model known as ‚Äúflashlight in the eye‚Äù, then there are no visible shadows, because any shadows are behind visible objects. To add physically correct shadows, we must understand the interaction between light and material properties. Consider a simple shadow that falls on the surface Ôøø = 0. Not only is this shadow a flat polygon, called a shadow polygon, but it is also the projection of the original polygon onto this surface. More specifically, the shadow polygon is the projection of the polygon onto the surface with the centre of projection at the light source. It is possible to compute the vertices of the shadow polygon by means of a suitable projection matrix. 5.11 Shadow maps Summary Interactive Computer Graphics Page 26 of 38 Chapter 6: Lighting and Shading Local lighting models, as opposed to global lighting models, allow us to compute the shade to assign to a point on a surface, independent of any other surfaces in the scene. The calculations depend only on the material properties assigned to the surface, the local geometry of the surface, and the locations and properties of the light sources. This model is well suited for a fast pipeline graphics architecture. 6.1 Light and Matter From a physical perspective, a surface can either emit light by self-emission, as a light bulb does, or reflect light from other surfaces that illuminate it. Some surfaces may both reflect light and emit light at the same time. When we look at a point on an object, the colour that we see is determined by multiple interactions among light sources and reflective surfaces: we see the colour of the light reflected from the surface toward our eyes. Interactions between light and materials can be classified into three groups: 1. Specular surfaces appear shiny because most of the light that is reflected or scattered is in a narrow range of angles close to the angle of reflection. With a perfectly specular surface, an incoming light ray may be partially absorbed, but all reflected light from a given angle emerges at a single angle, obeying the rule that the angle of incidence is equal to the angle of reflection. 2. Diffuse surfaces are characterized by reflected light being scattered in all directions. Perfectly diffuse surfaces scatter light equally in all directions. 3. Translucent surfaces allow some light to penetrate the surface and to emerge from another location on the object. This process of refraction characterizes glass and water. 6.2 Light Sources We describe a light source through a three-component intensity, or luminance, function: Each component represents the intensity of the independent red, green, and blue components. We consider four basic types of sources: ÔÇ∑ Ambient light: In many rooms, such as class rooms, the lights have been designed and positioned to provide uniform illumination throughout the room. Ambient illumination is characterized by an intensity, Ôøøa, that is identical at every point in the scene. ÔÇ∑ Point sources: An ideal point source emits light equally in all directions. The intensity of illumination received from a point source is proportional to the inverse square of the distance between the source and surface, called the distance term. ÔÇ∑ Spotlights: Spotlights are characterized by a narrow range of angles (a cone) through which light is emitted. More realistic spotlights are characterized by the distribution of light within the cone ‚Äì usually with most of the light concentrated in the centre of the cone. ÔÇ∑ Distant light source: All rays are parallel and we replace the location (point) of the light source with the direction (vector) of the light. Summary Interactive Computer Graphics Page 27 of 38 6.3 The Phong Reflection Model The Phong model uses the four vectors shown in the diagram to calculate a colour for an arbitrary point Ôøø on a surface. The vector Ôøø is the normal at Ôøø; the vector Ôøø is in the direction from Ôøø to the viewer or COP; the vectorÔøø is in the direction of a line from Ôøø to an arbitrary point on the light source; finally, the vector Ôøø is in the direction that a perfectly reflected ray from Ôøø would take... Note that Ôøø is determined by Ôøø and Ôøø. The Phong model supports the three types of material-light interactions: ambient, diffuse, and specular. Foreach light source we can have separate ambient, diffuse, and specular components for each of the three primary colours. Thus, we need nine coefficients to characterize these terms at any point ùíë. We can place these coefficients in a 3 √ó 3 illumination matrix for the ith light source: The intensity of ambient light Ôøøa is the same at every point on the surface. Some of this light is absorbed and some is reflected. The amount reflected is given by the ambient reflection coefficient, ùëòa (0 ‚â§ ùëòa ‚â§ 1). Thus: ùêºa = ùëòaùêøa A perfectly diffuse reflector scatters the light that it reflects equally in all directions. Hence, such a surface appears the same to all viewers (i.e. neitherùíó norùíì need be considered). The amount of light reflected depends both on the material ‚Äì because some of the incoming light is absorbed ‚Äì and on the position of the light source relative to the surface. Diffuse surfaces, sometimes called Lambertian surfaces, can be modelled mathematically with Lambert‚Äôs law. According to Lambert‚Äôs law, we see only the vertical component of the incoming light. Lambert‚Äôs law states that ùëÖùëë ‚àù cosùúÉ where Ôøø is the angle between the normal at the point of interest Ôøø and the direction of the light source Ôøø. If both Ôøø and Ôøø are unit-length vectors, then cosùúÉ = ùíç ‚àô ùíè. If we add in a reflection coefficient ùëòd (0 ‚â§ ùëòd ‚â§ 1) representing the fraction of incoming diffuse light that is reflected, we have the diffuse reflection term: ùêºd = ùëòdùêød(ùíç ‚àô ùíè). Specular reflection adds a highlight that we see reflected from shiny objects. The amount of light that the viewer sees depends on the angle Ôøø between Ôøø, the direction of a perfect reflector and Ôøø, the direction of the viewer. The Phong model uses the equation ùêºùë† = ùëòùë†ùêøùë†cosŒ±ùúô The coefficient ùëòùë† (0 ‚â§ ùëòùë† ‚â§ 1) is the fraction of incoming specular light that is reflected. The exponent Œ± is a shininess coefficient. As Œ± is increased, the reflected light is concentrated in a narrower region centered on the angle of a perfect reflector. Values in the range 100 to 500 correspond to most metallic surfaces. If ùíì and ùíó are normalized, then ùêºùë† = ùëòùë†ùêøùë†(ùíì ‚àô ùíó)ùõº Summary Interactive Computer Graphics Page 28 of 38 The Phong model, including the distance term, is written Ôøø = 1 Ôøø + ÔøøÔøø + ÔøøÔøø2 (Ôøø dÔøød max(ùíç ‚àô ùíè, 0) + ùëò ÔøøÔøøÔøø max((ùíì ‚àô ùíó)ùõº,0)) + ùëò aÔøøa. This formula is computed foreach light source and for each primary. If we use the Phong model with specular reflections, the dot product ùíì ‚àô ùíó should be recalculated at every point on the surface. An approximation to this involves the unit vector halfway between the viewer vector and the light-source vector: Ôøø = Ôøø + Ôøø . |Ôøø + Ôøø| If we replace ùíì ‚àô ùíó with ùíè ‚àô ùíâ, we avoid calculating ùíì. When we use the halfway vector in the calculation of the specular term, we are using the Blim-Phong, or modified Phong, lighting model. 6.4 Computation of Vectors If we are given three noncolinear points ‚Äì ùëù0, ùëù1, ùëù2 ‚Äì we can calculate the normal to the plane in which they lie as follows ùíè = (ùëù2 ‚àí ùëù0) √ó (ùëù1 ‚àí ùëù0). The order in which we cross-multiply vectors determines the direction of the resulting vector (see illustration). At every point (Ôøø, Ôøø, Ôøø) on the surface of a sphere centered at the origin, we have that Ôøø = (Ôøø, Ôøø, Ôøø). To calculate Ôøø, we first normalize both Ôøø and Ôøø, and then use the following equation ùíì = 2(ùíç ‚àô ùíè)ùíè ‚àí ùíç. GLSL provides a function, reflect(), which we can use in our shaders to compute Ôøø. 6.5 Polygonal Shading A polygonal mesh comprises many flat polygons, each of which has a well-defined normal. We consider three ways to shade these polygons: ÔÇ∑ Flat shading (or constant shading): The shading calculation is carried out only once foreach polygon, and each point on the polygon is assigned the same shade. Flat shading will show differences in shading among adjacent polygons. We will see stripes, known as Mach bands, along the edges. ÔÇ∑ Gouraud shading (or smooth shading): The lighting calculation is done at each vertex using the material properties and the vectors Ôøø, Ôøø, and Ôøø . Thus, each vertex will have its own colour that the rasterizer can use to interpolate a shade foreach fragment. We define the normal at a vertex to be the normalized average of the normals of the polygons that share the vertex. We implement Gouraud shading either in the application or in the vertex shader. ÔÇ∑ Phong shading: Instead of interpolating vertex intensities (colours), we interpolate normals across each polygon. We can thus make an independent lighting calculation foreach fragment. We implement Phong shading in the fragment shader. Summary Interactive Computer Graphics Page 29 of 38 6.6 Approximation of a Sphere by Recursive Subdivision The sphere is not a primitive type supported by WebGL. By a process known as recursive subdivision, we can generate approximations to a sphere using triangles. Recursive subdivision is a powerful technique for generating approximations to curves and surfaces to any desired level of accuracy. 6.7 Specifying Lighting Parameters For every light source, we must specify its colour and either its location (fora point source or spotlight) or its direction (for a distant source). The colour of a source will have three components ‚Äì ambient, diffuse, and specular‚Äì that we can specify. For positional light sources, we may also want to account for the attenuation of light received due to its distance from the source. We can do this by using the distance ‚Äìattenuation model which contains constant, linear, and quadratic terms. Material properties should match up directly with the supported light sources and with the chosen reflection model. We specify ambient, diffuse, and specular reflectivity coefficients (Ôøøa, Ôøød, Ôøøs) for each primary colour via three colours using either RGB or RGBA colours. Note that often the diffuse and specular reflectivity coefficients are the same. For the specular component, we also need to specify its shininess coefficient. We also want to allow for scenes in which a light source is within the view volume and thus might be visible. We can create such effects by including an emissive component that models self-luminous sources. This term is unaffected by any of the light sources, and it does not affect any other surfaces. It simply adds a fixed colour to the surface. 6.8 Implementing a Lighting Model Because light from multiple sources is additive, we can repeat our lighting calculation foreach source and add up the individual contributors. We have three choices as to where we do lighting calculations: in the application, in the vertex shader, or in the fragment shader. But for the sake of efficiency, we will almost always want to do lighting calculations in the shaders. Light sources are special types of geometric object and have geometric attributes, such as position, just like polygons and points. Hence, light sources can be affected by transformations. To implement lighting in the shader, we must carry out three steps: 1. Chose a lighting model. Do we use the Blim-Phong or some other model? Do we include distance attenuation? Do we want two-sided lighting? 2. Write the shader to implement the model. 3. Finally, we have to transfer the necessary data to the shader. Some data can be transferred using uniform variables, and other data can be transferred as vertex attributes. On page 318: The calculation view_direction = v ‚Äì origin; should be view_direction = origin ‚Äì v; Summary Interactive Computer Graphics Page 30 of 38 6.9 Shading of the Sphere Model The smoother the shading, the fewer polygons we need to model a sphere (or any curved surface). To obtain the smoothest possible display we can get with relatively few triangles; is to use the actual normals of the sphere foreach vertex in the approximation. For a sphere centered at the origin, the normal at a point p is simply p. 6.10 Per-Fragment Lighting By doing the lighting calculations on a per-fragment basis, as opposed to a per-vertex basis, we can obtain highly smooth and realistic looking shadings. With a fragment shader, we can do an independent lighting calculation foreach fragment. The fragment shader needs to get the interpolated values of the normal vector, the light source position, and the eye position from the rasterizer. Programmable shaders make it possible to not only incorporate more realistic lighting models in real time but also to create interesting non-photorealistic effects. Two such examples are the use of only a few colours and emphasizing the edges in objects. 6.11 Nonphotorealistic Shading 6.12 Global Illumination Since each object is shaded independently, there are limitations imposed by the local lighting models. Shadows, reflections, and blockage of light are global effects and require a global lighting model, but these models are incompatible with the pipeline architecture. Rendering strategies, including ray tracing and radiosity, can handle global effects. Ray tracing starts with the synthetic- camera model but determines foreach projector that strikes a polygon if that point is indeed illuminated by one or more sources before computing the local shading at each point. A radiosity renderer is based upon energy considerations. A ray tracer is best suited to a scene consisting of highly reflective surfaces, whereas a radiosity renderer is best suited for a scene in which all the surfaces are perfectly diffuse. Summary Interactive Computer Graphics Page 31 of 38 Chapter 7: Discrete Techniques 7.3 Mapping Methods Mapping algorithms can be thought of as either modifying the shading algorithm based on a two- dimensional array, the map, or as modifying the shading by using the map to alter surface parameters, such as material properties and normals. There are three major techniques: ÔÇ∑ Texture mapping: Uses an image (texture) to influence the colour of a fragment. The texture image can either be a digitized image or generated by a procedural texture-generation method. ÔÇ∑ Bump mapping: Distorts the normal vectors during the shading process to make the surface appear to have small variations in shape, such as the bumps on a real orange. ÔÇ∑ Environment mapping (or reflection mapping): Allow us to create images that have the appearance of reflected materials without our having to trace reflected rays. An image of the environment is painted onto the surface as that surface is being rendered. All three methods: ÔÇ∑ Alter the shading of individual fragments as part of fragment processing; ÔÇ∑ Rely on the map being stored as a one-, two-, or three-dimensional digital image; ÔÇ∑ Keep the geometric complexity low while creating the illusion of complex geometry; and ÔÇ∑ Are subject to aliasing errors. 7.4 Texture Mapping Conceptually, texture mapping is the process of mapping a small area of the texture pattern to the area of the geometric surface, corresponding to a pixel in the final image. In most applications, textures start out as two-dimensional images that might be formed by application programs or scanned in from a photograph, but, regardless of their origin, they are eventually brought into processor memory as arrays. We call the elements of these arrays texels. We can think of this array as a continuous rectangular two- dimensional texture pattern Ôøø(Ôøø, Ôøø). The independent variables Ôøø and Ôøø are known as texture coordinates and vary over the interval [0, 1]. 7.5 Texture Mapping in WebGL WebGL‚Äôs texture maps rely on its pipeline architecture. There are actually two parallel pipelines: the geometric pipeline and the pixel pipeline. For texture mapping, the pixel pipeline merges with fragment processing after rasterization. Texture mapping is done as part of fragment processing. Texture mapping requires interaction among the application program, the vertex shader, and the fragment shader. There are three basic steps: 1. We must form a texture image and place it in texture memory on the GPU; 2. We must assign texture coordinates to each fragment; 3. We must apply the texture to each fragment. Texture objects allow the application program to define objects that consist of the texture array and the various texture parameters that control its application to surfaces. Many of the complexities of Summary Interactive Computer Graphics Page 32 of 38 how we can apply the texture are inside the texture object and thus will allow us to use very simple fragment shaders. To create a texture object: 1. Get some unused texture identifiers by calling glGenTextures(); 2. Bind the texture object (glBindTexture()) to make it the current texture object; 3. Use texture functions to specify the texture image and its parameters, which become part of the current texture object. The key element in applying a texture in the fragment shader is the mapping between the location of a fragment and the corresponding location within the texture image where we will get the texture colour for that fragment. We specify texture coordinates as a vertex attribute in the application. We then pass these coordinates to the vertex shader and let the rasterizer interpolate the vertex texture coordinates to fragment texture coordinates. The key to putting everything together is a variable called a sampler which most often appears only in a fragment shader. A sampler variable provides access to a texture object, including all its parameters. Aliasing of textures is a major problem. When we map texture coordinates to the array of texels, we rarely get a point that corresponds to the centre of a texel. There are two basic strategies: ÔÇ∑ Point sampling: We use the value of the texel that is closest to the texture coordinate output by the rasterizer. This strategy is the one most subject to visible aliasing errors. ÔÇ∑ Linear filtering: We use a weighted average of a group of texels in the neighborhood of the texel determined by point sampling. This results in smoother texturing. The size of the pixel that we are trying to colour on the screen may be smaller or larger than one texel (i.e. the resolution of the texture image does not match the resolution of the area on the screen to which the texture is mapped). If the texel is larger than one pixel (the resolution of the texture image is less than that of the area on the screen), we call it magnification; if the texel is smaller than one pixel (the resolution of the texture image is greater than that of the area on the screen), it is called minification. Mipmaps can be used to deal with the minification problem. For objects that project to an area of screen space that is small compared with the size of the texel array, we do not need the resolution of the original texel array. WebGL allows us to create a series of texture arrays, called the mipmap hierarchy, at reduced sizes. WebGL will automatically use the appropriate sized mipmap from the mipmap hierarchy. 7.7 Environment Maps Highly reflective surfaces are characterized by specular reflections that mirror the environment. Environment mapping or reflection mapping is a variant of texture mapping that can give approximate results that are visually acceptable. We can achieve this effect by using a two-step rendering pass: 1. In the first pass, we render the scene without the reflecting object, say a mirror, with the camera placed at the centre of the mirror pointed in the direction of the normal of the mirror. Thus, we obtain an image of the objects in the environment as ‚Äúseen‚Äù by the mirror. 2. We can then use this image to obtain the shades (texture values) to place on the mirror for the second, normal, rendering with the mirror placed back in the scene. Summary Interactive Computer Graphics Page 33 of 38 There are two difficulties with this approach: 1. The images that we obtain in the first pass are not quite correct, because they have been formed without one of the objects ‚Äì the mirror‚Äì in the environment. 2. The mapping issue: onto what surface should we project the scene in the first pass, and where should we place the camera? The classic approach to solve the second difficulty is to project the environment onto a sphere centered at the centre of projection. WebGL supports a variation of this method called sphere mapping. The application program supplies a circular image that is the orthographic projection of the sphere onto which the environment has been mapped. The advantage of this method is that the mapping from the reflection vector to two-dimensional texture coordinates on this circle is simple and can be implemented in either hardware or software. The difficult part is obtaining the required circular image. Another approach is to compute six projections, corresponding to the six sides of a cube, using six virtual cameras located at the centre of the box, each pointing in a different direction. Once we computed the six images, we can specify a cube map in WebGL with six function calls, one foreach face of a cube centered at the origin. The advantage of this approach is that we can compute the environment map using the standard projections that are supported by the graphics system. These techniques are examples of multi-pass rendering (or multi-rendering) techniques, where, in order to compute a single image, we compute multiple images, each using the rendering pipeline. 7.9 Bump Mapping Bump mapping is a texture-mapping technique that can give the appearance of great complexity in an image without increasing the geometric complexity. The technique of bump mapping varies the apparent shape of the surface by perturbing the normal vectors as the surface is rendered; the colours that are generated by shading then show a variation in the surface properties. Unlike simple texture mapping, bump mapping will show changes in shading as the light source or object moves, making the object appear to have variations in surface smoothness. Bump mapping cannot be done in real time without programmable shaders. In the case of bump mapping, the texture map, called a normal map, stores displacement (height) values. 7.11 Blending Techniques WebGL provides a mechanism through alpha(Ôøø) blending, that can, among other effects, create images with translucent objects. The alpha channel is the fourth colour in RGBA (or RGBÔøø) colour mode. Like the other colours, the application program can control the value of A (or Ôøø) for each pixel. However, in RGBA mode, if blending is enabled, the value of Ôøø controls how the RGB values are written into the frame buffer. Because fragments from multiple objects can contribute to the colour of the same pixel, we say that these objects are blended or composited together. The opacity of a surface is a measure of how much light penetrates through that surface. An opacity of 1 (ùõº = 1) corresponds to a completely opaque surface that blocks all light incident on it. A surface with an opacity of 0 is transparent; all light passes through it. The transparency or translucency of a surface with opacity ùõº is given y 1 ‚àí ùõº. Summary Interactive Computer Graphics Page 34 of 38 To use blending in WebGL is straightforward: 1. Enable blending by glEnable(GL_BLENDING); 2. Set up the desired source and destination factors by glBlendFunc(source_factor, destination_factor); 3. The application program must use RGBA colours. The major difficulty with compositing is that for most choices of the blending factors, the order in which we render the polygons affects the final image. Consequently, unlike most WebGL programs where the user does not have to worry about the order in which polygons are rasterized, to get a desired effect we must now control this order within the application. In applications where handling of translucency must be done in a consistent and realistic manner, we often must sort the polygons from front to back within the application. Then depending on the application, we can do a front-to- back or back-to-front rendering using WebGL‚Äôs blending functionality. A more subtle but visibly apparent problem occurs when we combine opaque and translucent objects in a scene. In a scene containing both opaque and translucent polygons, any polygon (or part of a polygon) behind an opaque polygon should not be rendered, but polygons (or parts of polygons) behind translucent polygons should be composited. If all polygons are rendered with the standard z- buffer algorithm, compositing will not be performed correctly, particularly if a translucent polygon is rendered first, and an opaque behind it is rendered later. However, if we make the z-buffer read- only when rendering translucent polygons, we can prevent the depth information from being updated when rendering translucent objects. In other words, if the depth information allows a pixel to be rendered, it is blended (composited) with the pixel already stored there. If the pixel is part of an opaque polygon, the depth data is updated, but if it is a translucent pixel, the depth data is not updated. One of the major uses of the Ôøø channel is for antialiasing. When rendering a line, instead of colouring an entire pixel with the colour of the line if it passes through it, the amount of contribution of the line to the pixel is stored in the pixels alpha value. This value is then used to calculate the intensity of the colour (specified by the RGB values), and avoids the sharp contrasts and steps of aliasing. Rather than antialiasing individual lines and polygons, we can anti alias the entire scene using a technique called multisampling. In this mode, every pixel in the frame buffer contains a number of samples. Each sample is capable of storing a colour, depth, and other values. When a scene is rendered, it is as if the scene is rendered at an enhanced resolution. However, when the image must be displayed in the frame buffer, all of the samples foreach pixel are combined to produce the final pixel colour. Chapter 8: Geometry to Pixels Clipping involves eliminating objects that lie outside the viewing volume and thus cannot be visible in the image. Rasterization produces fragments from the remaining objects. Hidden-surface removal determines which fragments correspond to objects that are visible, namely, those that are in the view volume and are not blocked from view by other objects closer to the camera. Summary Interactive Computer Graphics Page 35 of 38 8.1 Basic Implementation Strategies At a high level, we can consider the graphics system as a black box whose inputs are the vertices and states defined in the program ‚Äì geometric objects, attributes, camera specifications ‚Äì and whose output is an array of coloured pixels in the frame buffer. Within this black box, we must do many tasks, including transformations, clipping shading, hidden-surface removal, and rasterization of the primitives that can appear on the display. Every geometric object must be passed through this system, and we must assign a colour to every pixel in the colour buffer that is displayed. In the object-oriented approach, we loop over the objects. A pipeline renderer fits this description Vertices flow through a sequence of modules that transform them, colours them, and determines whether they are visible. Data(vertices) flow forward through the system. Because we are doing the same operations on every primitive, the hardware to build an object-based system is fast and relatively inexpensive. Because each geometric primitive is processed independently, the main limitation of object-oriented implementations is that they cannot handle most global calculations. Image-oriented approaches loop over pixels, or rows of pixels called scan-lines, that constitute the frame buffer. Foreach pixel, we work backward, trying to determine which geometric primitives can contribute to its colour. The main disadvantage of this approach is that all the geometric data must be available at all times during the rendering process. 8.2 Four Major Tasks There are four major tasks that any graphics system must perform to render a geometric entity: 1. Modelling: The results of the modelling process are sets of vertices that specify a group of geometric objects supported by the rest of the system. Because the modeler knows the specifics of the application, it can often use a good heuristic to eliminate many, if not most, primitives before they are sent on through the standard viewing process. 2. Geometry processing: The goals of the geometry processor are to determine which geometric objects can appear on the display and to assign shades or colours to the vertices of these objects. Four processes are required: projection, primitive assembly, clipping, and shading. The first step is to apply the model-view transformation. The second step is to transform vertices using the projection transformation. Vertices are now represented in clip coordinates. Before clipping can take place, vertices must be grouped into primitives. After clipping takes place, the remaining vertices are still in four-dimensional homogeneous coordinates. Perspective division converts them to three-dimensional representations in normalized device coordinates. Per-vertex shading is also performed during this stage. 3. Rasterization (scan conversion): The rasterizer starts with vertices in normalized device coordinates and outputs fragments whose locations are in units of the display - window coordinates. Although only the Ôøø and Ôøø values of the vertices are needed to determine which pixels in the frame buffer can be affected by the primitive, we still need to retain depth information for hidden-surface removal. For line segments, rasterization determines which fragments should be used to approximate a line segment between the projected vertices. For polygons, rasterization determines which pixels lie inside the two-dimensional polygon determined by the projected vertices. The colours that we assign to these fragments can be determined by the vertex attributes or obtained by interpolating the shades at the vertices that were computed. Screen coordinates refer to the two-dimensional system that is the same as window coordinates but lacks the depth coordinate. Summary Interactive Computer Graphics Page 36 of 38 Ôøø = Ôøømax Ôøø = Ôøømin 4. Fragment processing: Per-fragment shading, texture-mapping, bump-mapping, alpha blending, antialiasing, and hidden-surface removal all take place in this stage. 8.3 Clipping Clipping is the process of determining which primitives, or parts of primitives, should be eliminated because they lie outside the viewing volume. Clipping is done before the perspective division that is necessary if the Ôøø component of a clipped vertex is not equal to 1. The portions of all primitives that can possibly be displayed lie within the cube ùë§ ‚â• ùë• ‚â• ‚àíùë§, ùë§ ‚â• ùë¶ ‚â• ‚àíùë§, ùë§ ‚â• ùëß ‚â• ‚àíùë§. Note that projection has been carried out only partially at this stage: perspective division and the final orthographic projection must still be performed. 8.4 Line-Segment Clipping A clipper decides which primitives, or parts of primitives, can possibly appear on the display and be passed on to the rasterizer. Primitives that fit within the specified view volume pass through the clipper (are accepted); those that fall outside are eliminated (or rejected or culled); and those that are only partially within the view volume must be clipped such that any part lying outside the volume is removed. Cohen-Sutherland clipping: The algorithm starts by extending the sides of the clipping rectangle to infinity, thus breaking up space into the nine regions shown in the diagram below. 1001 1000 1010 0001 0000 0010 0101 0100 0110 Ôøø = Ôøømin Ôøø = Ôøømax Each region is assigned a unique 4-bit binary number called an outcode, Ôøø0Ôøø1Ôøø2Ôøø3, as follows. Suppose that (Ôøø, Ôøø) is a point in the region; then Likewise, ùëè1 is 1 if ùë¶ < ùë¶min, and ùëè2 and ùëè3 are determined by the relationship between ùë• and the left and right sides of the clipping window. The resulting codes are indicated in the diagram above. For each endpoint of a line segment, we first compute the endpoint‚Äôs outcode. Consider a line segment whose outcodes are given by ùëú1 = ùëúùë¢ùë°ùëêùëúùëëùëí(ùë•1,ùë¶1) and ùëú2 = ùëúùë¢ùë°ùëêùëúùëëùëí(ùë•2, ùë¶2). There are four cases to consider: 1. (Ôøø1 = Ôøø2 = 0). Both endpoints are inside the clipping window, as is true for segment AB in the figure below. The entire line segment can be sent on to be rasterized. 2. (ùëú1 ‚â† 0, ùëú2 = 0; or vice versa). One endpoint is inside the clipping window; one is outside (see segment CD in the figure below). The line segment must be shortened. The nonzero outcode indicates which edge or edges of the window are crossed by the segment. One or two intersections must be computed. Note that after one intersection is computed, we can Summary Interactive Computer Graphics Page 37 of 38 compute the outcode of the point of intersection to determine whether another intersection calculation is required. 3. (ùëú1 & ùëú2 ‚â† 0). By taking the bitwise AND of the outcodes, we determine whether or not the two endpoints lie on the same outside side of the window. If so, the line segment can be discarded (see segment EF in the figure below). 4. (Ôøø1 & Ôøø2 = 0). Both endpoints are outside, but they are on the outside of different edges of the window. As we can see from segments GH and IJ in the figure below, we cannot tell from just the outcodes whether the segment can be discarded or must be shortened. The best we can do is to intersect with one of the sides of the window and to check the outcode of the resulting point. Thus, with this algorithm we do intersection calculations only when they are needed, as in the second case, or where the outcodes did not contain enough information, as in the fourth case. The Cohen-Sutherland algorithm works best when there are many line segments but few are actually displayed (line segments lie fully outside one or two of the extended sides of the clipping rectangle). This algorithm can be extended to three dimensions. The main disadvantage of the algorithm is that it must be used recursively. Liang-Barsky Clipping: Suppose that we have a line segment defined by the two endpoints Ôøø1 = [Ôøø1, Ôøø1]Ôøø and Ôøø2 = [Ôøø2,Ôøø2]Ôøø. We can parametrically express this line in either matrix form: ùëù(ùõº) = (1 ‚àí ùõº)ùëù1 + ùõºùëù2, or as two scalar equations: ùë•(ùõº) = (1 ‚àí ùõº)ùë•1 + ùõºùë•2, ùë¶(ùõº) = (1 ‚àí ùõº)ùë¶1 + ùõºùë¶2. As the parameter Ôøø varies from 0 to 1, we move along the segment from Ôøø1 to Ôøø2; negative values of Ôøø yield points on the line on the other side of Ôøø1 from Ôøø2; values of Ôøø > 1 gives points on the line past Ôøø2. Consider a line segment and the line of which it is part, as shown in the figure below As long as the line is not parallel to a side of the window (if it is, we can handle that situation with ease), there are four points where the line intersects the extended sides of the window. These points correspond to the four values of the parameter: Ôøø1 (bottom), Ôøø2 (left), Ôøø3 (top) and Ôøø4 (right). We can order these values and determine which correspond to intersections (if any) that we need for clipping. For the first example, 0 < Ôøø1 < Ôøø2 < Ôøø3 < Ôøø4 < 1. Hence, all four intersections are inside the original line segment, with the two innermost (Ôøø2 and Ôøø3) determining the clipped line segment. The case in the second example also has the four intersections between the endpoints of the line segment, but notice that the order for this case is 0 < Ôøø1 < Ôøø3 < Ôøø2 < Ôøø4 < 1. The line intersects both the top and the bottom of the window before it intersects either the left or the right; Summary Interactive Computer Graphics Page 38 of 38 thus, the entire line segment must be rejected. Other cases of the ordering of the points of intersection can be argued in a similar way. The efficiency of this approach, compared to that of the Cohen-Sutherland algorithm, is that we avoid multiple shortening of line segments and the related re-executions of the clipping algorithm. 8.8 Rasterization Pixels have attributes that are colours in the colour buffer. Fragments are potentially pixels. Each fragment has a colour attribute and a location in screen coordinates that corresponds to a location in the colour buffer. Fragments also carry depth information that can be used for hidden-surface removal. The DDA algorithm (Digital Differential Analyser): Suppose that we have a line segment defined by the endpoints (Ôøø1,Ôøø1) and (Ôøø2,Ôøø2). Because we are working in a colour buffer, we assume that these are all integer values. The slope of his line is given by This algorithm is based on writing a pixel foreach value of ix in write_pixel as Ôøø goes from Ôøø1 to ùë•2. For any change in ùë• equal to ‚àÜùë•, the corresponding changes in ùë¶ must be ‚àÜùë¶ = ùëö‚àÜùë•. As we move from Ôøø1 to Ôøø2, we increase Ôøø by 1 in each iteration; thus, we must increase Ôøø by ‚àÜùë¶ = ùëö. This algorithm in pseudo code is: for(ix = x1; ix <= x2;ix++){ y += m; write_pixel(x, round(y), line_colour); } For large slopes, the separation between fragments can be large, generating an unacceptable approximation to the line segment. To alleviate this problem, for slopes greater than 1, we swap the roles of Ôøø and Ôøø. 8.9 Bresenham‚Äôs Algorithm The DDA algorithm requires a floating-point addition foreach pixel generated. The Bresenham algorithm avoids all floating point calculations and has become the standard algorithm used in hardware and software rasterizers. The calculation of each successive pixel in the colour buffer requires only an addition and a sign test. 8.10 Polygon Rasterization One of the major advantages that the first raster systems brought to users was the ability to display filled polygons. Flat simple polygons have well-defined interiors. If they are also convex, they are guaranteed to be rendered correctly by WebGL. Summary Interactive Computer Graphics Page 39 of 38 Inside-outside testing: Conceptually, the process of filling the inside of a polygon with a colour or pattern is equivalent to deciding which points in the plane of the polygon are interior(inside) points. ÔÇ∑ The crossing (or odd-even) test is the most widely used test for making inside-outside decisions. Suppose that p is a point inside a polygon. Any ray emanating from p and going off to infinity must cross an odd number of edges. Any ray emanating from a point outside the polygon and entering the polygon crosses an even number of edges before reaching infinity. Usually, we replace rays through points with scan-lines, and we count the crossing of polygon edges to determine inside and outside. ÔÇ∑ The winding test considers the polygon as a knot being wrapped around a point or a line. We start by traversing the edges of the polygon from any starting vertex and going around the edge in a particular direction until we reach the starting point. Next we consider an arbitrary point. The winding number for this point is the number of times it is encircled by the edges of the polygon. We count clockwise encirclements as positive and counter- clockwise encirclements as negative. A point is inside the polygon if its winding number is not zero. Since WebGL can only render triangles; to render a more complex (non-flat or concave) polygon, we can apply a tessellation algorithm to this polygon to subdivide it into triangles. A goo tessellation should not produce triangles that are long and thin; it should, if possible, produce sets of triangles that can use supported features, such as triangle strips and triangle fans. Flood fill: This algorithm works directly with pixels in the frame buffer. We first rasterize a polygon‚Äôs edges into the frame buffer using Bresenham‚Äôs algorithm. If we can find an initial point (ùë•, ùë¶) that lie inside these edges (called a seed point), we can look at its neighboring pixels recursively, colouring them with the fill colour only if they are not coloured already. We can obtain a number of variants of flood fill by removing the recursion. One way to do so is to work one scan-line at a time (called scan- line fill). 8.11 Hidden Surface Removal Hidden-surface removal (or visible-surface determination) is done to discover what part, if any, of each object in the view volume is visible to the viewer or is obscured from the viewer by other objects. Culling: For situations where we cannot see back faces, such as scenes composed of convex polyhedral, reduce the work required for hidden-surface removal by eliminating all back-facing polygons before we apply any other hidden-surface-removal algorithms. A polygon is facing forward if and only if ùëõ ‚àô ùë£ ‚â• 0, where ùë£ is in the direction of the viewer and ùëõ is the normal to the front face of the polygon. Usually, culling is performed after the transformation to normalized device coordinates (perspective division). The z-buffer is a buffer which usually has the same spatial resolution as the colour buffer, and before each scene rendering, each of its elements is initialized to a depth corresponding to the maximum distance away from the centre of projection. At any time during rasterization and fragment processing, each location in the z-buffer contains the distance along the ray corresponding to the location of the closest polygon found so far. Rasterization is done polygon by polygon using some rasterization algorithm. Foreach fragment on the polygon corresponding to the intersection of the polygon with a ray through a pixel, we compute the depth from the centre of projection. The Summary Interactive Computer Graphics Page 40 of 38 method compares this depth to the value in the z-buffer corresponding to this fragment. If this depth is greater than the depth in the z-buffer, this fragment is discarded. If the depth is less than the depth in the z-buffer, update the depth in the z-buffer and place the shade computed for this fragment at the corresponding location in the colour buffer. The z-buffer algorithm is the most widely used hidden-surface-removal algorithm. It has the advantages of being easy to implement, in either hardware or software, and of being compatible with pipeline architectures, where it can execute at the speed at which fragments are passing through the pipeline. Suppose that we have already computed the z-extents of each polygon. The next step of depth sort is to order all the polygons by how far away from the viewer their maximum z-value is. If no two polygons‚Äô z-extents overlap, we can paint the polygons back to front and we are done. However, if the z-extents of two polygons overlap, we still may be able to find an order to paint (render) the polygons individually and yield the correct image. The depth-sort algorithm runs a number of increasingly more difficult tests, attempting to find such an ordering. Two troublesome situations remain: If three or more polygons overlap cyclically, or if a polygon can pierce another polygon, there is no correct order for painting without having to subdivide some polygons. The main idea behind this class of algorithms is that if one object obscures part of another then the first object is painted after the object that it obscures. The painter‚Äôs algorithm is an example of such a method. 8.12 Antialiasing Rasterized line segments and edges of polygons can appear jagged. Aliasing errors are caused by three related problems with the discrete nature of the frame buffer: 1. If we have an ùëõ √ó ùëö frame buffer, the number of pixels is fixed, and we can generate only certain patterns to approximate a line segment. 2. Pixel locations are fixed on a uniform grid; regardless of where we would like to place pixels, we cannot place them at other than evenly spaced locations. 3. Pixels have a fixed size and shape. The scan-conversion algorithm forces us, for lines of slope less than 1, to choose exactly one pixel value foreach value of Ôøø. If, instead, we shade each box by the percentage of the ideal line that crosses it, we get a smoother looking rendering. This technique is known as antialiasing by area averaging. If polygons share a pixel, and each polygon has a different colour, the colour assigned to the pixel is the one associated with the polygon closest to the viewer. We could obtain a much more accurate image if we could assign a colour based on an area-weighted average of the colours of these polygons. Such algorithms can be implemented with fragment shaders on hardware with floating point frame buffers. These algorithms are collectively known as spatial-domain aliasing. Terminology: Pipeline: ÔÇ∑ Affine: Line preserving ÔÇ∑ Nonuniform foreshortening: images of objects farther away from COP are reduced in size compared to closer images. ÔÇ∑ Vertex processing: each Vertex are processed independently. Per-vertex lighting calculation done here ÔÇ∑ Clipping and primitive assembly: Sets of vertices assembled into primitives. Output is a set of primitives whose projections should be appear in the image. ÔÇ∑ Rasterization: Primitives that emerge from clipper are still represented in terms of their vertices and must be converted to pixels in frame buffer. Output of rasterizer is a set of primitives whose projections should appear in the image. ÔÇ∑ Fragment processing: takes fragments generated by rasterizer and updates pixels in frame buffer. Hidden surface removal; texture mapping, bump mapping, and alpha blending can be applied here. ÔÇ∑ Advantages: o Increases performance when same sequence of concurrent operations on many, or large, data sets o Process on each primitive can be done independently ÔÇ∑ Disadvantages: o Latency of the system must be balanced against increased throughput. o Global effects may not be handled correctly. Sierpinski Gasket: ÔÇ∑ Immediate mode graphics: As vertices are generated, they are sent to graphics processor for rendering on display. No memory of geometric data ÔÇ∑ Retained mode graphics: All geometric data is computed and stored in some data structure. Then the scene is displayed by sending all data to graphics processor at once. Overheads are avoided from sending small amounts of data to graphics processor for each vertex generated. API Function groups: PAVTICQ Colour: ÔÇ∑ Primitive functions: Define low level objects or atomic entities that the system can display. ÔÇ∑ Attribute functions: Governs the way primitives appear on display. ÔÇ∑ Viewing functions: Allows us to specify various views. ÔÇ∑ Transformation functions: Allows us to carry out transformations of objects. (rotation, translation, and scaling) ÔÇ∑ Input functions: Deal with input devices ÔÇ∑ Control functions: Communicate with window system, initialise our programs, and deal with any errors taking place during program execution. ÔÇ∑ Query functions: Allow us to obtain info about operating environment, camera parameters, values in frame buffer etc. Primitives and attributes: ÔÇ∑ Well defined interior if it satisfies the following: o Simple: In 2D, as long as no 2 edges of a polygon cross each other o Convex: Object is convex if all points on the line segment between any two points inside the object, or on the boundary, are inside the object. Triangles, tetrahedral, rectangles, parallelepipeds, circles, and spheres. o Flat: All vertices specifying the polygon lie in the same plane. Forms of text: ÔÇ∑ Stroke text: Constructed as are other geometric objects. Vertices are used to specify line segments or curves outlining each character. Can be manipulated like any other graphical primitive ÔÇ∑ Raster text: Characters are defined as rectangles of bits, called bit blocks. Each block defines single character by pattern of 0 and 1 bits in the block. ÔÇ∑ Additive: 3 primary colours (RGB) add together to give perceived colour ÔÇ∑ Subtractive: Start off with white surface. Coloured pigments remove colour components from light striking a surface. CMY is used ÔÇ∑ Colour cube: View a colour as a point in a colour solid. ÔÇ∑ RGB System: Each pixel may consist of 24 bits (3 bytes), one for each colour. Specification is based on the colour cube. Numbers between 0.0 and 1.0 denote the saturation of each colour. ÔÇ∑ Indexed colour: Frame buffers limited in depth. 8 bits deep, not subdivided into groups, but the limited depth pixel interpreted as an integer value indexing to the colour lookup table. Only works with dynamic images that must be shaded. Types of events: ÔÇ∑ Mouse events: when mouse button is pressed or depressed, or mouse is moved with mouse button ÔÇ∑ Reshape events: When user resizes the window. ÔÇ∑ Keyboard events: Generated when mouse is in window and one of the keys is pressed or released ÔÇ∑ Idle callback: invoked when there are no other events. Double Buffering: ÔÇ∑ Distortion caused when a redisplay of the frame buffer can cause a partially drawn display. ÔÇ∑ Common solution is double buffering. o Hardware has 2 frame buffers o Front buffer: Buffer that is displayed o Back buffer: Available for constructing what to display next o Then buffers are swopped, and new back buffer is cleared and starts drawing next display. Homogenous Coordinates: ÔÇ∑ Advantages: o All affine transformations can be represented using matrix multiplications o We can carry out operations on points and vectors using homogenous coordinate representations and ordinary matrix algebra. o Uniform representation of all affine transformations makes carrying out successive transformations far easier than in 3D space o Less arithmetic is involved o Modern hardware implements homogenous coordinate operations directly, using parallelism to achieve high speed calculations. Transformations: OWECNWS ÔÇ∑ Frames in WebGL: o 1. Object coordinates: Object details are kept in the model or object frame o 2. World coordinates: A scene may contain many objects. After the application program applies a sequence of transformations to each object, to display in the frame. This frame is called world frame ‚Äì values in world coordinates. o 3. Eye coordinates: All graphics systems use a frame with origin is the centre of camera. Frame is called the camera or eye frame. o 4. Clip coordinates: Once objects are in clip coordinates, WebGL checks whether they lie within view volume. If not, clipped from scene prior to rasterization. o 5. Normalised device coordinates: Vertices still represented in homogenous coordinates, division by the 4th component, called perspective division, yields 3D representations in normalised device coordinates. o 6. Window coordinates: Final transformation take position in normalised device coordinates and taking the viewport into account, creates a 3D representation in window coordinates. o 7. Screen coordinates: if depth coordinate is removed, we work with 2D screen coordinates. Rotation, Translation, and scaling: o Translation: Displaces point by a fixed distance in a given direction. o Rotation: Rotates points by fixed angle about a point or line. o Both rotation and translation are known as rigid body transformations. No combination of rotations and translations can alter shape or volume of an object. Only location and orientation. o Scaling: Affine non rigid body transformation by which we can make objects bigger or smaller. Scaling has a fixed point, unaffected by transformation. ÔÇß Uniform: Scaling factor is identical in all directions. Shape of scaled object is preserved ÔÇß Non uniform: Scaling factor of each direction not be identical, shape is distorted. Transformation Matrices in GL: ÔÇ∑ Most used transformations: o Model-view transformation: Affine transformation matrix bringing representations of geometric objects from application or model frames to the camera frame. o Projection transformation: Projection matrix is usually not affine and is responsible for carrying out both desired projections, and also changes the representation to clip coordinates Projection normalisation: ÔÇ∑ Projection normalisation: o Technique converts all projections into simple orthogonal projections by distorting objects such that the orthogonal projection of distorted objects is same as desired projection of original objects. o Done by applying a matrix called normalisation matrix o Vertices transformed such that vertices within specified view volume are transformed to vertices within the canonical view volume. o Advantages: ÔÇß Both perspective and parallel views can be supported by same pipeline. ÔÇß Clipping process is simplified because sides of canonical view volume are aligned within coordinate axes o Shape of viewing volume for an orthogonal projection is a right parallelepiped: ÔÇß Perform translation to move centre specified view volume to centre of canonical view volume. ÔÇß Scale sides of specified view volume such that they have length of 2. Positioning of camera: ÔÇ∑ Model view matrix encapsulates relationship between camera frame and object frame o Modelling transformation: ÔÇß Takes instances of objects in object coordinates and brings them into world frame. o Viewing transformations: ÔÇß Transforms world coordinates to camera coordinates. ÔÇ∑ First method for construction: o Concatenating a carefully selected series of affine transformations. ÔÇ∑ Second approach: o Specify camera frame with respect to the world frame and construct matrix, called view-orientation matrix, taking us from world to camera coordinates. o 3 parameters need to be specified: ÔÇß View reference point VRP: Specifies location of COP in world coordinates ÔÇß View plane normal VPN: Also known as n, specifies normal to the projection plane ÔÇß View-up vector VUP: Specifies what direction is up from camera‚Äôs perspective. ÔÇ∑ Third method: o Called look at function and similar to second approach. o Only differs in way VPN is specified ÔÇ∑ Depth sort: ÔÇß Complexity is proportional to number of fragments generated by rasterizer ÔÇß Can be implemented with small number of additional calculations. ÔÇß Easy to implement ÔÇß Compatible with pipeline architecture. Frustum projection: Hidden Surface removal: ÔÇ∑ Algorithms removing surfaces that cannot be seen from the viewer. ÔÇ∑ Classes: o Object space: Attempt to order surfaces of objects in the scene such that rendering surfaces in a particular order provide correct image. o Image space: Work as part of projection process and seek to determine relationship among object points on each projector. ÔÇ∑ Z-Buffer: o Stores depth data of objects o Has same spatial resolution as colour buffer. o Advantages: o Orders all polygons by how far away from the viewer their max z- value is. o If none overlap, we paint the polygons back to front and we are done. o If they overlap, we may still be able to find an order to paint the polygons individually and yield correct image. o The algorithm runs increasingly more difficult tests, attempting to find such ordering. ÔÇ∑ Culling: For situations where back faces cannot be seen, reduce work required for hidden surface removal by eliminating all back-facing polygons before applying other hidden surface removal algorithms. o Usually performed after transformation to normalized device coordinates. Light and matter: ÔÇ∑ Classified into 3 groups: o Specular surfaces: Appear shiny, most light that is reflected or scattered in a narrow range of angles close to angle of reflection. Perfectly specular is when all light reflected emerges at a single angle. o Diffuse surfaces: Reflected light is scattered in all directions. Perfectly diffuse surfaces scatter light equally in all directions. o Translucent surfaces: Allow some light to penetrate and to emerge from another location on the object. Glass and water. Light sources: ÔÇ∑ Ambient light: Lights been designed and positioned to provide uniform illumination Meshes: ÔÇß ‚àù increased: reflected light is concentrated in narrower region centered on angle of a perfect reflector. ÔÇ∑ Point sources: Ideal point source emits light equally in all directions. ÔÇ∑ Spotlights: Characterised by narrow range of angles through which light is emitted. ÔÇ∑ Distant light source: All rays parallel and we replace location of the light source with direction of the light. Reflection models: ÔÇ∑ Phong: o Four vectors used to colour arbitrary point p on a surfaces n ‚Äì normal at p v ‚Äì in direction from p to the viewer or COP l ‚Äì in direction of a line from p to arbitrary point on light source r ‚Äì in direction a perfectly reflected ray from l would take o Supports diffuse, ambient, and specular. o Lambert‚Äôs law: We only see the vertical component of incoming light ÔÇß Ôøø is angle between normal at point of interest n and direction of light source l. ÔÇ∑ Set of polygons that share vertices and edges ÔÇ∑ Can be used to display height data. Polygonal shading: ÔÇ∑ Flat shading: constant shading. o Shading calculation only carried out once for each polygon, and each point on polygon is assigned the same shade o Will show differences among adjacent polygons ÔÇ∑ Gouraud shading: smooth shading o Lighting calculation done at each vertex using material properties and vectors n, v, and l. o Each vertex will have its own colour that the rasterizer can use to interpolate a shade for each fragment. ÔÇ∑ Phong shading: o Instead of interpolating vertex intensities, we interpolate normal across each polygon. o Independent lighting calculation for each fragment is made. Implementing lighting model: ÔÇ∑ Can be implemented in application, vertex shading, or fragment shader. ÔÇ∑ For efficiency sake, we almost always wat to do lighting calculations in shaders. ÔÇ∑ 3 steps to carry out implementation: o Choose lighting model o Write shader to implement model o Finally, transfer necessary data to the shader Global vs Local shading: ÔÇ∑ Global: Shadows, reflections, and blockage of light are global effects and require global lighting model. o Disadvantages: global models are incompatible with the pipeline architecture. ÔÇ∑ Local: Used when objects are shaded independently Mapping methods ÔÇ∑ Texture mapping: o Uses an image to influence the colour of a fragment. o Texture can be digitised image or generated by a procedural texture generation method. o WebGL: ÔÇß Texture maps rely heavy on pipeline architecture. ÔÇß 2 parallel pipelines ‚Äì geometric and pixel pipeline. ÔÇß Pixel pipeline merges with fragment processing after rasterization ÔÇß Done by: ÔÇ∑ Form texture image and place in texture memory on GPU ÔÇ∑ Assign texture coordinates to each fragment ÔÇ∑ Apply texture to each fragment ÔÇß Key element in applying a texture in the fragment shader is the mapping between location of a fragment, and corresponding location with texture image where we get the texture colour for that fragment. ÔÇß Strategies for mapping texture coordinates to array of texels: ÔÇ∑ Point sampling: Use value of the texel closest to the texture coordinate output but the rasterizer. Most subject to aliasing errors ÔÇ∑ Linear filtering: Use a weighted average for a group of texels in the neighborhood of texel determined by point sampling. Smoother texturing. ÔÇß Mipmaps can used to deal with minification problem ÔÇ∑ Bump mapping: o Distorts normal vectors during shading process to make surface appear to have small variations in shape. ‚Äì such as bumps on a real orange. o Technique varies the apparent shape of the surface by perturbing normal vectors as surface is rendered. o Cannot be done in real time without programmable shaders. ÔÇ∑ Environment maps: o Allows us to create images that have the appearance of reflected materials without having to trace deflected rays. o Image of environment is painted onto the surface as that surface is being rendered. o Variant of texture mapping that can give approximate results that are visually acceptable. o 2 step rendering pass: ÔÇß 1st pass: render scene without reflecting object. Camera placed at the centre of the mirror pointed in the direction of the normal of the mirror. ÔÇß 2nd pass: Then use the image to obtain the shades to place on the mirror for the second rendering with the mirror placed back in the scene o Difficulties: ÔÇß Images we obtain in the first pass are not quite correct because they were formed without the mirror ÔÇß Mapping issues: onto what surface should we project the scene in the first pass and where should the camera be placed. ‚Äì to solve project the environment onto a sphere at COP. Blending: ÔÇ∑ Alpha blending on RGB (RGBA) ÔÇ∑ The value of A controls how the RGB values are written into frame buffer. ÔÇ∑ Value of 1 is opaque. Value of 0 is transparent. ÔÇ∑ Difficulty: The order in which polygons are rendered affects final image. o In a scene containing both opaque and translucent polygons, any polygon behind an opaque polygon should not be rendered, but polygons should be composited. Z-Buffer will not composite polygons correctly if a translucent polygon is rendered first, and an opaque polygon is rendered afterwards behind it Z-buffer must be made read only when rendering translucent polygons, depth information will then be prevented from being updated when rendering translucent objects. ÔÇ∑ One major use is antialiasing. When rendering a line, instead of colouring a whole pixel with colour of line if it passes through it, amount of contribution of the line to the pixel is stored in pixels alpha value. Value then used to calculate intensity of a colour, avoiding sharp contrasts and steps of aliasing. ÔÇ∑ Rather antialiasing individual lines and polygons, we can use antialiasing on entire scenes using a technique, multisampling. Every pixel in the frame buffer contains number of samples, capable of storing colour, depth, and other values. When scene is rendered, the scene looks as if it is rendered at an enhanced resolution. When image must be displayed in the frame buffer, all samples for each pixel are combined to produce final pixel colour. Rasterization: Produces fragments from remaining objects. ÔÇ∑ Pixels have attributes that are colours in colour buffer. ÔÇ∑ Fragments are potentially pixels and each fragment has colour attribute and location in screen coordinates corresponding to location in colour buffer ÔÇ∑ DDA Algorithm: ÔÇ∑ Bresenham‚Äôs algorithm: o DDA requires a floating-point addition for each pixel generated. o Bresenham avoids all floating-point calculations and has become the standard algorithm used in hardware and software rasterizers. o Calculation of each successive pixel in the colour buffer requires only an addition and sign test ÔÇ∑ Polygon rasterization: o Crossing (odd even) test ÔÇß Any ray emanating from point outside polygon and entering the polygon crosses even number of edges before infinity ÔÇß Any ray emanating from point inside polygon and going off to infinity must cross an odd number of edges o Winding test: ÔÇß Considers the polygon as a knot being wrapped around a point or line. ÔÇß Start by traversing edges of polygon from any starting vertex and going around the edge in a particular direction until we reach the starting point. ÔÇß Next an arbitrary point is considered. ÔÇß Winding number for this point is the number of times it is encircles by the edges of the polygon. ÔÇß Count clockwise as positive and counter-clockwise as negative. ÔÇß Point is inside a polygon if its winding number is not zero Major tasks from Geometry to pixels: ÔÇ∑ Modelling: o Results are sets of vertices specifying group of geometric objects supported by rest of system ÔÇ∑ Geometry processing: o Determines which geometric objects can be appear on the display and assign shades or colour to object vertices o 4 processes: Projection, primitive assemble, clipping, and shading ÔÇ∑ Rasterization: o Rasterizer starts with vertices in normalised device coordinates and outputs fragments whose locations are in units of display o Line segments: Rasterization determines which fragments can be used to approximate a line segment between projected vertices. o Colours assigned to fragments are determined by vertex attributes or obtained by interpolating shades at vertices compound. ÔÇ∑ Fragment processing: o Per fragment shading, texture mapping, bump mapping, alpha blending, antialiasing, and hidden surface removal take place here Clipping: ÔÇ∑ Clipper decides which primitives, or parts of primitives, can possibly appear onto the display and be passed on to the rasterizer. ÔÇ∑ Those fitting within the specified view volume are accepted. ÔÇ∑ Those that fall outside are eliminated. ÔÇ∑ Those partially inside are clipped so anything outside the volume is removed. ÔÇ∑ Cohen-Sutherland clipping: o Algorithm extends sides of the clipping rectangle to infinity. o The algorithm includes, excludes or partially includes the line based on whether: ÔÇß Both endpoints are in the viewport region ÔÇß Both endpoints share at least one non-visible region, which implies that the line does not cross the visible region ÔÇß Both endpoints are in different regions: in case of this nontrivial situation the algorithm finds one of the two points that is outside the viewport region (there will be at least one point outside). The intersection of the outpoint and extended viewport border is then calculated (i.e. with the parametric equation for the line), and this new point replaces the outpoint. The algorithm repeats until a trivial accept or reject occurs. ÔÇ∑ Liang-Barsky Clipping: o Algorithm uses the parametric equation of a line and inequalities describing the range of the clipping window to determine the intersections between the line and the clip window. o More efficient that Cohen-Sutherland algorithm by avoiding multiple shortening of line segments and related re-executions of clipping algorithm o Idea of the Liang‚ÄìBarsky clipping algorithm is to do as much testing as possible before computing line intersections Antialiasing: ÔÇ∑ Caused by 3 related problems with discrete nature of the frame buffer: o If we have an n x m frame buffer, number of pixels are fixed. Only can generate certain patters to approximate a line segment. o Pixel locations are fixed on uniform grid o Pixels have a fixed size and shape. ÔÇ∑ Scan-conversion algorithm forces us, for lines of slope less than 1, to choose exactly one-pixel value for each value of x. ÔÇ∑ Antialiasing by area: We can shade each box by the percentage of ideal line that crosses it, we get a smoother looking rendering. ÔÇ∑ If polygons share a pixel, and each polygon has a different colour, colour assigned to the pixel is one associated with the polygon closest to the viewer. We could obtain a much more accurate image if we could assign a colour based on an area weighted average of colours of these polygons. 2 COS340-A October/November 2009 [TURN OVER] QUESTION 1 [10] a) OpenGL uses z-buffering for hidden surface removal. Explain how the z-buffer algorithm works and give one advantage of using this method. (5) b) Orthogonal, oblique and axonometric view scenes are all parallel view scenes. Define the term parallel view and explain the differences between orthogonal, axonometric, and oblique view scenes. (5) Answer: OpenGL uses a hidden-surface method called z-buffering that saves depth information into an extra buffer which is used as objects are rendered so that parts of objects that are behind other objects do not appear in the image. The z-buffer records the depth or the distance from the COP of a pixel. Thus, as polygons are scan-converted in random order, the algorithm checks the z-value of the pixel at the coordinates (in the frame buffer) that it is trying to draw to. If the existing pixel at these coordinates (i.e. if there already was a pixel) has a z-value further from the viewer than the pixel we are trying to draw, then the colour and depth of the existing pixel are updated, otherwise it continues without writing to the colour buffer or the depth buffer. Advantages:(1 mark for either answer) easy to implement in either software or hardware it is compatible with pipeline architectures - can execute at the same speed at which vertices are passing Answer: Parallel views - views with the COP at infinity. Orthogonal views - projectors are perpendicular to the projection plane and projection plane is parallel to one of the principal faces of an object. A single orthogonal view is restricted to one principal face of an object. Axonometric view - projectors are perpendicular to the projection plane but projection plane can have any orientation with respect to object. Oblique projection - projectors are parallel but can make an arbitrary angle to the projection plane and projection plane can have any orientation with respect to object. [TURN OVER] 3 COS340-A October/November 2009 QUESTION 2 [8] a) Define the term homogeneous coordinates and explain why they are used in computer graphics. (4) b) Consider the line segment with endpoints a and b at (1, 2, 3) and (2, 1, 0) respectively. Compute the coordinates of vertices a and b that result after an anticlockwise rotation by 15 about the z- axis.(Show your workings) (4) Hint: The transformation matrix for rotation around the z-axis is (where Œ∏ is the angle of anticlockwise rotation). Answer: Homogeneous coordinates are four dimensional column matrices used to represent both point and vectors in three dimensions. When points and vectors are represented using 3-dimensional column matrices one cannot distinguish between a point and a vector, with homogeneous coordinates we can make this distinction. marks can be awarded for any of the points mentioned below: A matrix multiplication in 3-dimensions cannot represent a change in frames, while this can be done using homogeneous coordinates. All affine transformations can be represented as matrix multiplications in homogeneous coordinates. Less arithmetic work is involved when using homogeneous coordinates. The uniform representation of all affine transformations makes carrying out successive transformations far easier than in 3 dimensional space. Modern hardware implements homogeneous coordinates operations directly, using parallelism to achieve high speed calculations. [TURN OVER] 4 COS340-A October/November 2009 (4) So a'' = (0.448, 2.191, 3) and b'' = (1.673, 1.484, 0) Similarly Then, to rotate a' and b' by 15 about the z-axis, we use the rotation matrix QUESTION 3 [8] OpenGL is a pipeline model. In a typical OpenGL application, a vertex will go through a sequence of 6 transformations or change of frames. In each frame the vertex has different coordinates. a) Name these coordinates in the order that they occur in the OpenGL vertex transformation process. [5] [TURN OVER] 5 COS340-A October/November 2009 1/2 mark each for correct names ; 2 marks for correct order Object or model coordinates World coordinates Eye or camera coordinates Clip coordinates Normalized device coordinates Window or screen coordinates b) Define the term perspective division and state specifically where it is used in the OpenGL vertex transformation process. [3] QUESTION 4 [12] a) Discuss the difference between the RGB colour model and the indexed colour model with respect to the depth of the frame (colour) buffer. (4) b) Describe, with the use of diagrams, the Cohen-Sutherland line clipping algorithm. (8) Perspective division is the division of homogeneous coordinates by the w component . It is used to transform the clip coordinates, which are still represented by homogenous coordinates into three- dimensional representations in normalized device coordinates. In both models, the number of colours that can be displayed depends on the depth of the frame (colour) buffer. The RGB model is used when a lot of memory is available, eg 12 or 24 bits per pixel. /2 These bits are divided into three groups, representing the intensity of red, green and blue at the pixel, respectively. The RGB model becomes unsuitable when the depth is small, because shades become too distinct/discreet. The indexed colour model is used where memory in the colour buffer is limited. /2 The bits per pixel are used to index into a colour-lookup table where any shades of any colours can be specified (depending only on the colours that the monitor can show). [TURN OVER] 6 COS340-A October/November 2009 Angel 7.4.1 (2 bonus marks) The algorithm starts by extending the sides of the window to infinity, thus breaking up space into nine regions. Each region is assigned a 4 -bit binary number, or outcode. as shown in the diagram below. 1001 1000 1010 1 0 10 101 100 110 For each endpoint of a line segment, we first compute the endpoint‚Äôs outcode. Consider a line segment whose outcodes are given by X(endpoint 1) and Y(endpoint 2). We have four cases: X = Y = 0 :Both endpoints are inside the clipping window(AB in diagram), segment can be rasterized. X 0, Y = 0. One endpoint is inside the window and one is outside.(segment CD) We must shorten the line segment by calculating 1 or 2 intersections. X & Y 0: By taking the butwise AND of the outcodes we determine whether or not the two endpoints lie on the same outside side of the window. If so the line is discarded.(segment EF) X & Y = 0: Both segments are outside but they are outside on different edges of the window. We cannot tell from outcodes whether segment can be discarded or must be shortened(segments IJ and GH). Need to intersect with one side of window and determine outcode of resulting point. [TURN OVER] 7 COS340-A October/November 2009 QUESTION 5 [12] a) Discuss the distinguishing features of ambient, point, spot and distant light sources. (8) b) Describe the difference between flat and smooth shading. (4) QUESTION 6 [8] a) Describe environment maps, and explain the difference between the use of cube maps and spherical maps to implement them. (4) 2 marks each: ‚Ä¢ An ambient light source produces light of constant intensity throughout the scene. All objects are illuminated from all sides. ‚Ä¢ A point light source emits light equally in all directions, but the intensity of the light diminishes with (i.e. proportional to the inverse square of) the distance between the light and the objects it illuminates. Surfaces facing away from the light source are not illuminated. ‚Ä¢ A spot light source is similar to a point light source except that its illumination is restricted to a cone in a particular direction. A spot light source can also have more light concentrated in the centre of the cone. ‚Ä¢ A distant light source is like a point light source except that the rays of light are all parallel. The main difference is the improved speed of rendering calculations. Flat shading is where a polygon is filled with a single colour or shade across its surface. A single normal is calculated for the whole surface, and this determines the single colour. Smooth shading is where colour is interpolated across the surface of a polygon. This is usually used to give the appearance of a rounded surface, and is achieved by defining different normals at each of the vertices of the polygon, and interpolating the normals across the polygon. [TURN OVER] 8 COS340-A October/November 2009 One way to create fairly realistic rendering of an object with a highly reflective surface is to map a texture to its surface (called an environment map) which contains an image of other objects around the highly reflective object as they would be seen reflected in its surface. A spherical map is an environment map in the form of a sphere, which is then converted to a flat image (by some projection) to be applied to the surface. A cube map is in the form of a cube, i.e. six projections of the other objects in the scene. The renderer then picks the appropriate part of one of these projections to map to each polygon on the surface of the reflective object (as a texture). (b) Briefly discuss the main difference between the opacity of a surface and the transparency of a surface with respect to alpha blending. (4) Answer: Angel Section 7.9: The alpha channel is the fourth colour in the RGBA (or RGBŒ±) colour mode. Like the other colour components, the application program can control the value of A (or Œ±) for each pixel. If blending is enabled in RGBA mode, the value of Œ± controls how the RGB values are written into the frame buffer. Because surfaces of multiple objects lying behind one another can contribute to the colour of a single pixel, we say that the colours of these surfaces are blended or composited together. Opacity of a surface is a measure of how much light penetrates through that surface. An opacity of 1 (Œ± = 1) corresponds to a completely opaque surface that blocks all light from surfaces hidden behind it. A surface with an opacity of 0 is transparent: All light passes through it. The transparency or translucency of a surface with opacity Œ± is given by 1 Œ±. [TURN OVER] 9 COS340-A October/November 2009 QUESTION 7 [12] Consider the following OpenGL program that draws two walls of a room that meet at a corner. In the middle of the room is a rotating square suspended in mid air. Answer the questions that follow 1. #include <gl/glut.h> 2. #include <fstream> 3. #include <cmath> 4. float eye_pos[3] = {100, 50, -100}; 5. GLfloat angle = 0; 6. bool rotating = true; 7. bool clockwise = true; 8. void drawPolygon() 9. { 10. glColor3f (1.0, 1.0, 0.0); 11. glBegin(GL_POLYGON);// Draw square 12. glVertex3d(20, 20, 0); 13. glVertex3d(20, -20, 0 ); 14. glVertex3d(-20, -20, 0); 15. glVertex3d(-20, 20, 0); 16. glEnd(); 17. } 18. void drawRoom() 19. { 20. glColor3f (1.0, 1.0, 1.0); // left wall 21. glBegin(GL_POLYGON); 22. glVertex3d(0, 0, 0); 23. glVertex3d(0, 100, 0 ); 24. glVertex3d(0, 100, -100); 25. glVertex3d(0, 0, -100); 26. glEnd(); 27. glBegin(GL_POLYGON); // right wall 28. glVertex3d(0, 0, 0); 29. glVertex3d(100, 0, 0); 30. glVertex3d(100, 100, 0); 31. glVertex3d(0, 100, 0); 32. glEnd(); 33. } 34. void display() 35. { glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); [TURN OVER] 10 COS340-A October/November 2009 36. glMatrixMode(GL_MODELVIEW); 37. glLoadIdentity(); 38. gluLookAt(eye_pos[0], eye_pos[1], eye_pos[2], 0.0, 50.0, 0.0, 0.0, 1.0, 0.0); 39. drawRoom(); 40. glTranslatef(40, 40, -40); 41. glRotatef(angle, 0, 1, 0); 42. drawPolygon(); 43. glFlush(); 44. glutSwapBuffers();} 45. void idle() 46. { 47. if (rotating) 48. { if (!clockwise) // rotate counter-clockwise {angle-= 0.5; if (angle < -360.0) angle += 360.0;} else // rotate clockwise 49. { angle+= 0.5; if (angle > 360.0) angle -= 360.0;} 50. } 51. glutPostRedisplay(); 52. } 53. void myInit() 54. { 55. glPolygonMode(GL_FRONT, GL_FILL); 56. glMatrixMode(GL_PROJECTION); 57. gluPerspective(90, -1, 10, 210); 58. } 59. int main(int argc, char** argv) 60. { 61. glutInitDisplayMode(GLUT_DOUBLE | GLUT_RGB | GLUT_DEPTH); 62. glutInitWindowSize(700, 700); 63. glutCreateWindow(\"Room with floating shapes\"); 64. myInit(); 65. glutDisplayFunc(display); 66. glutIdleFunc(idle); 67. glutMainLoop(); 68. return 0; 69. } [TURN OVER] 11 COS340-A October/November 2009 a) Say the following function is inserted in the program, and is called between lines 64 and 65: void setupMenus() { int main_menu = glutCreateMenu(mainMenuCallback); glutAddMenuEntry(\"Rotate Clockwise\", 0); glutAddMenuEntry(\"Rotate anti-clockwise\", 1); glutAddMenuEntry(\"Pause Rotation\", 2); glutAddMenuEntry(\"Quit\", 3); glutAttachMenu(GLUT_RIGHT_BUTTON); } Write the callback function for this menu to do the following: Rotate square clockwise Rotate square anti-clockwise Pause rotation of square Exit Program Hint : The rotation of the square is controlled by boolean variables rotating and clockwise.(see lines 6 and 7 as well as function void idle ). [4] void mainMenuCallback(int id) { switch (id) { case 0:rotating = true; clockwise = true; break; case 1: rotating = true; clockwise = false; break; case 2: rotating = false; break; case 3: exit(0); } [TURN OVER] 12 COS340-A October/November 2009 b) Write a keyboard callback function that will allow the user to zoom in towards the corner of the room and back out again. Use the ‚Äò<‚Äò to zoom in and ‚Äò>‚Äô key to zoom out.. You do not need to set maximum or minimum values for the camera position .The y value of the camera position will remain constant and the zoom increment can be set to 3.0. You can assume that glutKeyboardFunc(keyboard) is added to the main function Hint: Take note of the initial position of the camera in line 4 relative to the visible corner of the room. c) Consider the following function for loading an image from a file into an array, and for specifying it as a texture: void setTexture() { const int WIDTH = 512; const int HEIGHT = 1024; GLubyte image[WIDTH][HEIGHT][3]; FILE * fd; fd = fopen(\"texture.bmp\",\"rb\"); fseek(fd, 55, SEEK_SET); // Move file pointer past header info fread(&image, 1, WIDTH*HEIGHT*3, fd); fclose(fd); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_REPEAT); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_REPEAT); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR); glTexImage2D(GL_TEXTURE_2D, 0, 3, WIDTH, HEIGHT, 0, GL_RGB, GL_UNSIGNED_BYTE, image); } The file texture.bmp contains a 512x1024 image.If this function is inserted in the above program (and called in the main function just before myinit is called), give the statements that void keyboard(unsigned char key, int x, int y) {switch (key) {case '<' : eye_pos[0] += 3.0;//increment value can be different eye_pos[2] -= 3.0; break; case '>' : eye_pos[0] -= 3.0; eye_pos[2] += 3.0; break; } return; } [TURN OVER] 13 COS340-A October/November 2009 need to be inserted in the drawroom function (and state where) to apply the texture to the surface of the right wall of the room . In function drawroom, insert the following calls of glTexCoord2f before the respective calls of gkVertex3f: glTexCoord2f(0, 0); //after line 27 glTexCoord2f(0, 1); //after line 28 glTexCoord2f(1, 1); //after line 29 glTexCoord2f(0, 0); //after line 30 [TURN OVER] 2 COS340A October/November 2010 COS340-A JANUARY/FEBRUARY 2011 COMPUTER SCIENCE COMPUTER GRAPHICS Duration: 2 hours Total: 70 marks Examiners: First: Mr L Aron and Mr C Dongmo External: Dr P Marais ........................................................................................................... ........................................................................................................... MEMORANDUM [TURN OVER] 3 COS340A October/November 2010 QUESTION 1 [10] a) What is meant by the term ‚Äúdouble buffering‚Äù and for what purpose is it used? [3] b) What is the difference between a frame buffer and a z-buffer? [2] c) Give brief definitions of the following terms in the context of computer graphics: (i) Rasterization [1] (ii) Fragment [2] (iii) Interpolation [2] The process of using 2 buffers ,a front and back buffer, in computer animation. The front buffer is displayed while the application renders into the back buffer. When the rendering to the back buffer is completed a buffer swap is done, the formerly back buffer is the one displayed (it becomes the new front buffer). The information needed to define the picture displayed on the screen is stored in a part of the memory called the frame buffer. z-buffer contains depth information used in hidden surface removal (i)Rasterisation is the conversion of primitives (points, lines and polygons) to fragments. (ii)A fragment is a potential pixel and consists of its colour values, location and possibly its depth value. (iii)Interpolation is a way of determining value (of some parameter) for any point between two endpoints of which the parameter values are known (e.g. the colour of any points between two points, or the normal of any point between two points) [TURN OVER] 4 COS340A October/November 2010 ÔÉ∫ QUESTION 2 [12] a) Transformations are often carried out using a homogeneous co-ordinate representation. Why is this representation used? [2] b) Rotation transformations are not commutative. Demonstrate this by computing i) the transformation matrix for a rotation by 90¬∫ about x followed by a rotation by 90¬∫ about y and [3] ii) the transformation matrix for a 90¬∫ rotation about y followed by a 90¬∫ rotation about x. [2] iii) Apply the two composite transformation matrices to the point (1, 1, 1) to demonstrate that rotations are not commutative. [3] All rotations are clockwise.The matrices for rotation about x and rotation about y are ÔÉ© cosÔÅ± 0 ÔÄ≠sin ÔÅ± 0ÔÉπ ÔÉ™ RyÔÄΩÔÉ™ 0 cosÔÅ± 0 0ÔÉ∫ 0ÔÉπ ÔÉ™ÔÄ≠sin ÔÅ± 0 cosÔÅ± 0ÔÉ∫ Rx ÔÄΩ ÔÉ∫ ÔÉ∫ 0ÔÉ∫ ÔÉ∫ 1ÔÉª ÔÉ™ ÔÉ∫ Ô£∞ÔÉ™ 0 0 0 1ÔÉª see next page for solution c) Give two other combinations of transformations that do not commute. [2] In the 3D coordinate representation it is difficult to distinguish between points and vectors. Homogeneous coordinates avoids this difficulty by using a four dimensional representation for both points and vectors in 3 dimensions. Rotation and translation Rotation and non-uniform scaling Rotation and shear 0 ÔÉ©1 0 0 ÔÉ™ ÔÉ™0 cosÔÅ± ÔÄ≠sinÔÅ± ÔÉ™0 ÔÉ™ ÔÉ´0 sinÔÅ± 0 cosÔÅ± 0 [TURN OVER] 5 COS340A October/November 2010 ÔÉ™ ÔÉ∫ÔÉ™ ÔÉ∫ ÔÉ™ ÔÉ∫ i) ÔÉ©1 0 0 0ÔÉπÔÉ© 0 0 ÔÄ≠1 0ÔÉπ ÔÉ©0 0 ÔÄ≠1 0ÔÉπÔÉ™0 0 ÔÄ≠1 0ÔÉ∫ÔÉ™ 0 0 0 0ÔÉ∫ ÔÉ™1 0 0 0ÔÉ∫ Rx. RyÔÄΩ ÔÉ™ ÔÉ∫ÔÉ™ ÔÉ∫ ÔÄΩ ÔÉ™ ÔÉ∫ ÔÉ™0 1 0 0ÔÉ∫ÔÉ™ÔÄ≠1 0 0 0ÔÉ∫ ÔÉ™0 0 0 0ÔÉ∫ ÔÉ™ ÔÉ∫ÔÉ™ ÔÉ∫ ÔÉ™ ÔÉ∫ ÔÉ´0 0 0 1ÔÉªÔÉ´ 0 0 0 1ÔÉª ÔÉ´ 0 0 0 1ÔÉª ii) ÔÉ© 0 0 ÔÄ≠1 0ÔÉπÔÉ©1 0 0 0ÔÉπ ÔÉ© 0 ÔÄ≠1 0 0ÔÉπÔÉ™ 0 0 0 0 ÔÉ∫ÔÉ™ 0 0 ÔÄ≠1 0ÔÉ∫ ÔÉ™ 0 0 0 0ÔÉ∫ Ry. RxÔÄΩ ÔÉ™ ÔÉ∫ÔÉ™ ÔÉ∫ ÔÄΩ ÔÉ™ ÔÉ∫ ÔÉ™ÔÄ≠1 0 0 0ÔÉ∫ÔÉ™0 1 0 0ÔÉ∫ ÔÉ™ÔÄ≠1 0 0 0ÔÉ∫ ÔÉ™ ÔÉ∫ÔÉ™ ÔÉ∫ ÔÉ™ ÔÉ∫ ÔÉ´ 0 0 0 1ÔÉªÔÉ´ 0 0 0 1ÔÉª ÔÉ´ 0 0 0 1ÔÉª iii) ÔÉ© 0 ÔÄ≠1 0 0ÔÉπÔÉ©1ÔÉπ ÔÉ©ÔÄ≠1ÔÉπ ÔÉ©0 0 ÔÄ≠1 0ÔÉπÔÉ©1ÔÉπ ÔÉ©ÔÄ≠1ÔÉπÔÉ™ 0 0 0 0 ÔÉ∫ÔÉ™ 1 ÔÉ∫ ÔÉ™ 0 ÔÉ∫ ÔÉ™1 0 0 0ÔÉ∫ÔÉ™1ÔÉ∫ ÔÉ™ 1 ÔÉ∫ Ryx.a ÔÄΩÔÉ™ ÔÉ∫ÔÉ™ ÔÉ∫ ÔÄΩÔÉ™ ÔÉ∫ Rxy. a ÔÄΩ ÔÉ™ ÔÉ∫ÔÉ™ ÔÉ∫ ÔÄΩ ÔÉ™ ÔÉ∫ ÔÉ™ÔÄ≠1 0 0 0ÔÉ∫ÔÉ™1ÔÉ∫ ÔÉ™ÔÄ≠1ÔÉ∫ ÔÉ™0 0 0 0ÔÉ∫ÔÉ™1ÔÉ∫ ÔÉ™ 0 ÔÉ∫ ÔÉ™ ÔÉ∫ÔÉ™ ÔÉ∫ ÔÉ™ ÔÉ∫ ÔÉ´0 0 0 1ÔÉªÔÉ´ 1ÔÉª ÔÉ´ 1 ÔÉª ÔÉ´ 0 0 0 1ÔÉªÔÉ´ 1ÔÉª ÔÉ´ 1 ÔÉª For the xy transformation the new point is (-1,1,0) For the yx transformation the new point is (-1,0,-1) [TURN OVER] 6 COS340A October/November 2010 QUESTION 3 [12] a) What is the difference between parallel and perspective projections? Describe an application where each type of projection would be preferable. [5] b) A synthetic camera co-ordinate reference frame is given by a view reference point (VRP) a view plane normal (VPN) and a view up vector (VUP). i) Using a diagram show how these quantities describe the location and orientation of the synthetic camera. [5] ii) The OpenGL call gluLookAt takes an eye point, an at point and an up point. Express VRP, VPN and VUP in terms of these three points. [2] In a perspective projection, lines (called projectors) are drawn from the objects to a point called the centre of projection (COP). The projection of the objects is where these lines intersect the projection plane. animation. In an orthographic projection, the projectors do not converge to a point but are parallel to one another, in a particular direction - the so-called direction of projection. In this case, the COP is assumed to be at an infinite distance. As with a perspective projection, the projection of the objects is where the projectors intersect the projection plane. architecture/working drawings **v and u not necessary Camera is positioned at the origin, pointing in the negative z direction Camera is centred at point called the VRP. Orientation of the camera is specified by VPN and VUP. VPN is the orientation of the projection plane or back of camera. The orientation of the plane does not specify the up direction of the camera hence we have VUP which is the up direction of the camera. VUP fixes the camera. U VRP V VUPVPN VRP = eye point VPN = at point ‚Äì eye point VUP = up point [TURN OVER] 7 COS340A October/November 2010 QUESTION 4 [12] a) Hidden-surface-removal algorithms can be divided into two broad classes: Object-space algorithms and Image space algorithms. i) Differentiate between the object space approach and image space approach to hidden surface removal? [4] ii) Name one algorithm for each approach? [2] b) Using diagrams describe briefly the Liang-Barsky clipping algorithm. [6] Object space algorithms attempt to order surfaces of the objects in the scene such. If surfaces are rendered in the correct order then the correct image will be created. Image space algorithms work as part of the projection process and seek to determine the relationship among object points on each projector. Object space: Depth sort/ painters algorithm image space: z-buffer Two marks for diagram [TURN OVER] 8 COS340A October/November 2010 Suppose we have a line segment defined by two endpoints p (x1, y1) q(x1, y1). The parametric equation of the line segment gives x-values and y-values for every point in terms of a parameter Œ± that ranges from 0 to 1. x(Œ±) = (1 -Œ±) x1 + Œ± x2 y(Œ±) = (1- Œ±) y1 + Œ± y2 There are four points where line intersects side of windows tB tL tT tR we can order these points and then determine where clipping needs to take place. If for example tL > tR , this implies that the line must be rejected as it falls outside the window. To use this strategy effectively we need to avoid computing intersections until they are needed. Many lines can be rejected before all four intersections are known. QUESTION 5 [12] a) In a simple computer graphics lighting model we assume the ambient component Iamb = ka La . i) What lighting situation does the ambient component approximate? [2] ii) What does ka represent? [1] iii) Is ka a property of the light or the surface? [1] surfaceiii) the amount of light reflectedii) a situation where lights have been designed or positioned to provide uniform lighting across an area. i) [TURN OVER] 9 COS340A October/November 2010 b) Discuss the difference between a global and a local lighting model [4] c) Why does Phong shaded images appear smoother than Smooth (Gouraud) or Flat shaded images? [4] QUESTION 6 [12] a) Describe environment maps, and explain the difference between the use of cube maps and spherical maps to implement them. [4] With the graphics pipeline employed by many graphics systems, each object is processed/rendered (incl. shading) independently. Consequently, shadows and reflections from other objects cannot be taken into account. This is called a local lighting model. A global lighting model does take shadows and reflections from other objects into account when shading objects. This is usually done by means of ray- tracing and radiosity techniques. In flat shading a polygon is filled with a single colour or shade across its surface. A single normal is calculated for the whole surface, and this determines the colour. In smooth shading colour per vertex is calculated using vertex normals and then this colour is interpolated across the polygon. In Phong shading, the normals at the vertices are interpolated across the surface of the polygon. The lighting model is then applied at every point of within the polygon. Because normals gives the local surface orientation, by interpolating the normals across the surface of a polygon, the surface appears to be curved rather than flat hence the smoother appearance of Phong shaded images. One way to create fairly realistic rendering of an object with a highly reflective surface is to map a texture to its surface (called an environment map) which contains an image of other objects around the highly reflective object as they would be seen reflected in its surface. A spherical map is an environment map in the form of a sphere, which is then converted to a flat image (by some projection) to be applied to the surface. A cube map is in the form of a cube, i.e. six projections of the other objects in the scene. The renderer then picks the appropriate part of one of these projections to map to each polygon on the surface of the reflective object (as a texture). [TURN OVER] 10 COS340A October/November 2010 b) Describe briefly how texture mapping is implemented in OpenGL. [4] c) Briefly explain what antialiasing is and then explain how the alpha channel and the accummulation buffer can be used to achieve this. [4] Two dimensional texture mapping starts with an array of texels, which is the same as a two dimensional pixel rectangle.We specify that this array is to be used as a two dimensional texture; We need to enable texture mapping through . The second part of setting up texture mapping is to specify how the texture is mapped onto a geometric object. OpenGL uses two coordinates (say s and t) both of which range over the interval (0.0, 1.0) over the texture image. Any values of s and t in the unit interval correspond to a unique texel in the texel array. OpenGL leaves the mapping of texture coordinates to vertices to the application by having the values of s and t as part of the OpenGl state. These values are assigned by the function glTexCoord2f(s, t).The renderer uses the present texture coordinates when processing a vertex. If we want to change the texture coordinate assigned to a vertex., we must set the texture coordinate before we specify the vertex. Angel pages 346-348; 351; 408-409 In the conversion between analog values of object or world coordinates and colours to the discrete values of screen coordinates and colours, rasterised (rendered) line segments and edges of polygons often become jagged or pixels that contrast too strongly with there neighbourhood are displayed.. This can be prevented by using antialiasing. Antialiasing blends and smooths points, lines, or polygons to get rid of sharp contrasts or other unwanted patterns. Alpha values: Angel page 346 Section 7.9.4 -‚ÄùOne of the major uses of the alpha channel is for antialiasing.\" When rendering a line, instead of colouring an entire pixel with the colour of the line if it passes through it, the amount of contribution of the line to the pixel is stored in the pixels alpha value. This value is then used to calculate the intensity of the colour (specified by the RGB values), and avoids the sharp contrasts and steps of aliasing. Accumulation buffer - Angel Section 7.9.4 and page 351 Section 7.10.1: ‚ÄùOne of the most important uses of the accumulation buffer is for antialiasing.\" Rather than antialiasing individual lines and polygons we can antialias an entire scene using the accumulation buffer. The idea is that if we regenerate the same scene with all the objects, or the viewer, shifted slightly (less than one pixel), then we generate different aliasing artifacts. If we can average together the resulting images, the aliasing effects are smoothed out. COS3712 May /June 2014 COMPUTER GRAPHICS Duration: 2 hours Total: 70 marks Examiners: First: Mr L Aron Second: Mr C Dongmo External: Mr JCW Kroeze(UP) ........................................................................................................... ........................................................................................................... MEMORANDUM COS3712 MAY/JUNE 2014 QUESTION 1: OpenGL Pipeline [12] 1.1 The OpenGL pipeline makes use of homogeneous coordinates. Explain how to convert Cartesian coordinates, (x, y, z), to homogeneous coordinates and how to convert homogeneous coordinates, (x,y,z,w) to Cartesian coordinates. [2] 1.2 Explain how OpenGL model view matrix mode relates to the different coordinate spaces in the OpenGL pipeline? [2] 1.3 The 4 major stages of a graphics pipeline are i. Modeling ii. Geometry Processing iii. Rasterization iv. Fragment processing Describe briefly the purpose of each stage. [8] 2 Cartesian coordinates (x,y,z) = homogeneous coordinates P(x,y,z,1) ‚úì homogeneous coordinates P(x,y,z,w) = Cartesian coordinates (x/w,y/w,z/w) ‚úì 4x4 matrices that represent the transformation from model coordinates to world and then from world to eye are concatenated in the model-view transformation which is specified by the model-view matrix ‚úì‚úì i. modeler can be seen as a black box that produces geometric objects and is usually an application program.... ‚úìresults of modelling are a set of vertices that specify a group of geometirc objects... ‚úì ii. geometry processing - works with vertices... ‚úì a geometry processor determines which geometric objects can appear on the display.(through projection, primitive assembly and clipping). ‚úì. and assigns shades or colors to the vertices of these objects.... COS3712 May/june 2014 3 iii. scan conversion or rasterization - the primitives that emerge after geometry processing are still represented in terms of their vertices and must be further br processed to generate pixels in the frame buffer. ‚úì A fragment can be thought of as a potential pixel that carries with it information including its colour, location, and depth information, that is used to update the corresponding pixel in the frame buffer. Rasterization is the process of determining these fragments. ‚úì iv. Fragment porcessing - process of taking fragments generated by rasterizer and updating pixels in the frame buffer.. ‚úì Depth information together with transparency of fragments ‚Äúin front‚Äù as well as texture and bump mapping are used to update the fragments in the frame buffer to form pixels that can be displayed on the screen. ‚úì QUESTION 2: Transformations [8] 2.1 Consider the diagram below and answer the question that follows: Give a matrix which will transform the square ABCD to the square A‚Ä≤B‚Ä≤C‚Ä≤D‚Ä≤. Show all workings. Below is the transformation matrix for clockwise and anticlockwise rotation about the z-axis. COS3712 May/june 2014 4 ÔÉ™ ÔÉ∫ ÔÉ™ ÔÉ∫ ÔÄ≠ ÔÉ∫ÔÉ™ ÔÉ∫ÔÉ™ ÔÉ∫ ÔÄ≠ ÔÉ∫ ÔÉ™ ÔÉ™ ÔÉ™ ÔÉ© cos ÔÅ± ÔÉ™ ÔÄ≠ sin ÔÅ± sin ÔÅ± cos ÔÅ± 0 0ÔÉπ 0 0ÔÉ∫ ÔÉ©cos ÔÅ± ÔÉ™ sin ÔÅ± ÔÄ≠ sin ÔÅ± cos ÔÅ± 0 0ÔÉπ 0 0ÔÉ∫ ÔÉ™ 0 0 1 0ÔÉ∫ ÔÉ™ ÔÉ∫ Ô£∞ÔÉ™ 0 0 0 1ÔÉ∫ÔÉª ÔÉ™ 0 0 1 0ÔÉ∫ ÔÉ™ ÔÉ∫ Ô£∞ÔÉ™ 0 0 0 1ÔÉªÔÉ∫ Clockwise rotation matrix Anti-clockwise rotation matrix [6] Answer: Other permutations are possible and hence each answer must be looked at seperately. Final matrix is the same: Small square is translated by (4,1,0), then rotated clockwise by 45.Small square has length ‚àö2 on each side. Large square has length of 3 hence scaling factor is 3/‚àö2 on the x and y axis. Three matrices are multiplied as follows: ÔÉ©1 0 0 4ÔÉπÔÉ©ÔÉ™ 0 1 0 1ÔÉ∫ÔÉ™ÔÄ≠ 2 / 2 2 / 2 / 2 0 0ÔÉπÔÉ©3 / ÔÉ∫ÔÉ™ 0 2 0 0 0ÔÉπ ÔÉ∫ 2 ÔÉ™ ÔÉ∫ÔÉ™ ÔÉ™0 ÔÉ™ ÔÉ™ÔÉ´0 ÔÉ´ / 2 0 0ÔÉ∫ÔÉ™ 3 / 0 0ÔÉ∫ ÔÉ∫ ÔÉ∫ 1ÔÉ∫ÔÉª ÔÉ© 2 / 2 2 / 2 0 4ÔÉπÔÉ©3 / 2 0 0 0ÔÉπ ÔÉ™ 2 / 2 2 / 2 0 1 ÔÉ∫ÔÉ™ 0 3 / 2 0 ÔÉ∫ 0ÔÉ∫ ÔÉ™ 0 0 1 0ÔÉ∫ÔÉ™ 0 0 1 0ÔÉ∫ Ô£∞ÔÉ™ 0 0 0 1 ÔÉªÔÉ´ 0 0 0 1 ÔÉ© 3 / 2 3 / 204ÔÉπ ÔÉ™3 / 23 / 201ÔÉ∫ ÔÉ™ 0 0 10ÔÉ∫ ÔÉ™ ÔÉ∫ Ô£∞ÔÉ™0 0 01 ÔÉª 2 2 00 1 0ÔÉ∫ ÔÉ∫ ÔÉ™ 0ÔÉ™ 0 1 0ÔÉ∫ÔÉ™ 0 ÔÉ∫ÔÉ™ 0 1 0 0 1ÔÉ∫ÔÉªÔÉ™ 0 0 0 1ÔÉªÔÉ´ 0 0 0 COS3712 May/june 2014 5 2.2 Which two of the following transformation combinations do not commute? [2] i) Rotation and translation iii) Translation and scaling ii) Rotation and shear iv) Rotation and uniform scaling QUESTION 3: Viewing [8] 3.1 Define the term View Volume with respect to computer graphics and with reference to both perspective and orthogonal views. [3] The view volume is analogous to the volume that a real camera would see through its lens (except that it is also limited in distance from the front and back). It is a section of 3D space that is visible from the camera or viewer between two distances. ‚úì When using orthogonal (or parallel) projection, the view volume is rectangular. ‚úì When using perspective projection, the view volume is a frustum and has a truncated pyramid shape. ‚úì 3.2 A transformation that is not linear is the perspective transformation, still we can describe it using homogeneous coordinates as a matrix multiplication. Draw a sketch and derive the formula for perspective projection, with an observer looking orthogonal at the view plane at the distance d from it. Write down the resulting transformation matrix. [5] i and ii The perspective formula is very easily derived using uniform triangles. y'/d = y/z => y' = d*y/z = Y/W => Y=1*y, W=1/d*z Same for x' leads to the following: x'=d*x/z y'=d*y/z z'=d P' = MP where [ 1 0 0 0 ] M = [ 0 1 0 0 ] [ 0 0 1 0 ] [ 0 0 1/d 0 ] COS3712 May/june 2014 6 QUESTION 4: Hidden surface removal [10] 4.1 Describe briefly how hidden surface removal is implemented in OpenGL [4] 4.2 How does OpenGL deal with transparent surfaces during hidden surface removal [3] 4.3 Does OpenGL perform shading before or after hidden surface removal? Explain [3] OpenGL uses a hidden-surface method called z-buffering ‚úì that saves depth information into an extra buffer which is used as objects are rendered so that parts of objects that are behind other objects do not appear in the image. The z-buffer records the depth or the distance from the COP of a pixel. ‚úì Thus, as polygons are scan-converted in random order, the algorithm checks the z-value of the pixel at the coordinates (in the frame buffer) that it is trying to draw to. If the existing pixel at these coordinates (i.e. if there already was a pixel) has a z-value further from the viewer than the pixel we are trying to draw, then the colour and depth of the existing pixel are updated, otherwise it continues without writing to the colour buffer or the depth buffer. ‚úì ‚úì In a scene containing both opaque and translucent polygons, any polygon (or part of a polygon) behind an opaque polygon should not be rendered, but polygons (or parts of polygons) behind translucent polygons should be composited. ‚úì If all polygons are rendered with the standard z-buffer algorithm, compositing will not be performed correctly, particularly if a translucent polygon is rendered first, and an opaque behind it is rendered later. However, if we make the z-buffer read-only when rendering translucent polygons, we can prevent the depth information from being updated when rendering translucent objects. ‚úìIn other words, if the depth information allows a pixel to be rendered, it is blended (composited) with the pixel already stored there. If the pixel is part of an opaque polygon, the depth data is updated, but if it is a translucent pixel, the depth data is not updated. ‚úì Shading is performed before hidden surface removal. ‚úìIn the z-buffer algorithm polygons are first rasterized and then for each fragment of the polygon depth values are determined and compared to the z-buffer. ‚úì‚úì COS3712 May/june 2014 7 QUESTION 5: Lighting and Shading [15] 5.1 Surface normals play a central role in Computer Graphics. i. What is a surface normal and what information does it gives us about a surface. [2] ii. Explain how surface normals are used in flat and smooth shading [4] 5.2 State and explain Lamberts Law. [3] iii. i) Normals are vectors that are perpendicular to a surface. They can be used to describe the orientation or direction of that surface ‚úì‚úì. iv. v. vi. vii. ii) Flat shading: Most of the calculations for rendering a scene involve the determination of the required vectors and dot products. For each special case, simplifications are possible. For example, if the surface is a flat polygon, the normal is the same at all points on the surface. ‚úì‚úì. viii. ix. x. Smooth (Gourand and Phong) shading: A smooth surface can be simulated by shading a mesh of polygons by interpolation. In Gourand shading, the \"normal\" at each vertex is defined as the average of the normals of each of the polygons that meet at the vertex. This normal is used to determine the shade of the vertex, and the colours are interpolated across the polygon to its other vertices. In Phong shading, vertex normals are calculated in the same way, but these normals are interpolated from one vertex to the next to give a more realistic appearance of a curved surface ‚úì‚úì. Rd Œ± cos Œ∏ ‚úì Lamberts Law states that the amount of diffuse light reflected (Rd )is directly proportional to cos Œ∏ where Œ∏ is the angle between the normal at the point of interest and the direction of the light source. ‚úì‚úì COS3712 May/june 2014 8 5.3 Using Lamberts Law derive the equation for calculating approximations to diffuse reflection on a computer. [3] If we consider the direction of the light source(l) and the normal at the point of interest(n) to be unit-length vectors, then cos Œ∏ = l ‚àô n ‚úì If we add a reflection component kd representing the fraction of incoming diffuse light that is reflected , we have the diffuse reflection term: ‚úì Id = kd (l ‚àô n) Ld , where L is the light source. ‚úì 5.4 Discuss the difference between a global and a local lighting model. [3] QUESTION 6: Rasterization [8] 6.1 The simplest scan-conversion algorithm for line segments has become known as the DDA algorithm. Give the algorithm in pseudocode form and explain briefly how it is derived. [6] The most simple and straightforward approach for drawing a line is using the Line Equation: , where m is the lines slope defined as: . As you can see the equation has two constants (m and n) and two variables (x and y). By looping and incrementing the x variable the y variable is calculated. DDA is used to eliminate the recalculation of the y value by assuming that the x value is always incremented since we are drawing it on the computer screen. So if we take the start point and the end point which gives us: so that is and we know that . Algorithm: for(i = x0 ; i <= x1; i++) { y+=m write_pixel(i, round(y), line_color)} } With the graphics pipeline employed by many graphics systems, each object is processed/rendered (incl. shading) independently. Consequently, shadows and reflections from other objects cannot be taken into account. This is called a local lighting model ‚úì‚úì A global lighting model does take shadows and reflections from other objects into account when shading objects. This is usually done by means of ray-tracing and radiosity techniques. ‚úì COS3712 May/june 2014 9 Although x is an integer, y is not because m is a floating point number, hence y needs to be rounded off. 6.2 Bresenham derived a line-rasterization algorithm that has become the standard approach used in hardware and software rasterizers as opposed to the more simpler DDA algorithm. Why is this so? [2] QUESTION 7: Discreet techniques [9] In computer graphics, mirror like effects are created by a process called environment or reflection mapping 7.1 Describe briefly the process of environment or reflection mapping. [4] 7.2 Give two difficulties associated with this approach? [2] The DDA algorithm is efficient and can be coded easily, but it requires a floating-point addition for each pixel generated. Bresenhams algorithm avoids all floating point calculations. ‚úì‚úì One way to create fairly realistic rendering of an object with a highly reflective surface is to map a texture to its surface (called an environment map) which contains an image of other objects around the highly reflective object as they would be seen reflected in its surface. ‚úì We use two step process, in the first pass we render scene without mirror polygon ‚úìand with camera placed at center of mirror with camera placed in direction of mirror. We obtain an image of objects as seen by the mirror. ‚úì In the second step we use this image as a texture map on the mirror in the complete scene ‚úì Any two First pass image is not quite correct because they have been formed with one object (the mirror) in scene. Determining onto what surface we want project scene in first pass. 9Determining the position of the camera in the first pass. COS3712 May/june 2014 10 7.3 Describe how environment mapping is implemented in OpenGL. Give one advantage of this approach. [3] OpenGL uses an environment map in the form of a sphere, where environment is mapped on sphere, which is then converted to a flat circular image (by some projection) to be applied to the surface. ‚úì‚úì The advantage is that the mapping from reflection vector(shere) onto circle is simple and can be done in hardware or software. ‚úì Examiners : First : Mr L Aron Second : Mr C Dongmo External : Mr JCW Kroeze(UP) ¬© UNISA 2014 [TURN OVER] COS3712 1 COS3712 May/June 2012 May /June 2012 COMPUTER SCIENCE COMPUTER GRAPHICS Duration: 2 hours Total: 70 marks Examiners: First: Mr L Aron Second: Mr C Dongmo External: Prof P Marais (University of Cape Town) ........................................................................................................... ........................................................................................................... MEMORANDUM [TURN OVER] 2 COS340A/COS3712 May/June 2011 QUESTION 1 : Graphics Pipeline [12] a) What is the main advantage and disadvantage of using the pipeline approach to form computer generated images? [4] The main advantage of the pipeline is that each primitive can be processed independently‚úì. Not only does this architecture lead to fast performance, it reduces memory requirements because we need not keep all objects available. ‚úì The main disadvantage is that we cannot handle most global effects such as shadows, reflections, and blending in a physically correct manner. ‚úì‚úì b) Differentiate between the object oriented and image oriented pipeline implementation strategies? [4] In the object oriented strategy vertices are defined by a program and flow through a sequence of modules that tranforms them, colors them, and determines where they are visible etc. The output is the pixels in a frame buffer. ‚úì‚úì Image oriented approaches loop over pixels, or rows of pixels called scanlines. For each pixel we work backwards determining which geometric primitives can contribute to its colour. ‚úì‚úì c) Name the frames in the usual order in which they occur in the OpenGL pipeline. [4] (¬Ω mark for each frame listed and 1 for correct order) Object or model coordinates World coordinates Eye or camera coordinates Clip coordinates Normalized device coordinates Window or screen coordinates QUESTION 2 :Transformations [8] a) What is an instance transformation? [1] It is the product of a translation, a rotation and a scaling. ‚úì b) Will you get the same effect if the order of transformations that comprise an instance transformation were changed? Explain using an example. [3] [TURN OVER] 3 COS3712 May/June 2012 No, ‚úì Although we will have the same number of degrees of freedom in the objects we produce, the class of objects will be very different. ‚úì For example if we rotate a square before we apply a nonuniform scale, we will shear the square, something we cannot do if we scale then rotate. ‚úì(any example) c) Provide a mathematical proof to show that rotation and uniform scaling commute. [5] Multiply matrices both ways to show result is the same. QUESTION 3 : Hidden surface removal [8] a) Hidden surface removal can be divided into two broad classes State and explain each of these classes. [4] Object space algorithms attempt to order surfaces of the objects in the scene such. If surfaces are rendered in the correct order then the correct image will be created.‚úì‚úì Image space algorithms work as part of the projection process and seek to determine the relationship among object points on each projector. ‚úì‚úì b) Explain the problem of rendering translucent objects using the z-buffer algorithm, and describe how the algorithm can be adapted to deal with this problem (without sorting the polygons). [4] In a scene containing both opaque and translucent polygons, any polygon (or part of a polygon) behind an opaque polygon should not be rendered, but polygons (or parts of polygons) behind translucent polygons should be composited. ‚úì If all polygons are rendered with the standard z-buffer algorithm, compositing will not be performed correctly, particularly if a translucent polygon is rendered first, and an opaque behind it is rendered later‚úì. However, if we make the z-buffer read-only when rendering translucent polygons, we can prevent the depth information from being updated when rendering translucent objects. ‚úì In other words, if the depth information allows a pixel to be rendered, it is blended (composited) with the pixel already stored there. If the pixel is part of an opaque polygon, the depth data is updated, but if it is a translucent pixel, the depth data is not updated. ‚úì QUESTION 4 [10] Viewing [TURN OVER] 4 COS340A/COS3712 May/June 2011 a) What is parallel projection? What specialty do orthogonal projections provide? What is the advantage of the normalization transformation process? [4] A parallel projection is the limit of perspective projection in which the centre of projection is infinitely far from the objects being viewed, resulting in projectors that are parallel ‚úì‚úì Orthogonal projections are a special kind of parallel projections in which the projector are parallel to the view plane. ‚úì The normalization process allows us to carry parallel and perspective projections with the same pipeline. ‚úì b) Why are projections produced by parallel and perspective viewing known as planar geometric projections? [2] In both these classes of projections the surface is a plane and the projectors are lines.‚úì‚úì c) The specification of the orientation of a synthetic camera can be divided into the specification of the view reference point (VRP), view-plane normal (VPN) and the view-up vector(VUP). Explain each of these? [4] Camera is positioned at the origin, pointing in the negative z direction Camera is centered at point called the View Reference Point. ‚úì Orientation of the camera is specified by VPN and VUP‚úì. VPN is the orientation of the projection plane or back of camera‚úì. The orientation of the plane does not specify the up direction of the camera hence we have VUP which is the up direction of the camera. ‚úìVUP fixes the camera. QUESTION 5 : Shading and Lighting [12] a) Interactions between light and materials can be classified into three catergoies. State and describe these catergories? [4] Specular surfaces appear shiny because most of the light is reflected or scattered is in a narrow range of angles close to the angle of reflection‚úì‚úì Diffuse surfaces are characterized by reflected light being scattered in all [TURN OVER] Œ∏ directions. ‚úì 5 COS3712 May/June 2012 Translucent surfaces allow some light to penetrate the surface and to emerge from another location in the object. ‚úì b) State and explain Lamberts Law using a diagram. [4] n d ‚úì Lamberts Law states that the amount of diffuse light reflected is directly proportional to cos Œ∏ where Œ∏ is the angle between the normal at the point of interest and the direction of the light source. ‚úì‚úì Rd Œ± cos Œ∏‚úì c) Using Lamberts Law derive the equation for calculating approximations to diffuse reflection on a computer. [4] If we consider the direction of the light source(l) and the normal at the point of interest(n) to be unit-length vectors, then cos Œ∏ = l ¬∑ n‚úì‚úì If we add a reflection component kd representing the fraction of incoming diffuse light that is reflected , we have the diffuse reflection term: Id = kd (l ¬∑ n) Ld , where L is the light source. ‚úì‚úì QUESTION 6 : Discrete Techniques [10] a) Texture mapping requires interaction between the application program, the vertex shader and the fragment shader. What are the three basic steps of texture mapping? [3] First the a texture image is formed and placed in texture memory, ‚úì then texture coordinates are assigned to each fragment, ‚úì finally the texture is applied to [TURN OVER] each fragment. ‚úì 6 COS340A/COS3712 May/June 2011 b) Explain how the alpha channel and the accumulation buffer can be used to achieve antialiasing with line segments and edges of polygons. [4] When rendering a line, instead of colouring an entire pixel with the colour of the line if it passes through it, the amount of contribution of the line to the pixel is stored in the pixels alpha value. This value is then used to calculate the intensity of the colour (specified by the RGB values), and avoids the sharp contrasts and steps of aliasing. ‚úì‚úì Accumulation buffer - Rather than antialiasing individual lines and polygons we can antialias an entire scene using the accumulation buffer. The idea is that if we regenerate the same scene with all the objects, or the viewer, shifted slightly (less than one pixel), then we generate different aliasing artifacts. If we can average together the resulting images, the aliasing effects are smoothed out. ‚úì‚úì c) Explain what is meant by texture aliasing and show point sampling and linear filtering help to solve this problem. [3] When we map textures to the array of texels, we rarely get a point that corresponds to the center of a texel. ‚úì One option is to use the value of the texel that is closest to the texture coordinate output by the rasterizer . This is known as point sampling‚úì. One could also use a weighted average of a group of texels in the neighborhood determined by point sampling. This is known as linear filtering. ‚úì QUESTION 7 : Clipping [10] a) Describe, with the use of diagrams, the Cohen-Sutherland line clipping algorithm. [8] The algorithm starts by extending the sides of the window to infinity, thus breaking up space into nine regions.‚úîEach region is assigned a 4 -bit binary number, or outcode. as shown in the diagram below.‚úî [TURN OVER] J B D F I A C E 7 COS3712 May/June 2012 100 1 1000 1010 000 1 0000 0010 010 1 0100 0110 For each endpoint of a line segment, we first compute the endpoint‚Äôs outcode. Consider a line segment whose outcodes are given by X(endpoint 1) and Y(endpoint 2).We have four cases: H G X = Y = 0 :Both endpoints are inside the clipping window(AB in diagram), segment can be rasterized.‚úî X ‚â† 0, Y = 0. One endpoint is inside the window and one is outside.(segment CD) We must shorten the line segment by calculating 1 or 2 intersections.‚úî X & Y ‚â† 0: By taking the butwise AND of the outcodes we determine whether or not the two endpoints lie on the same outside side of the window.If so the line is discarded.(segment EF)‚úî X & Y = 0: Both segments are outside but they are outside on different edges of the window.We cannot tell from outcodes whether segment can be discarded or must be shortened(segments IJ and GH). Need to intersect with one side of window and determine outcode of resulting point. ‚úî [TURN OVER] 8 COS340A/COS3712 May/June 2011 b) What are the advantages and disadvantages of the Cohen-Sutherland line clipping algorithm? [2] Advantages ‚Äì 1 mark for either Cohen-Sutherland works best when there are many line segments but few are displayed. Cohen-sutherland can be extended to three dimensions. Disadvantage ‚Äì 1 marks It must be used recursively [TURN OVER] 1 COS3712 May/June 2013 COS3712 May/June 2013 School of Computing COMPUTER GRAPHICS Duration: 2 hours Total: 70 marks Examiners: First: Mr L Aron Second: Mr C Dongmo External: Prof P Marais (University of Cape Town) ........................................................................................................... ........................................................................................................... MEMORANDUM [TURN OVER] 2 COS3712 May/June 2013 QUESTION 1 [14] 1.1 Explain the difference between immediate mode graphics and retained mode graphics? [4] In immediate mode, primitives (vertices, pixels) flow through the system and produce images. Immediate mode rendering means that the primitives are rendered as soon as they have been specified - no copy of their attributes are kept in memory by the renderer, i.e. this data is lost. To redisplay, new images are created by re-executing the display function and regenerating the primitives. Retained mode rendering means that the primitives are defined once and stored in a scene database (almost like display lists) in \"compiled\" form, where their attributes can later be modified. The scene database is rendered upon request and images can be redisplayed without re-executing the display function and regenerating the primitives. 1.2 Name two other artifacts in computer graphics that may commonly be specified at the vertices of a polygon and then interpolated across the polygon to give a value for each fragment within the polygon. [2] colours , normals 1.2 Can the standard OpenGL pipeline easily handle light interactions from object to object? Explain? [3] No, local lighting model is used, vertices are processed independently and they cannot easily interact with each other. All objects appear same to the viewer. Reflections of objects and well as shadows are not handled easily. 1.4 as major stages of the graphics pipeline can be described a. Vertex processing b. Clipping and primitive assembly c. Rasterization d. Fragment processing In which of these 4 major stages would the following normally occur. (Give The 4 [TURN OVER] 3 COS3712 May/June 2013 the correct letter) [TURN OVER] 4 COS3712 May/June 2013 1.4.1 Texture mapping d 1.4.2 Perspective division a 1.4.3 Inside ‚Äì outside testing c 1.4.4 Vertices are assembled into objects C 1.4.5 Z-buffer algorithm d QUESTION 2: Transformations [8] 2.1 Do the following transformation sequences commute? If they do commute under certain conditions, state those conditions. 2.1.1 rotation and translation does not commute 2.1.2 rotation and scaling commute if scaling is uniform 2.1.3 two rotations commute if rotations are along the same axis [3] [TURN OVER] 5 COS3712 May/June 2013 2.2 Consider a line segment (in 3 dimensions) with endpoints a and b at (0, 1, 0) and (1, 2, 3) respectively after each application of the following sequence of transformations . Compute the coordinates of vertices a that result of the line segment: 2.2.1 Perform scaling by a factor of 3 along the x-axis. [1] 2.2.2 Then perform a translation of 2 units along the y-axis. [1] 2.2.3 Finally perform an anti-clockwise rotation by 60Ãä about the z-axis. [3] Hint: The transformation matrix for rotation about the z-axis is given below (where Œ∏ is the angle of anti-clockwise rotation). Answer: 2.2.1 To scale a by a factor of 3 along the x-axis, we simply multiply their x coordinates by 3. In other words a' = (0, 1, 0) 2.2.2 Then to translate a' by 2 units along the y axis, we simply add 2 to their y coordinates. In other words a'' = (0, 3, 0) 2.2.3 Finally, to rotate a'' by 60¬∫ about the z-axis, we use the rotation matrix [TURN OVER] 6 COS3712 May/June 2013 QUESTION 3: Viewing [8] 3.1 Explain what is meant by non-uniform foreshortening. [1] The images of objects farther from the centre of projection are reduced in size compared to objects that are closer to the COP 3.2 What is the purpose of projection normalization in computer graphics pipeline? Name one advantage of using this technique [3] Projection normalization ensures that all projections are converted to orthogonal projections by first distorting the objects such that the orthogonal projection of the distorted objects is the same as the desired projection of the original objects. The normalization process allows us to carry parallel and perspective projections with the same pipeline. 3.3 Draw a view frustum. Position and name the three important rectangular planes at their correct positions. Make sure that the position of the origin and the orientation of the z-axis are clearly distinguishable. State the name of the coordinate system (or frame) in which the view frustum is defined? [4] View coordinate system [TURN OVER] 7 COS3712 May/June 2013 QUESTION 4: Lighting and Shading [12] 4.1 4.1.1 Describe the four vectors the model uses to calculate a colour for an arbitrary point p. Illustrate with a figure. [4] ‚Ä¢ the vector n is the normal at p ‚Ä¢ the vector v is in the direction from p to the viewer (or the centre of projection, COP) ‚Ä¢ the vector l is in the direction from p to the (point) light source ‚Ä¢ the vector r is in the direction that a perfectly reflected ray from l would take; r is determined by vectors n and l 4.1.2 In the specu lar t erm, t here is a f act or of ( _r ¬∑ _v) p. W hat does p r ef er t o? What effect does varying the power phave? Shininess coefficient ‚Äì increasing p will cause reflected light is concentrated in a narrower region centered on the angle of a perfect reflector. Extremely high values of p will have a mirror like effect. 4.1.3 What is the term kaIa? What does ka refer to? How will decreasing ka affect the rendering of a surface? Ambient term ‚Äì reflection coefficient ‚Äì surface will be darker because less light is reflected 4.2 Consider Gouraud and Phong shading: Which one is more realistic, especially for highly curved surfaces? Why. [3] Phong. Gouraud shading computes colors at the vertices and then interpolates them across the triangle and so cannot produce an interior highlight. Phong interpolates surface normals and computes the color at each interior pixel. The Phong reflection model is an approximation of the physical reality to produce good renderings under a variety of lighting conditions and material properties. In this model there are three terms: an ambient term, a diffuse term, and a specular term. [TURN OVER] 7 COS3712 May/June 2013 QUESTION 5: Discrete Techniques [8] 5.1 Explain what is meant by bump mapping. What does the value at each pixel in a bump map correspond to? How is this data used in rendering? [4] Like the texture map that maps a pattern (of colours) to a surface, we can create a mapping that alters the normals in the polygon so the shading model can create the effect of a bumpy surface. This is called a bump map, and like Phong shading the normal for each individual pixel is computed separately. Here the pixel normal is computed as the normal from Phong shading plus a normal computed from the bump map by the gradient of the colour. The colour of each individual pixel is then computed from the standard lighting model. The colours produced by the shading then give the appearance of bumps on the surface. Note that the bump map itself can be defined simply as a 2D image where the height of each point is defined by the colour; this is called a height field. 5.2 What technique computes the surroundings visible as a reflected image in a shiny object? [1] environment mapping 5.4 Describe what is meant by is point sampling and linear filtering? Why is linear filtering a better choice than point sampling in the context of aliasing of textures. [3] When we map textures to the array of texels, we rarely get a point that corresponds to the center of a texel. One option is to use the value of the texel that is closest to the texture coordinate output by the rasterizer . This is known as point sampling. One could also use a weighted average of a group of texels in the neighborhood determined by point sampling. This is known as linear filtering. Point sampling is more subject to aliasing errors. QUESTION 6: Hidden surface removal [7] 6.1 Draw a picture of a set of simple polygons that the Depth sort algorithm cannot render without splitting the polygons. [1] Picture must have 3 polygons that overlap [TURN OVER] 8 COS3712 May/June 2013 6.2 Why can‚Äôt the standard z-buffer algorithm handle scenes with both opaque and translucent objects? W ha t modifications can be made to the z-buffer algorithm for it to handle this? [4] In a scene containing both opaque and translucent polygons, any polygon (or part of a polygon) behind an opaque polygon should not be rendered, but polygons (or parts of polygons) behind translucent polygons should be composited. If all polygons are rendered with the standard z-buffer algorithm, compositing will not be performed correctly, particularly if a translucent polygon is rendered first, and an opaque behind it is rendered later. However, if we make the z-buffer read-only when rendering translucent polygons, we can prevent the depth information from being updated when rendering translucent objects. In other words, if the depth information allows a pixel to be rendered, it is blended (composited) with the pixel already stored there. If the pixel is part of an opaque polygon, the depth data is updated, but if it is a translucent pixel, the depth data is not updated. 6.3 Explain what is meant by Backface culling. [2] To speed rendering, there is no need to model sides of polygons that are not visible. If we define our world such that the normal of our polygons always points outwards, we can speed up the rendering by removing all polygons with normals pointing away from the observer, as we know that these will be hidden anyway. QUESTION 7: Clipping [5] 7.1 Give one advantage and one disadvantage of the Cohen-Sutherland line clipping algorithm? [2] Advantages ‚Äì works best when there are many line segments but few are displayed. -It can be extended to three dimensions Disadvantage ‚Äì must be used recursively 7.2 What is the crossing or odd even test? Explain it with respect to a point p inside a polygon. [3] The odd-even test can be used to determine whether a point lies inside the polygon or not: if a \"ray\" from the point to infinity crosses a polygon edge an odd number of times the point is inside the polygon, otherwise it is not. In the scan- line algorithm for filling polygons, you simply increment a test variable by one every time the scan-line intersects a polygon edge: if the test variable is odd [TURN OVER] 9 COS3712 May/June 2013 (assuming that it is cleared initially) you are inside the polygon, if it is even you are outside. QUESTION 8: OpenGL [8] 8.1 Do you set a texture coordinate using the statement glTexCoord2f(s0, t0); before or after the vertex it applies to? [1] 8.2 GLUT uses a callback function event model. Describe how it works and state the purpose of the idle, display and reshape callbacks. [5] GLUT programs responds to events through functions called callbacks . A callback function is associated with a specific type of event. Idle callback ‚Äì for animated displays, to continue to generate praphical primitives through a display function while nothing else is happening. Reshape callback ‚Äì to change viewport size when a window size changes. Display callback ‚Äì generated when application program it is determined that graphics in a window needs to be redrawn. 8.3 What is the purpose of the OpenGL glFlush statement? [1] Ensures that all data are rendered asap, program will work if left out but delays are noticed in busy or networked environments. 8.4 Is the following code a fragment or vertex shader? in vec4 vPosition Void main() {gl_Position = vPosition} [1] Vertex shader Before 1 COS3712 May/Jun 2015 COS3712 May/June 2015 COMPUTER SCIENCE COMPUTER GRAPHICS Duration: 2 hours Total: 70 marks Examiners: First: Mr L Aron Second: Mr C Dongmo External: Mr JCW Kroeze ........................................................................................................... ........................................................................................................... MEMORANDUM [TURN OVER] Open Rubric 2 COS3712 May/June 2015 [TURN OVER] QUESTION 1 [12] 1.1 A real-time graphics program can use a single frame buffer for rendering polygons,clearing the buffer, and repeating the process. Why do we usually use two buffers instead? (3) 1.2 OpenGL uses a pipeline model to process vertices during rendering. In a typical OpenGL application a vertex will go through a sequence of 6 transformations or change of frames. In each frame the vertex has different coordinates. Name these coordinates and explain the process from one coordinate system to the next. (6) Double buffering is a technique for tricking the eye into seeing smooth animation of rendered scenes. One buffer (the front buffer) is displayed on the screen while the other (the back buffer) is currently being drawn to. ‚úìDuring the vertical retrace period the buffers are swapped. Double buffering is needed because of shearing or flickering that occurs when we draw to the colour buffer that is currently being displayed. ‚úìThe front buffer is displayed while the application renders into the back buffer. When the application completes rendering to the back buffer, it requests the graphics display hardware to swap the roles of the buffers, causing the back buffer to now be displayed, and the previous front buffer to become the new back buffer. ‚úì When we model an object, this is usually done in its own reference frame. The object coordinate system. ‚úìWe want to place our object in the world, thus we switch to the world coordinate system. This is done by standard affine transformations ‚úìWhen we look at our world, we prefere a frame where the observer is in the origin, the view coordinate system. Switching to this one is done by the view transformation. ‚úìWe wish to clip away as much as possible of the world as fast as possible. Clipping is simpler in some standardized clip coordinate system, ‚úì so we distort our view volume into e.g. a chopped pyramid, or a cube. This is usually called the clip coordinate system. If the clip coordinate system is not a standard cube with side two, we usually wish to transform our view volume inte that shape. So that we get Normaliced Device Coordinates. ‚úìThis is called projection normalisation, and is sometimes combined with the clip distortion. After the we may do orthographics projection down into 2D. And we end up in a window coordinate system. The last step is to move out on the screen, the view port. So we make a view port transformation to get to the screen coordinate system. ‚úì [TURN OVER] 3 COS3712 May/Jun 2015 1.3 Since clipping to a clip region that is a cube is so easy, graphics systems tranform any scene with its clip window to make the clip window a cube. Name this transformation technique. (1) ‚úì 1.4 In the graphics pipeline, when a triangle is processed, the (x,y,z) coordinates of the vertices are interpolated across the whole triangle to give the coordinates of each fragment. Name two other things that may commonly be specified at the vertices and then interpolated across the triangle to give a value for each fragment. (2) ‚úì‚úì QUESTION 2 :Transformations and Viewing [12] 2.1 Transformations are often carried out using a homogeneous co-ordinate representation. Give 3 reasons as to why this representation is used? (3) Any 3 reasons When points and vectors are represented using 3-dimensional column matrices one cannot distinguish between a point and a vector, with homogeneous coordinates we can make this distinction. ‚úì A matrix multiplication in 3-dimensions cannot represent a change in frames, while this can be done using homogeneous coordinates. ‚úì All affine transformations can be represented as matrix multiplications in homogeneous coordinates. ‚úì Less arithmetic work is involved when using homogeneous coordinates. ‚úì The uniform representation of all affine transformations makes carrying out successive transformations far easier than in 3 dimensional space. ‚úì Modern hardware implements homogeneous coordinates operations directly, using parallelism to achieve high speed calculations. ‚úì they allow us to express perspective projection as a 4 x 4 projection matrix ‚úì normals, colors. view normalization. [TURN OVER] 4 COS3712 May/June 2015 2.2 Differentiate between Orthographic and perspective projection. (4) 2.3 A synthetic camera co-ordinate reference frame is given by a view reference point (VRP) a view plane normal (VPN) and a view up vector (VUP). 2.3.1 Draw a diagram to show how these quantities describe the location and orientation of the synthetic camera. (2) 2.3.2 The view up vector is normally resolved to be orthogonal to the view plane normal. Why? (2) 2.3.3 Consider a camera located at point e, specified in the object frame, and is pointed at a second point a. Express VPN in terms of these two points. (1) In a perspective projection, lines (called projectors) are drawn from the objects to a point called the centre of projection(COP). The projection of the objects is where these lines intersect the projection plane. ‚úî‚úî In an orthographic projection, the projectors do not converge to a point but are parallel to one another, in a particular direction - the so called direction of projection. In this case, the COP is assumed to be at an infinite distance. As with a perspective projection, the projection of the objects is where the projectors intersect the projection plane. ‚úî‚úî Use of the projection allows us to specify any vector not parallel to v, rather than being forced to compute a vector lying in the projection plane. ‚úî We use the cross product of VUP and VPN to obtain VUP. ‚úî vpn = a - e [TURN OVER] 5 COS3712 May/Jun 2015 QUESTION 3 : Hidden surface removal [12] 3.1 Consider the z-buffer algorithm used by OpenGL for hidden surface removal 3.1 Briefly describe the z-buffer algorithm. (3) 3.2 Does the z-buffer algorithm operate in object space or image space? (1) Image space ‚úî 3.3 In the z-buffer algorithm, is shading performed before or after hidden surfaces are eliminated? Explain why. (2) 3.4 Why can‚Äô t the standard z-buffer algorithm handle scenes with both opaque and transluc ent objects? Wh a t modifications can be made to the z-buffer algori thm for it to handle this? (3) rasterization is done polygon by polygon‚úì. For each fragment on the polygon corresponding to the intersection of the polygon with a ray (from the centre of projection) through a pixel we compute the depth from the COP. If depth is greater than depth currently stored in z-buffe‚úìr it is ignored else z-buffer is updated and colour buffer is updated with new colour for fragment‚úì. Shading is performed before hidden surface removal. ‚úî In the z-buffer algorithm polygons are first rasterized and then for each fragment of the polygon depth values are determined and compared to the z-buffer. ‚úî If all polygons are rendered with the standard z-buffer algorithm, compositing will not be performed correctly, particularly if a translucent polygon is rendered first, and an opaque behind it is rendered later. ‚úî However, if we make the z-buffer read-only when rendering translucent polygons, we can prevent the depth information from being updated when rendering translucent objects. ‚úîIn other words, if the depth information allows a pixel to be rendered, it is blended (composited) with the pixel already stored there. If the pixel is part of an opaque polygon, the depth data is updated, but if it is a translucent pixel, the depth data is not updated. ‚úî [TURN OVER] 6 COS3712 May/June 2015 3.2 Briefly describe the algorithm for removing (or ‚Äúculling\") backfacing polygons. Assume that the normal points out from the visible side of the polygon. (3) v Œ∏ n ‚úì If Œ∏ is the angle between the normal and the viewer ‚úìthen the polygon is facing forward iff -90‚â§ Œ∏‚â§90 or cos Œ∏ ‚â• 0, using dot product n.v ‚â• 0 ‚úì QUESTION 4: Lighting and Shading [10] A polygonal mesh comprises many flat polygons, each of which has a well-defined normal. Name and describe three different ways to shade these polygons. ÔÇ∑ Flat shading (or constant shading): The shading calculation is carried out only once for each polygon, and each point on the polygon is assigned the same shade. Flat shading will show differences in shading among adjacent polygons. We will see stripes, known as Mach bands, along the edges. ÔÇ∑ Gouraud shading (or smooth shading): The lighting calculation is done at each vertex using the material properties and the vectors , , and . Thus, each vertex will have its own colourthat the rasterizer can use to interpolate a shade foreach fragment. We define the normal at a vertex to be the normalized average of the normals of the polygons that share the vertex. We implement Gouraud shading either in the application or in the vertex shader. ÔÇ∑ Phong shading: Instead of interpolating vertex intensities (colours), we interpolate normals across each polygon. We can thus make an independent lighting calculation for each fragment. We implement Phong shading in the fragment shader. QUESTION 5 : Discrete Techniques [8] A fairly simple and cheap (computationally) way of obtaining realistic-looking images is to use texture maps. [TURN OVER] 7 COS3712 May/Jun 2015 5.1 What are texture maps? (2) 5.2 In OpenGL, what steps are needed in order to apply texture on a polygon.? (3) 5.3 Describe one difficulty facing the implementer of a graphics package when the map is to be applied on an image? (1) 5.4 Describe what I might do if I wanted to make my object bumpy, but not change the actual object. (2) QUESTION 6 [8] Bresenham derived a line-rasterization algorithm that avoids floating-point arithmetic. 6.1 Explain, using an example, how Bresenham‚Äôs algorithm determines how to draw pixels based on the gradient of line. Use a diagram to assist with your explanation. (6) Texture mapping uses a pattern (or texture) to determine the color of a fragment. ‚úìThese patterns could be determined by a fixed pattern, such as the regular patterns often used to draw polygons; by a procedural texture-generation method; or through a digitized image. In all cases, we can characterize the image produced by a mapping of a texture to the surface . ‚úì First the a texture image is formed and placed in texture memory, ‚úì then texture coordinates are assigned to each fragment, ‚úì finally the texture is applied to each fragment. ‚úì One of the difficulties is that it is not always possible to find mapping functions to complete the mapping from texel to object coordinates. ‚úì Like the texture map that maps a pattern (of colours) to a surface, we can create a mapping that alters the normals in the polygon so the shading model can create the effect of a bumpy surface. This is called a bump map, 8 COS3712 May/June 2015 [TURN ‚úì‚úì 6.2 Bresenham‚Äôs algorithm has become the standard approach used in hardware and software rasterizers as opposed to the more simpler DDA algorithm. Why is this so? (2) OVER] Bresenham‚Äôs algorithm begins with the point (0, 0) and ‚Äúilluminates‚Äù that pixel. ‚úì .Rather than keeping track of the y coordinate (which increases by m = ‚àÜy/‚àÜx, each time the x increases by one), the algorithm keeps an error bound E at each stage, ‚úìwhich represents the negative of the distance from the point where the line exits the pixel to the top edge of the pixel . This value is first set to m ‚àí 1, and is incremented by m each time the x coordinate is incremented by one. ‚úìIf E becomes greater than zero, we know that the line has moved upwards one pixel, and that we must increment our y coordinate and readjust the error to represent the distance from the top of the new pixel ‚Äì which is done by subtracting one from E. ‚úì The DDA algorithm is efficient and can be coded easily, but it requires a floating-point addition for each pixel generated. Bresenhams algorithm avoids all floating point calculations. 9 COS3712 May/Jun 2015 QUESTION 7 [8] 7.1 What is the purpose of the reshape callback function? (1) 7.2 The following two OpenGL statements are used for double duffering: 7.2.1 glutInitDisplayMode(GLUT_DOUBLE); 7.2.2 glutSwapBuffers(); Explain the purpose for each statement. (2) 8.3 What is the purpose of the OpenGL glFlush statement? (2) 8.4 You are required to add a simple menu, with 4 items, to an OpenGL program. The menu is activated by the right click of the mouse. State which OpenGL statements you would use to do this. (3) [TURN OVER] Reshape callback ‚Äì to change viewport size when a window size changes. ‚úì 7.2.1 The use of both front and back (colour) buffers is engaged. ‚úì 7.2.2 Here the contents of the back buffer are copied into the front buffer. ‚úì Ensures that all data are rendered asap, ‚úì program will work if left out but delays are noticed in busy or networked environments. ‚úì glutCreateMenu(); ‚úì glutAddMenuEntry(); ‚úì glutAttachMenu(GLUT_RIGHT_BUTTON); ‚úì 1 COS3712 May/Jun 2015 COS3712 May/June 2015 COMPUTER SCIENCE COMPUTER GRAPHICS Duration: 2 hours Total: 70 marks Examiners: First: Mr L Aron Second: Mr C Dongmo External: Mr JCW Kroeze ........................................................................................................... ........................................................................................................... MEMORANDUM [TURN OVER] Open Rubric 2 COS3712 May/June 2015 [TURN OVER] QUESTION 1 [12] 1.1 A real-time graphics program can use a single frame buffer for rendering polygons,clearing the buffer, and repeating the process. Why do we usually use two buffers instead? (3) 1.2 OpenGL uses a pipeline model to process vertices during rendering. In a typical OpenGL application a vertex will go through a sequence of 6 transformations or change of frames. In each frame the vertex has different coordinates. Name these coordinates and explain the process from one coordinate system to the next. (6) Double buffering is a technique for tricking the eye into seeing smooth animation of rendered scenes. One buffer (the front buffer) is displayed on the screen while the other (the back buffer) is currently being drawn to. ‚úìDuring the vertical retrace period the buffers are swapped. Double buffering is needed because of shearing or flickering that occurs when we draw to the colour buffer that is currently being displayed. ‚úìThe front buffer is displayed while the application renders into the back buffer. When the application completes rendering to the back buffer, it requests the graphics display hardware to swap the roles of the buffers, causing the back buffer to now be displayed, and the previous front buffer to become the new back buffer. ‚úì When we model an object, this is usually done in its own reference frame. The object coordinate system. ‚úìWe want to place our object in the world, thus we switch to the world coordinate system. This is done by standard affine transformations ‚úìWhen we look at our world, we prefere a frame where the observer is in the origin, the view coordinate system. Switching to this one is done by the view transformation. ‚úìWe wish to clip away as much as possible of the world as fast as possible. Clipping is simpler in some standardized clip coordinate system, ‚úì so we distort our view volume into e.g. a chopped pyramid, or a cube. This is usually called the clip coordinate system. If the clip coordinate system is not a standard cube with side two, we usually wish to transform our view volume inte that shape. So that we get Normaliced Device Coordinates. ‚úìThis is called projection normalisation, and is sometimes combined with the clip distortion. After the we may do orthographics projection down into 2D. And we end up in a window coordinate system. The last step is to move out on the screen, the view port. So we make a view port transformation to get to the screen coordinate system. ‚úì [TURN OVER] 3 COS3712 May/Jun 2015 1.3 Since clipping to a clip region that is a cube is so easy, graphics systems tranform any scene with its clip window to make the clip window a cube. Name this transformation technique. (1) ‚úì 1.4 In the graphics pipeline, when a triangle is processed, the (x,y,z) coordinates of the vertices are interpolated across the whole triangle to give the coordinates of each fragment. Name two other things that may commonly be specified at the vertices and then interpolated across the triangle to give a value for each fragment. (2) ‚úì‚úì QUESTION 2 :Transformations and Viewing [12] 2.1 Transformations are often carried out using a homogeneous co-ordinate representation. Give 3 reasons as to why this representation is used? (3) Any 3 reasons When points and vectors are represented using 3-dimensional column matrices one cannot distinguish between a point and a vector, with homogeneous coordinates we can make this distinction. ‚úì A matrix multiplication in 3-dimensions cannot represent a change in frames, while this can be done using homogeneous coordinates. ‚úì All affine transformations can be represented as matrix multiplications in homogeneous coordinates. ‚úì Less arithmetic work is involved when using homogeneous coordinates. ‚úì The uniform representation of all affine transformations makes carrying out successive transformations far easier than in 3 dimensional space. ‚úì Modern hardware implements homogeneous coordinates operations directly, using parallelism to achieve high speed calculations. ‚úì they allow us to express perspective projection as a 4 x 4 projection matrix ‚úì normals, colors. view normalization. [TURN OVER] 4 COS3712 May/June 2015 2.2 Differentiate between Orthographic and perspective projection. (4) 2.3 A synthetic camera co-ordinate reference frame is given by a view reference point (VRP) a view plane normal (VPN) and a view up vector (VUP). 2.3.1 Draw a diagram to show how these quantities describe the location and orientation of the synthetic camera. (2) 2.3.2 The view up vector is normally resolved to be orthogonal to the view plane normal. Why? (2) 2.3.3 Consider a camera located at point e, specified in the object frame, and is pointed at a second point a. Express VPN in terms of these two points. (1) In a perspective projection, lines (called projectors) are drawn from the objects to a point called the centre of projection(COP). The projection of the objects is where these lines intersect the projection plane. ‚úî‚úî In an orthographic projection, the projectors do not converge to a point but are parallel to one another, in a particular direction - the so called direction of projection. In this case, the COP is assumed to be at an infinite distance. As with a perspective projection, the projection of the objects is where the projectors intersect the projection plane. ‚úî‚úî Use of the projection allows us to specify any vector not parallel to v, rather than being forced to compute a vector lying in the projection plane. ‚úî We use the cross product of VUP and VPN to obtain VUP. ‚úî vpn = a - e [TURN OVER] 5 COS3712 May/Jun 2015 QUESTION 3 : Hidden surface removal [12] 3.1 Consider the z-buffer algorithm used by OpenGL for hidden surface removal 3.1 Briefly describe the z-buffer algorithm. (3) 3.2 Does the z-buffer algorithm operate in object space or image space? (1) Image space ‚úî 3.3 In the z-buffer algorithm, is shading performed before or after hidden surfaces are eliminated? Explain why. (2) 3.4 Why can‚Äô t the standard z-buffer algorithm handle scenes with both opaque and transluc ent objects? Wh a t modifications can be made to the z-buffer algori thm for it to handle this? (3) rasterization is done polygon by polygon‚úì. For each fragment on the polygon corresponding to the intersection of the polygon with a ray (from the centre of projection) through a pixel we compute the depth from the COP. If depth is greater than depth currently stored in z-buffe‚úìr it is ignored else z-buffer is updated and colour buffer is updated with new colour for fragment‚úì. Shading is performed before hidden surface removal. ‚úî In the z-buffer algorithm polygons are first rasterized and then for each fragment of the polygon depth values are determined and compared to the z-buffer. ‚úî If all polygons are rendered with the standard z-buffer algorithm, compositing will not be performed correctly, particularly if a translucent polygon is rendered first, and an opaque behind it is rendered later. ‚úî However, if we make the z-buffer read-only when rendering translucent polygons, we can prevent the depth information from being updated when rendering translucent objects. ‚úîIn other words, if the depth information allows a pixel to be rendered, it is blended (composited) with the pixel already stored there. If the pixel is part of an opaque polygon, the depth data is updated, but if it is a translucent pixel, the depth data is not updated. ‚úî [TURN OVER] 6 COS3712 May/June 2015 3.2 Briefly describe the algorithm for removing (or ‚Äúculling\") backfacing polygons. Assume that the normal points out from the visible side of the polygon. (3) v Œ∏ n ‚úì If Œ∏ is the angle between the normal and the viewer ‚úìthen the polygon is facing forward iff -90‚â§ Œ∏‚â§90 or cos Œ∏ ‚â• 0, using dot product n.v ‚â• 0 ‚úì QUESTION 4: Lighting and Shading [10] A polygonal mesh comprises many flat polygons, each of which has a well-defined normal. Name and describe three different ways to shade these polygons. ÔÇ∑ Flat shading (or constant shading): The shading calculation is carried out only once for each polygon, and each point on the polygon is assigned the same shade. Flat shading will show differences in shading among adjacent polygons. We will see stripes, known as Mach bands, along the edges. ÔÇ∑ Gouraud shading (or smooth shading): The lighting calculation is done at each vertex using the material properties and the vectors , , and . Thus, each vertex will have its own colourthat the rasterizer can use to interpolate a shade foreach fragment. We define the normal at a vertex to be the normalized average of the normals of the polygons that share the vertex. We implement Gouraud shading either in the application or in the vertex shader. ÔÇ∑ Phong shading: Instead of interpolating vertex intensities (colours), we interpolate normals across each polygon. We can thus make an independent lighting calculation for each fragment. We implement Phong shading in the fragment shader. QUESTION 5 : Discrete Techniques [8] A fairly simple and cheap (computationally) way of obtaining realistic-looking images is to use texture maps. [TURN OVER] 7 COS3712 May/Jun 2015 5.1 What are texture maps? (2) 5.2 In OpenGL, what steps are needed in order to apply texture on a polygon.? (3) 5.3 Describe one difficulty facing the implementer of a graphics package when the map is to be applied on an image? (1) 5.4 Describe what I might do if I wanted to make my object bumpy, but not change the actual object. (2) QUESTION 6 [8] Bresenham derived a line-rasterization algorithm that avoids floating-point arithmetic. 6.1 Explain, using an example, how Bresenham‚Äôs algorithm determines how to draw pixels based on the gradient of line. Use a diagram to assist with your explanation. (6) Texture mapping uses a pattern (or texture) to determine the color of a fragment. ‚úìThese patterns could be determined by a fixed pattern, such as the regular patterns often used to draw polygons; by a procedural texture-generation method; or through a digitized image. In all cases, we can characterize the image produced by a mapping of a texture to the surface . ‚úì First the a texture image is formed and placed in texture memory, ‚úì then texture coordinates are assigned to each fragment, ‚úì finally the texture is applied to each fragment. ‚úì One of the difficulties is that it is not always possible to find mapping functions to complete the mapping from texel to object coordinates. ‚úì Like the texture map that maps a pattern (of colours) to a surface, we can create a mapping that alters the normals in the polygon so the shading model can create the effect of a bumpy surface. This is called a bump map, 8 COS3712 May/June 2015 [TURN ‚úì‚úì 6.2 Bresenham‚Äôs algorithm has become the standard approach used in hardware and software rasterizers as opposed to the more simpler DDA algorithm. Why is this so? (2) OVER] Bresenham‚Äôs algorithm begins with the point (0, 0) and ‚Äúilluminates‚Äù that pixel. ‚úì .Rather than keeping track of the y coordinate (which increases by m = ‚àÜy/‚àÜx, each time the x increases by one), the algorithm keeps an error bound E at each stage, ‚úìwhich represents the negative of the distance from the point where the line exits the pixel to the top edge of the pixel . This value is first set to m ‚àí 1, and is incremented by m each time the x coordinate is incremented by one. ‚úìIf E becomes greater than zero, we know that the line has moved upwards one pixel, and that we must increment our y coordinate and readjust the error to represent the distance from the top of the new pixel ‚Äì which is done by subtracting one from E. ‚úì The DDA algorithm is efficient and can be coded easily, but it requires a floating-point addition for each pixel generated. Bresenhams algorithm avoids all floating point calculations. 9 COS3712 May/Jun 2015 QUESTION 7 [8] 7.1 What is the purpose of the reshape callback function? (1) 7.2 The following two OpenGL statements are used for double duffering: 7.2.1 glutInitDisplayMode(GLUT_DOUBLE); 7.2.2 glutSwapBuffers(); Explain the purpose for each statement. (2) 8.3 What is the purpose of the OpenGL glFlush statement? (2) 8.4 You are required to add a simple menu, with 4 items, to an OpenGL program. The menu is activated by the right click of the mouse. State which OpenGL statements you would use to do this. (3) [TURN OVER] Reshape callback ‚Äì to change viewport size when a window size changes. ‚úì 7.2.1 The use of both front and back (colour) buffers is engaged. ‚úì 7.2.2 Here the contents of the back buffer are copied into the front buffer. ‚úì Ensures that all data are rendered asap, ‚úì program will work if left out but delays are noticed in busy or networked environments. ‚úì glutCreateMenu(); ‚úì glutAddMenuEntry(); ‚úì glutAttachMenu(GLUT_RIGHT_BUTTON); ‚úì 1 COS3712 May/Jun 2015 COS3712 May/June 2015 COMPUTER SCIENCE COMPUTER GRAPHICS Duration: 2 hours Total: 70 marks Examiners: First: Mr L Aron Second: Mr C Dongmo External: Mr JCW Kroeze ........................................................................................................... ........................................................................................................... MEMORANDUM [TURN OVER] Open Rubric 2 COS3712 May/June 2015 [TURN OVER] QUESTION 1 [12] 1.1 A real-time graphics program can use a single frame buffer for rendering polygons,clearing the buffer, and repeating the process. Why do we usually use two buffers instead? (3) 1.2 OpenGL uses a pipeline model to process vertices during rendering. In a typical OpenGL application a vertex will go through a sequence of 6 transformations or change of frames. In each frame the vertex has different coordinates. Name these coordinates and explain the process from one coordinate system to the next. (6) Double buffering is a technique for tricking the eye into seeing smooth animation of rendered scenes. One buffer (the front buffer) is displayed on the screen while the other (the back buffer) is currently being drawn to. ‚úìDuring the vertical retrace period the buffers are swapped. Double buffering is needed because of shearing or flickering that occurs when we draw to the colour buffer that is currently being displayed. ‚úìThe front buffer is displayed while the application renders into the back buffer. When the application completes rendering to the back buffer, it requests the graphics display hardware to swap the roles of the buffers, causing the back buffer to now be displayed, and the previous front buffer to become the new back buffer. ‚úì When we model an object, this is usually done in its own reference frame. The object coordinate system. ‚úìWe want to place our object in the world, thus we switch to the world coordinate system. This is done by standard affine transformations ‚úìWhen we look at our world, we prefere a frame where the observer is in the origin, the view coordinate system. Switching to this one is done by the view transformation. ‚úìWe wish to clip away as much as possible of the world as fast as possible. Clipping is simpler in some standardized clip coordinate system, ‚úì so we distort our view volume into e.g. a chopped pyramid, or a cube. This is usually called the clip coordinate system. If the clip coordinate system is not a standard cube with side two, we usually wish to transform our view volume inte that shape. So that we get Normaliced Device Coordinates. ‚úìThis is called projection normalisation, and is sometimes combined with the clip distortion. After the we may do orthographics projection down into 2D. And we end up in a window coordinate system. The last step is to move out on the screen, the view port. So we make a view port transformation to get to the screen coordinate system. ‚úì [TURN OVER] 3 COS3712 May/Jun 2015 1.3 Since clipping to a clip region that is a cube is so easy, graphics systems tranform any scene with its clip window to make the clip window a cube. Name this transformation technique. (1) ‚úì 1.4 In the graphics pipeline, when a triangle is processed, the (x,y,z) coordinates of the vertices are interpolated across the whole triangle to give the coordinates of each fragment. Name two other things that may commonly be specified at the vertices and then interpolated across the triangle to give a value for each fragment. (2) ‚úì‚úì QUESTION 2 :Transformations and Viewing [12] 2.1 Transformations are often carried out using a homogeneous co-ordinate representation. Give 3 reasons as to why this representation is used? (3) Any 3 reasons When points and vectors are represented using 3-dimensional column matrices one cannot distinguish between a point and a vector, with homogeneous coordinates we can make this distinction. ‚úì A matrix multiplication in 3-dimensions cannot represent a change in frames, while this can be done using homogeneous coordinates. ‚úì All affine transformations can be represented as matrix multiplications in homogeneous coordinates. ‚úì Less arithmetic work is involved when using homogeneous coordinates. ‚úì The uniform representation of all affine transformations makes carrying out successive transformations far easier than in 3 dimensional space. ‚úì Modern hardware implements homogeneous coordinates operations directly, using parallelism to achieve high speed calculations. ‚úì they allow us to express perspective projection as a 4 x 4 projection matrix ‚úì normals, colors. view normalization. [TURN OVER] 4 COS3712 May/June 2015 2.2 Differentiate between Orthographic and perspective projection. (4) 2.3 A synthetic camera co-ordinate reference frame is given by a view reference point (VRP) a view plane normal (VPN) and a view up vector (VUP). 2.3.1 Draw a diagram to show how these quantities describe the location and orientation of the synthetic camera. (2) 2.3.2 The view up vector is normally resolved to be orthogonal to the view plane normal. Why? (2) 2.3.3 Consider a camera located at point e, specified in the object frame, and is pointed at a second point a. Express VPN in terms of these two points. (1) In a perspective projection, lines (called projectors) are drawn from the objects to a point called the centre of projection(COP). The projection of the objects is where these lines intersect the projection plane. ‚úî‚úî In an orthographic projection, the projectors do not converge to a point but are parallel to one another, in a particular direction - the so called direction of projection. In this case, the COP is assumed to be at an infinite distance. As with a perspective projection, the projection of the objects is where the projectors intersect the projection plane. ‚úî‚úî Use of the projection allows us to specify any vector not parallel to v, rather than being forced to compute a vector lying in the projection plane. ‚úî We use the cross product of VUP and VPN to obtain VUP. ‚úî vpn = a - e [TURN OVER] 5 COS3712 May/Jun 2015 QUESTION 3 : Hidden surface removal [12] 3.1 Consider the z-buffer algorithm used by OpenGL for hidden surface removal 3.1 Briefly describe the z-buffer algorithm. (3) 3.2 Does the z-buffer algorithm operate in object space or image space? (1) Image space ‚úî 3.3 In the z-buffer algorithm, is shading performed before or after hidden surfaces are eliminated? Explain why. (2) 3.4 Why can‚Äô t the standard z-buffer algorithm handle scenes with both opaque and transluc ent objects? Wh a t modifications can be made to the z-buffer algori thm for it to handle this? (3) rasterization is done polygon by polygon‚úì. For each fragment on the polygon corresponding to the intersection of the polygon with a ray (from the centre of projection) through a pixel we compute the depth from the COP. If depth is greater than depth currently stored in z-buffe‚úìr it is ignored else z-buffer is updated and colour buffer is updated with new colour for fragment‚úì. Shading is performed before hidden surface removal. ‚úî In the z-buffer algorithm polygons are first rasterized and then for each fragment of the polygon depth values are determined and compared to the z-buffer. ‚úî If all polygons are rendered with the standard z-buffer algorithm, compositing will not be performed correctly, particularly if a translucent polygon is rendered first, and an opaque behind it is rendered later. ‚úî However, if we make the z-buffer read-only when rendering translucent polygons, we can prevent the depth information from being updated when rendering translucent objects. ‚úîIn other words, if the depth information allows a pixel to be rendered, it is blended (composited) with the pixel already stored there. If the pixel is part of an opaque polygon, the depth data is updated, but if it is a translucent pixel, the depth data is not updated. ‚úî [TURN OVER] 6 COS3712 May/June 2015 3.2 Briefly describe the algorithm for removing (or ‚Äúculling\") backfacing polygons. Assume that the normal points out from the visible side of the polygon. (3) v Œ∏ n ‚úì If Œ∏ is the angle between the normal and the viewer ‚úìthen the polygon is facing forward iff -90‚â§ Œ∏‚â§90 or cos Œ∏ ‚â• 0, using dot product n.v ‚â• 0 ‚úì QUESTION 4: Lighting and Shading [10] A polygonal mesh comprises many flat polygons, each of which has a well-defined normal. Name and describe three different ways to shade these polygons. ÔÇ∑ Flat shading (or constant shading): The shading calculation is carried out only once for each polygon, and each point on the polygon is assigned the same shade. Flat shading will show differences in shading among adjacent polygons. We will see stripes, known as Mach bands, along the edges. ÔÇ∑ Gouraud shading (or smooth shading): The lighting calculation is done at each vertex using the material properties and the vectors , , and . Thus, each vertex will have its own colourthat the rasterizer can use to interpolate a shade foreach fragment. We define the normal at a vertex to be the normalized average of the normals of the polygons that share the vertex. We implement Gouraud shading either in the application or in the vertex shader. ÔÇ∑ Phong shading: Instead of interpolating vertex intensities (colours), we interpolate normals across each polygon. We can thus make an independent lighting calculation for each fragment. We implement Phong shading in the fragment shader. QUESTION 5 : Discrete Techniques [8] A fairly simple and cheap (computationally) way of obtaining realistic-looking images is to use texture maps. [TURN OVER] 7 COS3712 May/Jun 2015 5.1 What are texture maps? (2) 5.2 In OpenGL, what steps are needed in order to apply texture on a polygon.? (3) 5.3 Describe one difficulty facing the implementer of a graphics package when the map is to be applied on an image? (1) 5.4 Describe what I might do if I wanted to make my object bumpy, but not change the actual object. (2) QUESTION 6 [8] Bresenham derived a line-rasterization algorithm that avoids floating-point arithmetic. 6.1 Explain, using an example, how Bresenham‚Äôs algorithm determines how to draw pixels based on the gradient of line. Use a diagram to assist with your explanation. (6) Texture mapping uses a pattern (or texture) to determine the color of a fragment. ‚úìThese patterns could be determined by a fixed pattern, such as the regular patterns often used to draw polygons; by a procedural texture-generation method; or through a digitized image. In all cases, we can characterize the image produced by a mapping of a texture to the surface . ‚úì First the a texture image is formed and placed in texture memory, ‚úì then texture coordinates are assigned to each fragment, ‚úì finally the texture is applied to each fragment. ‚úì One of the difficulties is that it is not always possible to find mapping functions to complete the mapping from texel to object coordinates. ‚úì Like the texture map that maps a pattern (of colours) to a surface, we can create a mapping that alters the normals in the polygon so the shading model can create the effect of a bumpy surface. This is called a bump map, 8 COS3712 May/June 2015 [TURN ‚úì‚úì 6.2 Bresenham‚Äôs algorithm has become the standard approach used in hardware and software rasterizers as opposed to the more simpler DDA algorithm. Why is this so? (2) OVER] Bresenham‚Äôs algorithm begins with the point (0, 0) and ‚Äúilluminates‚Äù that pixel. ‚úì .Rather than keeping track of the y coordinate (which increases by m = ‚àÜy/‚àÜx, each time the x increases by one), the algorithm keeps an error bound E at each stage, ‚úìwhich represents the negative of the distance from the point where the line exits the pixel to the top edge of the pixel . This value is first set to m ‚àí 1, and is incremented by m each time the x coordinate is incremented by one. ‚úìIf E becomes greater than zero, we know that the line has moved upwards one pixel, and that we must increment our y coordinate and readjust the error to represent the distance from the top of the new pixel ‚Äì which is done by subtracting one from E. ‚úì The DDA algorithm is efficient and can be coded easily, but it requires a floating-point addition for each pixel generated. Bresenhams algorithm avoids all floating point calculations. 9 COS3712 May/Jun 2015 QUESTION 7 [8] 7.1 What is the purpose of the reshape callback function? (1) 7.2 The following two OpenGL statements are used for double duffering: 7.2.1 glutInitDisplayMode(GLUT_DOUBLE); 7.2.2 glutSwapBuffers(); Explain the purpose for each statement. (2) 8.3 What is the purpose of the OpenGL glFlush statement? (2) 8.4 You are required to add a simple menu, with 4 items, to an OpenGL program. The menu is activated by the right click of the mouse. State which OpenGL statements you would use to do this. (3) [TURN OVER] Reshape callback ‚Äì to change viewport size when a window size changes. ‚úì 7.2.1 The use of both front and back (colour) buffers is engaged. ‚úì 7.2.2 Here the contents of the back buffer are copied into the front buffer. ‚úì Ensures that all data are rendered asap, ‚úì program will work if left out but delays are noticed in busy or networked environments. ‚úì glutCreateMenu(); ‚úì glutAddMenuEntry(); ‚úì glutAttachMenu(GLUT_RIGHT_BUTTON); ‚úì 1 COS3712 OCT/NOV 2012 COS3712 OCT / Nov 2012 COMPUTER SCIENCE COMPUTER GRAPHICS Duration: 2 hours Total: 70 marks Examiners: First: Mr L Aron Second: Mr C Dongmo External: Prof P Marais (University of Cape Town) ........................................................................................................... ........................................................................................................... MEMORANDUM [TURN OVER] Open Rubric 2 COS3712 OCT/NOV 2012 [TURN OVER] QUESTION 1 [12] 1.1 A real-time graphics program can use a single frame buffer for rendering polygons, clearing the buffer, and repeating the process. Why do we usually use two buffers instead? [3] 1.2 Briefly explain what the accumulation buffer is and what it is used for in computer graphics. [3] 1.3 Define the term View Volume with reference to both perspective and orthogonal views. Provide the OpenGL functions that used to define the respective view volumes. [6] Double buffering is a technique for tricking the eye into seeing smooth animation of rendered scenes. One buffer (the front buffer) is displayed on the screen while the other (the back buffer) is currently being drawn to. During the vertical retrace period the buffers are swapped. Double buffering is needed because of shearing or flickering that occurs when we draw to the colour buffer that is currently being displayed.The front buffer is displayed while the application renders into the back buffer. When the application completes rendering to the back buffer, it requests the graphics display hardware to swap the roles of the buffers, causing the back buffer to now be displayed, and the previous front buffer to become the new back buffer. (Angel, Section 8.12): The accumulation buffer has the same spatial resolution as the frame buffer, but has greater depth resolution. We can think of it as a special type of color buffer whose components are stored as floating-point numbers. We can use the additional resolution to render successive images into one location while retaining numerical accuracy. The accumulation buffer can be used for a variety of operations that involve combining multiple images. (Angel, Section 8.12.1): One of the most important uses of the accumulation buffer is for antialiasing. Rather than antialiasing individual lines and polygons, we can antialias an entire scene using the accumulation buffer. (Angel Section 8.12.3): Bump mapping and embossing The view volume is analogous to the volume that a real camera would see through its lens (except that it is also limited in distance from the front and back). It is a section of 3D space that is visible from the camera or viewer between two distances. When using orthogonal (or parallel) projection, the view volume is rectangular. In OpenGL, an orthographic projection is defined with the function call glOrtho(left, right, bottom, top, near, far); 3 COS3712 OCT/NOV 2012 [TURN OVER] When using perspective projection, the view volume is a frustum and has a truncated pyramid shape. In OpenGL, a perspective projection is defined with the function call glFrustum(xmin, xmax, ymin, ymax, near, far); or gluPerspective(fovy, aspect, near, far); QUESTION 2 :Transformations [8] 2.1 Transformations are often carried out using a homogeneous co-ordinate representation. Give reasons as to why this representation is used? [2] 2.2 Consider the following 4x4 matrices: A B C D E F Which of matrices reflect the following (write down the correct letter): 2.2.1 Identity matrix (no effect) D 2.2.2 Uniform scaling F 2.2.3 Non-uniform scaling E 2.2.4 Reflection C 2.2.5 Rotation about z A 2.2.6 Rotation B [6] When points and vectors are represented using 3-dimensional column matrices one cannot distinguish between a point and a vector, with homogeneous coordinates we can make this distinction. A matrix multiplication in 3-dimensions cannot represent a change in frames, while this can be done using homogeneous coordinates. All affine transformations can be represented as matrix multiplications in homogeneous coordinates. Less arithmetic work is involved when using homogeneous coordinates. The uniform representation of all affine transformations makes carrying out successive transformations far easier than in 3 dimensional space. Modern hardware implements homogeneous coordinates operations directly, using parallelism to achieve high speed calculations. they allow us to express perspective projection as a 4 x 4 projection matrix 4 COS3712 OCT/NOV 2012 [TURN OVER] QUESTION 3 : Hidden surface removal [12] 3.1 Differentiate between depthsort and z-buffer algorithms for hidden surface removal. [6] 3.2 Briefly describe, with any appropriate equations, the algorithm for removing (or ‚Äúculling\") backfacing polygons. Assume that the normal points out from the visible side of the polygon. [6] Z-buffer ‚Äì rasterization is done polygon by polygon. For each fragment on the polygon corresponding to the intersection of the polygon with a ray (from the centre of projection) through a pixel we compute the depth from the COP. If depth is greater than depth currently stored in z-buffer it is ignored else z-buffer is updated and colour buffer is updated with new colour for fragment. Depth sort - all polygons are rendered with hidden surface removal as a consequence of back to front rendering of polygons. Depth sort orders the polygons by how far away from the viewer their maximum z-value(z-extent) is. If the minimum depth ‚Äì the z-value ‚Äì of a given polygon is greater than the maximum depth of the polygon behind the one of interest, we can render the polygons back to front v Œ∏ n If Œ∏ is the angle between the normal and the viewer then the polygon is facing forward iff -90‚â§ Œ∏‚â§90 or cos Œ∏ ‚â• 0, using dot product n.v ‚â• 0 Test can be simplified since it is applied after transformation to normalized device coordinates. All views are orthographic with DOP along z-axis Hence in homogenous coordinates v = [0010] thus if the polygon is on the surface Ax + by + cz + d =0 in normalized device coordinates then sign of c determines whether polygon is front or back facing. 5 COS3712 OCT/NOV 2012 [TURN OVER] QUESTION 4 : Lighting and Shading [12] 4.1 The shading intensity at any given point p on a surface is, in general, comprised of three contributions, each of which corresponds to a distinct physical phenomenon. List and describe all three, stating how they are computed in terms of the following vectors: n - the normal at point p [9] v - from p to viewer l ‚Äì from p to light source r ‚Äì reflection of ray from l 4.2 Describe the difference between Gouraud and Phong shading. [3] QUESTION 5 : Discrete Techniques [12] 5.1 Describe the difference between bump mapping and texture mapping. [3] 5.2 What does the value at each pixel in a bump map correspond to? How is this data used in rendering? [3] Ambient (a constant amount of light gets added to the scene, independent of the position or distance of the light source) I = kL where k is the reflection coefficient , L is ambient term Diffuse (a light hits a surface and gets scattered equally into all directions - dull or matt surfaces that appear equally bright from all viewing angles) I = k (l.n) L Specular (an incoming light gets reflected in a particular direction) I = k L Max((r.v)Œ± ,0) Œ± is the shininess coefficient In smooth shading colour per vertex is calculated using vertex normals and then this colour is interpolated across the polygon. In Phong shading, the normals at the vertices are interpolated across the surface of the polygon. The lighting model is then applied at every point of within the polygon. Whereas texture maps give detail by mapping patterns onto surfaces, bump maps distort the normal vectors during the shading process to make the surface appear to have small variations in shape, like bumps or depressions. Like the texture map that maps a pattern (of colours) to a surface, we can create a mapping that alters the normals in the polygon so the shading model can create the effect of a bumpy surface. This is called a bump map, and like Phong shading the normal for each individual pixel is computed separately. Here the pixel normal is computed as the normal from Phong shading plus a normal computed from the bump map by the gradient of the colour. The colour of each individual pixel is then computed from the standard lighting model. The colours produced by the shading then give the 6 COS3712 OCT/NOV 2012 [TURN OVER] appearance of bumps on the surface. Note that the bump map itself can be defined simply as a 2D image where the height of each point is defined by the colour; this is called a height field. 5.3 Consider the texture map with U,V coordinates in the diagram on the left. Draw the approximate mapping if the square on right were textured using the above image. [2] Answer: 5.4 Discuss the difference between the RGB colour model and the indexed colour model with respect to the depth of the frame (colour) buffer. [4] Answer: In both models, the number of colours that can be displayed depends on the depth of the frame (colour) buffer. The RGB model is used when a lot of memory is available, eg 12 or 24 bits per pixel. These bits are divided into three groups, representing the intensity of red, green and blue at the pixel, respectively. The RGB model becomes unsuitable when the depth is small, because shades become too distinct/discreet. The indexed colour model is used where memory in the colour buffer is limited. The bits per pixel are used to index into a colour-lookup table where any shades of any colours can be specified (depending only on the colours that the monitor can show). 7 COS3712 OCT/NOV 2012 [TURN OVER] QUESTION 6 [6] Using diagrams describe briefly the Liang-Barsky clipping algorithm. [6] Two marks for diagram 8 COS3712 OCT/NOV 2012 [TURN OVER] Suppose we have a line segment defined by two endpoints p (x1, y1) q(x1, y1). The parametric equation of the line segment gives x-values and y-values for every point in terms of a parameter Œ± that ranges from 0 to 1. x(Œ±) = (1 -Œ±) x1 + Œ± x2 y(Œ±) = (1- Œ±) y1 + Œ± y2 There are four points where line intersects side of windows tB tL tT tR we can order these points and then determine where clipping needs to take place. If for example tL > tR , this implies that the line must be rejected as it falls outside the window. To use this strategy effectively we need to avoid computing intersections until they are needed. Many lines can be rejected before all four intersections are known. QUESTION 7 [8] Consider the following simple animation program in OpenGL: /* This program demonstrates the use of double buffering for * flicker-free animation of the spinning motion of a square * and is adapted from the code provided in * E. Angel, Interactive Computer Graphics */ #include <GL/glut.h> //Line 1 #include <stdlib.h> //Line 2 GLfloat spin = 0.0; //Line 3 GLfloat size = 1.0; //Line 4 void display() { //Line 5 glClear(GL_COLOR_BUFFER_BIT); //Line 6 glColor3f(1, 0, 0); //Line 7 glRectf(-0.5, -0.5, 0.5, 0.5); //Line 8 glutSwapBuffers(); } //Line 9 void idle() { //Line 10 spin = spin + 1.0; //Line 11 if (spin > 360.0) //Line 12 spin = spin - 360.0; //Line 13 glLoadIdentity(); //Line 14 glRotatef(spin, 0.0, 0.0, 1.0); //Line 15 glScalef(size, size, 1.0); //Line 16 9 COS3712 OCT/NOV 2012 [TURN OVER] glutPostRedisplay(); //Line 17 } void reshape(int w, int h) { //Line 18 if (w < h) //Line 19 glViewport(0, (h-w)/2, w, w); //Line 20 else //Line 21 glViewport((w-h)/2, 0, h, h); //Line 22 } int main(int argc, char** argv) { //Line23 glutInit(&argc,argv); //Line24 glutInitDisplayMode(GLUT_DOUBLE); //Line25 glutCreateWindow(\"spinning square\"); //Line26 glClearColor(1, 1, 1, 1); //Line27 glutDisplayFunc(display); //Line28 glutReshapeFunc(reshape); //Line29 glutIdleFunc(idle); //Line30 glutMainLoop(); //Line31 return 0; //Line32 } 7.1 How does the program rotate the square if it is always defined with the same co- ordinates (in line 8)? [2] 7.2 What does line 16 do? [1] 7.3 Answer the following questions with respect to the reshape function (lines 18 to 22) passed to glutReshapeFunc (in line 29): 7.3.1 What is the purpose of the reshape function? [1] 7.3.2 Why is the matrix mode switched from GL_PROJECTION to GL_MODELVIEW? [2] Answer: The idle function changes variable spin (in lines 11, 12, and 14) and then transforms the objects in the graphics pipeline by multiplying them by an appropriate rotation matrix (in lines 14 and 15). Rotation is about the z-axis, by spin degrees. Answer: It applies a scaling transformation matrix to the objects in the graphics pipeline, adjusting the x and y coordinates only. Answer: 7.3.1 To allow the user to change the size of the window (by maximising the window or dragging its borders) so that the contents are still displayed in a 10 COS3712 OCT/NOV 2012 [TURN OVER] meaningful way. 7.3.2 The gluOrtho function operates on the projection matrix (not the module-view matrix), so we have to swap to the projection matrix mode and back. (Most other transformations take place in model-view matrix mode.) 7.4 Identify the lines of the program that deal with double buffering and explain them briefly. [2] Answer: Line 25: The use of both front and back (colour) buffers is engaged. Line 9: Here the contents of the back buffer are copied into the front buffer. COS3712 OCT / NOV 2013 COMPUTER SCIENCE COMPUTER GRAPHICS Duration: 2 hours Total: 70 marks Examiners: First: Mr L Aron Second: Mr C Dongmo External: Mr J C W Kroeze (University of Pretoria) ........................................................................................................... ........................................................................................................... MEMORANDUM QUESTION 1: OpenGL Pipeline [12] 1.1 In the OpenGL pipeline vertices go through a series of transformations of ‚Äúframes‚Äù (coordinate systems with origin) from modeling to rasterisation. Name each frame and briefly describe what each frame is for. [7] When we model an object, this is usually done in its own reference frame. The object coordinate system‚úì. We want to place our object in the world, thus we switch to the world coordinate system. This is done by standard affine transformations‚úì. When we look at our world, we prefer a frame where the observer is in the origin, the view coordinate system‚úì.We wish to clip away as much as possible of the world as fast as possible. Clipping is simpler in some standardized clip coordinate system, so we distort our view volume into e.g. a chopped pyramid, or a cube. This is usually called the clip coordinate system‚úì. If the clip coordinate system is not a standard cube with side two, we usually wish to transform our view volume into that shape. So that we get Normaliced Device Coordinates‚úì. After projection we end up in a window coordinate system‚úì. The last step is to move out on the screen, the view port. So we make a view port transformation to get to the screen coordinate system‚úì. 1.2 The 4 major stages of a graphics pipeline are a. Modelling b. Geometry Processing c. Rasterization d. Fragment processing In which of these 4 major stages would the following normally occur. (Give the correct letter) 1.2.1 Projection b 1.2.2 I n s i d e o u t s i d e t e s t i n g c 1.2.3 Linear filtering d 1.2.4 Shading b or d 1.2.5 Colours are assigned to colour buffer d [5] 3 COS3712 OCT/NOV 2013 [TURN OVER] QUESTION 2: Transformations [8] 2.1 Define the term homogeneous coordinates. [1] Homogeneous coordinates are four dimensional column matrices used to represent both point and vectors in three dimensions. ‚úì 2.2 If we wish to apply a series of transformations on a complex object, instead of calling a series of dedicated functions in a row that performs each transformation on an object, how can we reduce the amount of calculations needed by using homogeneous coordinates and matrices? [3] The big gain in using homogeneous coordinates in computer graphics is that all the affine transformations can be expressed as a matrix multiplication, this is not possible if we are simply using ordinary Cartesian coordinates. ‚úì As our transformations all can be expressed as matrix multiplications, if we wish to apply many transformations in a row to many vertices (a complex object), we do not need to apply them one at a time, but can instead multiply together the different transformations matrices once and for all into a more complex \"all the way\" transformation matrix. ‚úì Which we then can apply (i.e. multyply with) to all the points. ‚úì 2.3 Show that a sequence of two translations commute. [4] Multiply two translations matrices both ways QUESTION 3: Viewing [12] 3.1 Explain what is meant by perspective projection. [2] In a perspective projection, lines (called projectors) are drawn from the objects to a point called the centre of projection (COP). The projection of the objects is where these lines intersect the projection plane. 3.2 The task of perspective projection is often separated into two steps: projection normalization (or distortion), and orthographic projection. [TURN OVER] 4 COS3712 OCT/NOV 2012 3.2.1 What does each of these two steps do? [3] The projection normalization is distorting the view volume‚úì(the part of the world we are acually looking at) into some standardized shape, e.g. a cube with side two. ‚úì The orthographic projection that is then to follow is only to project (means that we are reducing the dimensionality) the 3D cube down to 2D (and perhaps adjust it to fit the screen). ‚úìalso mark correct if student talks about parrellel projectors. 3.2.2 Give two advantages of using this approach to perspective projection? [2] By transforming the view volume into a standardized cube, clipping is greatly simplified ‚úì. The same pipeline can be used for both perspective and parallel projections. ‚úì 3.3 A synthetic camera co-ordinate reference frame is given by a view reference point (VRP) a view plane normal (VPN) and a view up vector (VUP).Using a diagram show how these quantities describe the location and orientation of the synthetic camera. [5] V VRP U ‚úì‚úì **v and u not necessary Camera is positioned at the origin, pointing in the negative z direction Camera is centred at point called the VRP‚úì. Orientation of the camera is specified by VPN and VUP. VPN is the orientation of the projection plane or back of camera. ‚úìThe orientation of the plane does not specify the up direction of the camera hence we have VUP which is the up direction of the camera. VUP fixes the camera. ‚úì VUPVPN [TURN OVER] 5 COS3712 OCT/NOV 2013 QUESTION 4: Hidden surface removal [12] 4.1 Hidden surface removal algorithms can be divided into two broad classes 4.1.1 Name and explain these two classes? [4] Object space algorithms attempt to order surfaces of the objects in the scene such that if surfaces are rendered in the correct order then the correct image will be created. ‚úì‚úì Image space algorithms work as part of the projection process and seek to determine the relationship among object points on each projector. ‚úì‚úì 4.1.2 What type of type of algorithm is the Depth‚Äìsort algorithm? Explain your answer. [2] Object Space, ‚úì Polygons are sorted according to how far away from the viewer their maximum z-value is. Polygons are then rendered back to font. ‚úì 4.2 OpenGL makes use of a z-buffer. 4.2.1 What information is stored in the z-buffer? [1] Depth information ‚úì 4.2.2 How does OpenGL use this information for hidden surface removal? [3] rasterization is done polygon by polygon. For each fragment on the polygon corresponding to the intersection of the polygon with a ray (from the centre of projection) through a pixel we compute the depth from the COP. ‚úì If depth is greater than depth currently stored in z-buffer it is ignored else z-buffer is updated and colour buffer is updated with new colour for fragment‚úì‚úì [TURN OVER] 6 COS3712 OCT/NOV 2012 4.2.3 Give two advantages of using this approach to hidden surface removal. [2] easy to implement in either software or hardware ‚úì it is compatible with pipeline architectures - can execute at the same speed at which vertices are passing through the pipeline ‚úì Effecient ‚úì QUESTION 5: Lighting and Shading [11] 5.1 The three-dimensional nature of objects can be shown by appropriate shading. Gradations or shades of colour give 2D images the appearance of being 3D. Describe each of the three major shading techniques and discuss how objects shaded by the different methods differ in appearance. [7] In flat shading a polygon is filled with a single colour or shade across its surface. A single normal is calculated for the whole surface, and this determines the colour. ‚úì Mach bands are visible. ‚úì In Gouraud shading colour per vertex is calculated using vertex normals and then this colour is interpolated across the polygon. ‚úì Objects appear smoother that flat shaded objects but mach bands still possible. ‚úì In Phong shading, the normals at the vertices are interpolated across the surface of the polygon. ‚úì The lighting model is then applied at every point of within the polygon. ‚úìBecause normals gives the local surface orientation, by interpolating the normals across the surface of a polygon, the surface appears to be curved rather than flat hence the smoother appearance of Phong shaded images. ‚úì 5.2 In a simple computer graphics lighting model we assume the specular reflection component I = ksL cos Œ± Œ¶. 5.2.1 What lighting effect does the specular reflection component approximate? Highlights that we see reflected from shiny objects ‚úì [1] 5.2.2 What do the term ks represent? [1] [TURN OVER] 7 COS3712 OCT/NOV 2013 Fraction of incoming specular light that is reflected ‚úì 5.2.3 What effect does increasing the angle Œ¶ have? [1] Increasing Œ¶ will cause the viewer to see more of the reflected light ‚úì 5.2.4 What effect does increasing the value of Œ± have? [1] The reflected light is concentrated on a narrower region and we have more mirror effect ‚úì QUESTION 6: Rasterization [5] Using diagrams describe briefly the Liang-Barsky clipping algorithm. Two marks for diagram [TURN OVER] 8 COS3712 OCT/NOV 2012 Suppose we have a line segment defined by two endpoints p (x1, y1) q(x1, y1). The parametric equation of the line segment gives x-values and y-values for every point in terms of a parameter Œ± that ranges from 0 to 1. x(Œ±) = (1 -Œ±) x1 + Œ± x2 y(Œ±) = (1- Œ±) y1 + Œ± y2 There are four points where line intersects side of windows tB tL tT tR we can order these points and then determine where clipping needs to take place. If for example tL > tR , this implies that the line must be rejected as it falls outside the window. To use this strategy effectively we need to avoid computing intersections until they are needed. Many lines can be rejected before all four intersections are known. QUESTION 7 [10] 7.1 A fairly simple and cheap (computationally) way of obtaining realistic- looking images is to use texture maps. 7.1.1 What are texture maps? [2] Texture mapping uses a pattern (or texture) to determine the color of a fragment. ‚úìThese patterns could be determined by a fixed pattern, such as the regular patterns often used to draw polygons; by a procedural texture-generation method; or through a digitized image. In all cases, we can characterize the image produced by a mapping of a texture to the surface . ‚úì 7.1.2 What are the difficulties facing the implementer of a graphics package when a texture map is to be applied on an image? [2] Texture maps associate a texel with each point on a geometric object. ‚úìOne of the difficulties is that it is not always possible to find mapping functions to complete the mapping from texel to object coordinates. ‚úì [TURN OVER] 9 COS3712 OCT/NOV 2013 7.1.3 In OpenGL, what steps are needed in order to apply a texture on a polygon? [3] Form a texture image and place it in texture memory ‚úì Assign texture coordinates to each fragment ‚úì Apply each texture to each fragment ‚úì 7.2 Image compositing (or blending) is used to blend colour shades from several objects. Briefly describe how one can accomplish image compositing in OpenGL (from the application programmer's point of view). [3] OpenGL makes use of the Alpha Channel. The alpha channel is the fourth colour in the RGBA (or RGBŒ±) colour mode. Like the other colour components, the application program can control the value of A (or Œ±) for each pixel. ‚úì If blending is enabled in RGBA mode, the value of Œ± controls how the RGB values are written into the frame buffer. ‚úì Opacity of a surface is a measure of how much light penetrates through that surface. An opacity of 1 (Œ± = 1) corresponds to a completely opaque surface that blocks all light from surfaces hidden behind it. A surface with an opacity of 0 is transparent: All light passes through it. The transparency or translucency of a surface with opacity Œ± is given by 1 ‚àí Œ±. ‚úì Graphics Pipeline: 1. Describe the purpose of the following groups of functions: a. Primitive ‚Äì Define low level objects or atomic entities that the system can display b. Attribute ‚Äì Governs the way primitives appear on the display c. Viewing ‚Äì Allows us to specify various views d. Transformation ‚Äì Allows us to carry out transformations of objects e. Query ‚Äì Allows us to obtain info about operating environment, camera parameters, values in frame buffer, etc 2. At which stage is each of the following performed: a. Projection normalisation B b. Hidden surface removal D c. Antialiasing D d. Primitive assembly B e. Perspective division B f. Usual results of this process are sets of vertices specifying a group of geometric objects supported by the rest of the system A g. Inside Outside testing C h. Converts vertices in normalised device coordinates to fragments whose locations are in window coordinates C i. Interpolation of per-vertex coordinates takes place, and the texture parameters determine how to combine texture colour and fragment colours to determine final colours in colour buffer C 3. Explain difference between immediate and retained graphics modes Immediate: As vertices are generated, they are sent to the graphics processor to be displayed. However there is no memory of geometric data and will have to be generated again if they need to be displayed again Retained: All geometric data is computed and stored in some storage structure. The scene is then displayed by sending all data to the graphics processor at once. 4. Give 2 advantages and a disadvantage of using the pipeline approach to form CGI. Advantages: - Increases performance when the same sequence of concurrent operations in carried out on many, or large, datasets - Process on each primitive can be done independently Disadvantage: - Latency of the system must be balanced against increased throughput. - Global effects may not be handled correctly. 5. Name the frames in the usual order in which they occur in the WebGL pipeline Object coordinates ÔÉ† World coordinates ÔÉ† eye coordinates ÔÉ† clip coordinates ÔÉ† Normalised device coordinates ÔÉ† window coordinates ÔÉ† Screen coordinates 6. What are the main advantages of programmable shaders? Programmable shaders make it possible to not only incorporate more realistic lighting models in real time, but to also create interesting non-photorealistic effects. 7. Explain differences between RGB, RGBA, and indexed colour systems RGB: Uses the primary colours (Red, blue, and green) in a 24 bit colour depth, with 8 bits assigned to each of the colours. A number between 0.0 and 1.0 denote the saturation of each colour. RGBA: Same as RGB, but with a 4th channel, the alpha channel. This denotes the translucency level assigned. A value of 0 is transparent, whereas a value of 1 is opaque. Indexed Colour system: The frame buffers are limited in colour depth, 8 bits deep, and not subdivided into groups. The limited depth pixel is interpreted as an integer value indexing to the colour lookup table. 8. From Geometry to pixels: 1. Besides the transformation to 2D, explain what other transformations must be done before we can show an image on a computer screen? 2. When is clipping normally performed in the graphics pipeline? Clipping happens after vertex processing but before rasterization. 3. Why is clipping performed at this point? Clipping is carried out on primitives. If it is done before this point, the primitives would still be vertices. If done after this point, the primitives would have already been converted to fragments. 4. Describe, with use of diagrams, how polygons are clipping using the Cohen-Sutherland line clipping algorithm The algorithm divides a 2 dimensional space into 9 regions (Centre being the inside region), then efficiently determines the line and portions of lines that are inside the given rectangular area. 5. Give one advantage and one disadvantage of using the Cohen-Sutherland line clipping algorithm? Advantage: Works best when there are many line segments but few are actually visible. Disadvantage: the algorithm has to be used recursively. 6. What is meant by the term ‚Äúdouble buffering‚Äù and for what purpose is it used? Double buffering solves the problem of distortion caused when the frame buffer redisplays causing a partially drawn display. There are 2 frame buffers used, a front buffer that is displayed, and a back buffer that is available for constructing the next display. The buffers are then swopped and the new back buffer is cleared and starts drawing the next display. 7. What information is stored in: a. Frame buffer ‚Äì stores collective pixels of an image. b. Z-buffer ‚Äì Stores depth information as primitives are rasterized 8. Describe briefly the Liang-Barsky clipping algorithm Four points are assigned where the line intersects the extended sides of the window. It then orders the points corresponding to intersections needed for clipping. P1 is the L/H window limit, and p2 is the R/H window limit. Any negative values yield points on the line on the other side of p1 from p2. All values more than 1 correspond to points on the line past p2. Line segments between the points p1 and p2 inside the window are clipped for display. 9. In WebGL graphics pipeline, when a triangle is processed, the (x,y,z) coordinates of the vertices are interpolated across whole triangle to give coordinates of each fragment. Name 2 other things that may commonly be specified at the vertices and then interpolated across the triangle to give a value for each fragment Colours, normals, and texture coordinates. 10. Explain crossing, or odd-even test, with respect to a point p inside a polygon. Any ray emanating from point p that is inside the polygon will have an odd number of crossings before infinity Any ray emanating from point p that is outside the polygon will have an even number of crossings before infinity 11. Briefly describe the Winding test The winding test considers the polygon as a knot being wrapped around a point or line. It starts by traversing the edges of the polygon from any starting vertex and going around the edges in a particular direction until the starting point is reached. An arbitrary point is then considered, and the winding number for this point is the number of times it encircles the edges of the polygons. Count clockwise as positive and counter-clockwise as negative. 12. Transformations and viewing: 1. Differentiate between parallel and perspective projection Parallel: These views have a COP at infinity. Parallel views do not form realistic views of objects Perspective: These views have a finite COP. Perspective views are characterised by the diminution of size as the object is moved farther from the viewer. They form a realistic picture of the object 2. Why are projections produced by parallel and perspective viewing known as planar geometric projections? Both parallel and perspective views are known as planar projections because the projection surface is a plane and the projectors are lines. 3. Explain briefly the projection normalisation technique This technique converts all projections into simple orthogonal projections by distorting objects such that the orthogonal projection of distorted objects is the same as desired projection of original objects. The vertices are then transformed such that vertices within the specified view volume are transformed to vertices within the canonical view volume. 4. What are the advantages of the normalization transformation process? - both perspective and parallel views can be supported by the same pipeline. - The clipping process is simplified because sides of canonical view volume are aligned with coordinate axes 5. Shape of the viewing volume for an orthogonal projection is right parallelepiped. Discuss steps involved in the projection normalisation process for an orthographical projection 1. Perform translation to move centre of specified view volume to centre of canonical view volume 2. Scale sides of specified view volume such that they have a length of 2. 6. Draw a view frustum ‚Äì 3 important rectangular planes at their correct positions. 7. Transformations are often carried out using a homogenous coordinate representation. Why is this representation used? - All affine transformations can be represented using matrix multiplications - Uniform representation of all affine transformations make carrying out successive transformations far easier than in 3D space. - Less arithmetic is involved 8. Explain what is meant by non-uniform foreshortening In a perspective projection, the farther an object is from the viewer, the smaller it will appear. 9. Since clipping to a clip region that is a cube is so easy, graphics systems transform any scene with its clip window to make the clip window a cube a. Name this transformation technique b. Give one other advantage of using this transformation technique a) Projection normalisation b) Both perspective and parallel views can be supported by the same pipeline. 10. What is unique about affine transformations Affine transformations preserve the line during transformations. 11. Give 2 examples of affine transformations Rotation and translation 12. Consider the following transformations: A. Translation B. Rotation C. Uniform scaling For each, state if statements are True. May have none, one, or more than one. a. Position of an arbitrary vertex is always the same after transformation None b. Length of an arbitrary line segment is the same after transformation A and B c. Angle between 2 arbitrary vectors is the same after transformation d. Arbitrary parallel lines are still parallel after transformation A, B, and C 13. Synthetic coordinate reference frame is given by a VRP, VPN, and VUP. Using a diagram show how these quantities describe the location and orientation of the synthetic camera 14. Explain the term transformation as used in CG Transformations are changes made to an object to either change angle, size, or direction. 15. Explain why translation and rotation are known as rigid body transformations Translation and rotation cannot alter the volume or the shape of an object 16. Model-view transformation is the concatenation of 2 transformations. Name and describe these 2 Modelling transformation: Takes instances of objects in object coordinates and brings them into the world frame Viewing transformation: Transforms world coordinates into camera coordinates. 17. Hidden surface removal: 1. Hidden surface removal can be divided into 2 broad classes, state and explain each of these classes Object space: Attempts to order surfaces of objects in the scene such that rendering surfaces in a particular order provide the correct image Image space: Works as part of projection process and seeks to determine the relationship among object points on each projector 2. Compare and contrast the depth sort and Z-buffer hidden surface removal algorithms with respect to the following: a. Rasterization process b. Type of algorithm c. Hardware implementation d. Scenes that are difficult to render a) Depth Sort: Z-Buffer: As primitives are rasterized, it keeps track of the distance from COP to closest point on each projector already rendered b) Depth Sort: Object space Z-Buffer: Image space c) Depth Sort: Z-Buffer: Easy to implement. d) Depth Sort: Z-Buffer: 3. Draw a picture of a set of simple polygons that the depth sort algorithm cannot render without splitting the polygons 4. Differentiate between depth sort and Z-buffer algorithms for hidden surface removal Z-Buffer: Z-Buffer stores the depth of the object from the COP, whereas the depth sort does sorting to determine if there are any polygons whose z-extents overlap. 5. State whether the following about depth sort and Z-buffer are true or false, correct if false. a. Depth sort is image space algorithm False, object space algorithm b. Z-buffer is image space algorithm True c. Z-buffer does rasterization polygon by polygon True d. Depth sort considers depth of each fragment in a polygon corresponding to intersection of a polygon with a ray from COP True e. Z-buffer find scenes where polygon pierces another difficult to render False, depth sort f. Depth-sort find scenes where 3 or more polygons operate cyclically difficult to render True g. Z-buffer can only be implemented in software False, hardware or software h. Depth sort orders polygons True 6. Briefly describe the algorithm for removing back facing polygons, assume normal points out from visible side of the polygon Culling is used in situations where back faces cannot be seen. This reduces the work for the hidden surface removal algorithms by eliminating all back facing polygons before applying other hidden surface removal algorithms. 7. WebGL makes use of a Z-buffer a. What info is stored in the Z-buffer b. How does WebGL use this info for hidden surface removal? c. Give 2 advantages of using this approach to hidden surface removal a) Depth information b) WebGL uses this to determine if a fragment rasterized will have a greater depth than that in the Z-buffer. If so, it is discarded. c) Easy to implement in either hardware of software, and is compatible with pipeline architectures. Lighting and shading: 1. Phong reflection model a. Describe the 4 vectors used to calculate a colour for an arbitrary point p, illustrate with a figure b. In the specular term, there is a factor of (r.v)p. What does p refer to? What effect does varying the power p have? c. What is the term kala? What does ka refer to? How will decreasing ka affect the rendering of a surface? d. Is ka a property of the light or the surface? a) n ‚Äì normal at p v ‚Äì in direction from p to viewer or COP l ‚Äì In direction of a line from p to arbitrary point on light source r ‚Äì in direction of a perfectly reflected ray from I would take b) p is the shininess coefficient. Reflected light is concentrated in a narrower region centered on the angle of a perfect reflector. c) ka is the ambient reflection coefficient, decreasing this will decrease the amount of light reflection given by the surface. 2. Consider Gouraud and Phong shading models: a. What information about the object or polygon to be shaded is needed by both models? b. Explain how this information is used in the 2 shading models c. Which of the 2 models is more realistic, especially for highly curved surfaces? Explain 3. Can the standard WebGL pipeline easily handle light interactions from object to object? Explain 4. Explain what diffuse reflection is in the real world A tar road would be an example of a diffuse reflection due to the rough surface and the impression it reflects light in all directions. 5. State and explain Lambert‚Äôs law using a diagram Lambert‚Äôs law states we only see the vertical component of the incoming light 6. Using Lambert‚Äôs law, derive the equation for calculating approximations to diffuse reflection on a computer ùêºùêºd= ùëòùëòd ùêøùêød( ùíçùíç‚àô ùíèùíè) 7. Name 2 artifacts in computer graphics that may commonly be specified at vertices of a polygon and then interpolated across the polygon to give a value for each fragment within the polygon Colours, normals, and texture coordinates 8. Shading intensity at any given point p on a surface is, in general comprised of 3 contributions, each corresponding to a distinct physical phenomenon. List and describe these 3 phenomenon Ambient, specular, and diffuse light interactions 9. We have 3 choices where we do lighting calculations. In the application, vertex shader, or fragment shader a. Describe 3 steps required to implement lighting in a shader b. Explain difference between doing lighting calculation on a per-fragment as opposed to per-vertex basis a. Choose lighting model, Write the shader to implement the model, and finally transfer necessary data to shader b. When doing lighting on a per-fragment vases, we obtain highly smooth and realistic looking shadings. 10. In CG, we have local and global lighting models a. Differentiate between the 2 lighting models b. Give one limitation of the local lighting model c. Give one limitation of the global lighting model d. Name one global lighting model used in CG a) Local ‚Äì lighting is done independently on objects. Global ‚Äì Lighting is universal throughout the scene b) Cannot manage shadows or reflections on the objects c) Global models are incompatible with the pipeline architecture d) Ray tracing 11. Interactions between light and material can be classified into 3 groups, state and explain each Diffuse surface: Reflected light is scattered in all directions Specular surface: Appear shiny because most of the light that is reflected or scattered is in a narrow range of angles Translucent surface: Some light penetrates the surface and emerges from another location on the object 12. Name and describe the 4 basic light sources used in CG Ambient: Lights provide uniform illumination throughout the room or area Spot: Narrow range of angles through which light is emitted. Point: Light emits equally in all directions. Distant: All rays are parallel and replace the location of the light source with direction of the light 13. Shading: a. Name the 3 major shading techniques used in computer graphics b. Describe the computation process for each of these c. Discuss how objects shaded by different methods differ in appearance. a) Flat shading, Gouraud shading, and Phong shading b) Flat: Shading calculation only done once for each polygon and each point is assigned same shade Gouraud: Light calculation is done at each vertex using material properties and vectors n, v, and l Phong: Instead of interpolating vertex intensities, we interpolate normal across each polygon and an independent lighting calculation for each fragment is made. c) Flat: Each point is assigned the same shade Gouraud: The rasterizer interpolates a shade for each fragment Phong: Independent lighting calculation is made for each fragment. 14. In a simple CG lighting model, we assume specular reflection component ls=ksLscosaO a. What lighting effect does specular reflection component approximate b. What does the term ks represent c. What effect does increasing angle O have? d. What effect does increasing angle a have? a) the intensity of the specular light b) the specular reflection coefficient c) The light will be projected at an angle closer to the normal d) Reflected light is concentrated in a narrower region centered on the angle of a perfect reflector. Discrete Techniques: 1. Explain what is meant by reflection mapping and discuss briefly how this is implemented in WebGL Allows us to create images that heave the appearance of reflected materials without having to deflect rays An image of the environment is painted onto the surface as that surface is being rendered. Done in 2 passes: 1) Renders scene without reflecting object. Camera place at centre of mirror pointed in direction of the normal of mirror 2) Use the image to obtain shades to place on the mirror for second rendering. 2. Explain what is meant by texture mapping and discuss briefly how this is implemented in WebGL Uses an image to influence the colour of a fragment. The texture can be a digitised image or generated by a procedural texture generation method. Done by: 1) Form texture image and place in texture memory on GPU 2) Assign texture coordinates to each fragment 3) Apply texture to each fragment 3. Explain the problem of rendering translucent objects using the Z-buffer algorithm, and describe how the algorithm can be adapted to deal with this problem When rendering a translucent object before an opaque, the Z-buffer will not composite polygons correctly if an opaque object is rendered afterwards, behind it. The Z-buffer can be made read only when rendering the translucent polygons and depth information will be prevented from being updated when rendering translucent objects 4. Explain what is meant by bump (or normal) mapping and discuss briefly how this is implemented in computer graphics Bump mapping distorts the normal vectors during the shading process to make the surface appear to have small variations in shape. The technique varies the apparent shape of the surface by perturbing normal vectors as the surface is rendered, 5. We would like to create a realistic looking 3D CG scene of room using WebGL. There is a mirror and a window, garden is visible through window. a. Briefly describe a fairly simple and cheap way for us to create the window through which the garden can be seen b. Briefly describe a fairly simple and cheap way for to create the mirror a. Using the texture mapping to map an image of the outside to create the scene of the garden on the window b. Using the environment mapping to get an image of the room from the mirror to the normal of the mirror and map it to the mirror. 6. Alpha channel is the 4th colour in RGBA colour mode a. Explain main purpose for the alpha channel b. Explain how alpha channel is used for antialiasing c. Explain how the alpha channel is used in WebGL d. Explain how the alpha channel is used for blending a) The alpha channel gives the object the ability to be transparent, translucent, or opaque b) When a line is rendered, instead of colouring a whole pixel with the colour of the line, it passes through it. It adjusts the intensity of the colour, avoiding sharp contrasts. c) Enable blending, setup the desired source and destination factors, and the application program must use RGBA colours. d) The alpha value controls how much RGB is written into frame buffer 7. What mapping technique computes the surroundings visible as a reflected image on a shiny object? Environment mapping. 8. Explain 2 methods of implementing the mapping technique described above You could project the environment onto a sphere centered at the COP; or compute six projections that correspond to the six sides of a cube, using six virtual cameras, each pointing in a dirrent direction. 9. When texture mapping, aliasing errors occurring when mapping texture coordinates to a texel. Name and describe 2 strategies used in CG to deal with this. 1. Point sampling: Use value of texel closest to the texture coordinate output 2. Linear filtering: Use a weighted average for a group of texels in the neighborhood determined by point sampling. From vertices to fragments 1. Provide the pseudocode for DDA line rasterization algorithm, and explained how it is derived for (ix = x1; ix <= x2; ix++) { y += m; write_pixel(x, round(y), line_colour); } 2. Explain the 2 techniques to determine the process of filling the inside of a polygon with a colour or pattern Odd -even testing & Winding test ‚Äì see previous explanations 3. Describe briefly, with the use of diagrams, the Cohen-Sutherland line clipping algorithm The algorithm divides a 2D space into 9 regions, then efficiently determining the lines and portions of lines inside the given rectangular area. Possible cases of the line or line segment: o Completely inside given rectangle o Completely outside given rectangle o Partially inside the window.","libVersion":"0.2.3","langs":""}