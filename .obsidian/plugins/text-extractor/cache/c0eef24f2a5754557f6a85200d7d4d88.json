{"path":"UNISA/98906 - BSc Science in Computing/COS3751 - Techniques of Artificial Intelligence/Student Notes/Stuvia-3208171-cos3751-assignment-3-ans-2023.pdf","text":"COS3751 ASSIGNMENT 3 ANS 2023 written by GoldenKey www.stuvia.com Downloaded by: stuvia7feva | stuvia.7feva@simplelogin.com Distribution of this document is illegal Want to earn R13,625 per year? QUESTION 1 To convert the given FOL sentence to Conjunctive Normal Form (CNF), we need to follow these steps: Step 1: Move the quantifiers to the leftmost position: ∀x(∀y(¬P(y) ∨ Q(x, y)) ⇒ (¬∀yQ(y, x))) Step 2: Eliminate the implication (⇒) using the equivalence A ⇒ B ≡ ¬A ∨ B: ∀x(∀y(¬P(y) ∨ Q(x, y)) ⇒ (¬∀yQ(y, x))) is equivalent to ∀x(∀y(¬(¬P(y) ∨ Q(x, y)) ∨ (¬∀yQ(y, x)))) Step 3: Apply De Morgan's law to eliminate the negation inside: ∀x(∀y((P(y) ∧ ¬Q(x, y)) ∨ (∃y¬Q(y, x)))) Step 4: Distribute the quantifiers over the disjunction (∨): Stuvia.com - The study-notes marketplace Downloaded by: stuvia7feva | stuvia.7feva@simplelogin.com Distribution of this document is illegal Want to earn R13,625 per year? Stuvia.com - The study-notes marketplace ∀x∀y((P(y) ∧ ¬Q(x, y)) ∨ (∃y¬Q(y, x))) Step 5: Convert the sentence to Conjunctive Normal Form (CNF): ∀x∀y(P(y) ∨ ¬Q(x, y)) ∧ ∀x∀y(∃y¬Q(y, x)) Therefore, the given FOL sentence has been converted to CNF as: ∀x∀y(P(y) ∨ ¬Q(x, y)) ∧ ∀x∀y(∃y¬Q(y, x)) QUESTION 2 2.1) Vocabulary for the statements: - AI exam: AI_exam(x) - Passes exams: Passes_exams(x) - Wins the lottery: Wins_lottery(x) - Is happy: Is_happy(x) - Studies hard: Studies_hard(x) - Is lucky: Is_lucky(x) - Jack: jack 2.2) Translation of English sentences to FOL statements: 1. ∀x (AI_exam(x) ∧ Wins_lottery(x)) ⇒ Is_happy(x) 2. ∀x (Studies_hard(x) ∨ Is_lucky(x)) ⇒ Passes_exams(x) 3. ¬Studies_hard(jack) 4. Is_lucky(jack) 5. ∀x (Is_lucky(x) ⇒ Wins_lottery(x)) 2.3) Conversion of FOL statements to CNF: 1. ¬(AI_exam(x) ∧ Wins_lottery(x)) ∨ Is_happy(x) 2. ¬(Studies_hard(x) ∨ Is_lucky(x)) ∨ Passes_exams(x) 3. ¬Studies_hard(jack) 4. Is_lucky(jack) 5. ¬Is_lucky(x) ∨ Wins_lottery(x) Stuvia.com - The study-notes marketplace Downloaded by: stuvia7feva | stuvia.7feva@simplelogin.com Distribution of this document is illegal Want to earn R13,625 per year? Stuvia.com - The study-notes marketplace 2.4) Using resolution refutation to prove that Jack is happy: To prove that Jack is happy, we need to negate the statement and derive a contradiction. Negating the statement \"Jack is happy\": ¬Is_happy(jack) Applying resolution refutation to derive a contradiction: 1. From statement 1, we have: (AI_exam(jack) ∧ Wins_lottery(jack)) ⇒ Is_happy(jack) 2. From statement 3, we have: ¬Studies_hard(jack) 3. Combining statements 1 and 2, we have: (Wins_lottery(jack) ∧ (AI_exam(jack) ∧ ¬Studies_hard(jack))) ⇒ Is_happy(jack) 4. Simplifying and applying resolution on the negation of Is_happy(jack) and the derived statement above, we have: (¬Wins_lottery(jack) ∨ ¬AI_exam(jack) ∨ Studies_hard(jack)) ∧ ¬Is_happy(jack) 5. From statement 4, we have: Is_lucky(jack) 6. Combining statements 5 and the derived statement above, we have: (¬Wins_lottery(jack) ∨ ¬AI_exam(jack) ∨ Studies_hard(jack)) ∧ Is_lucky(jack) ∧ ¬Is_happy(jack) 7. From statement 5, we have: ¬Wins_lottery(jack) ∨ Wins_lottery(jack) 8. Applying resolution on statements 6 and 7, we have: Stuvia.com - The study-notes marketplace Downloaded by: stuvia7feva | stuvia.7feva@simplelogin.com Distribution of this document is illegal Want to earn R13,625 per year? Stuvia.com - The study-notes marketplace Downloaded by: stuvia7feva | stuvia.7feva@simplelogin.com Distribution of this document is illegal Want to earn R13,625 per year? Stuvia.com - The study-notes marketplace (¬AI_exam(jack) ∨ Studies_hard(jack)) ∧ ¬Is_happy(jack) 9. From statement 3, we have: Studies_hard(jack) 10. Applying resolution on statements 8 and 9, we have: ¬AI_exam(jack) ∧ ¬Is_happy(jack) 11. From statement 4, we have: Is_lucky(jack) 12. Applying resolution on statements 10 and 11, we have: Is_happy(jack) 13. From statement 4 and the derived statement above, we have: Is_happy(jack) ∧ ¬Is_happy(jack) 14. At this point, we have derived a contradiction, which means our negation of \"Jack is happy\" is false. Therefore, we can conclude that Jack is indeed happy. QUESTION 3 3.1) Conversion of the Boolean function to a decision tree: The Boolean function table is as follows: | A | B | C | f(A, B, C) | |---|---|---|-----------| | 0 | 0 | 0 | 0 | | 0 | 0 1 | 1 | | 0 | 1 | 0 | 0 | | 0 | 1 | 1 | 0 | | 1 | 0 | 0 | 0 | Stuvia.com - The study-notes marketplace Downloaded by: stuvia7feva | stuvia.7feva@simplelogin.com Distribution of this document is illegal Want to earn R13,625 per year? Stuvia.com - The study-notes marketplace | 1 | 0 | 1 | 1 | | 1 | 1 | 0 | 1 | | 1 | 1 | 1 | 0 | To construct the decision tree: 1. Starting at the top, we choose variable A as the root node. 2. For A = 0, we have the following remaining data points: {(0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1)}. 3. Looking at the remaining data points for A = 0, we see that for both B and C, there are separate values of f(A, B, C). Therefore, we choose variable B as the first split. 4. For B = 0, we have the following remaining data points: {(0, 0, 0), (0, 0, 1)}. 5. For B = 1, we have the following remaining data points: {(0, 1, 0), (0, 1, 1)}. 6. For both B and C, the corresponding values of f(A, B, C) are the same (0). Therefore, we can assign the value 0 at the leaf node for both cases. 7. Returning to step 3, we now consider the remaining data points for A = 1, which are: {(1, 0, 0), (1, 0, 1), (1, 1, 0), (1, 1, 1)}. 8. Looking at the remaining data points for A = 1, we see that for both B and C, there are separate values of f(A, B, C). Therefore, we choose variable B as the next split. 9. For B = 0, we have the following remaining data points: {(1, 0, 0), (1, 0, 1)}. 10. For B = 1, we have the following remaining data points: {(1, 1, 0), (1, 1, 1)}. 11. For B = 0, C is consistent with the value of f(A, B, C) (0). Therefore, we can assign the value 0 at the leaf node. Stuvia.com - The study-notes marketplace Downloaded by: stuvia7feva | stuvia.7feva@simplelogin.com Distribution of this document is illegal Want to earn R13,625 per year? Stuvia.com - The study-notes marketplace 12. For B = 1, C is consistent with the value of f(A, B, C) (1). Therefore, we can assign the value 1 at the leaf node. The constructed decision tree is as follows: ``` A / \\ 0 1 | | B B | | 0 0 | | C C | | 0 1 ``` 3.2) Importance of variable evaluation order when constructing a decision tree without gain values: When constructing a decision tree without the benefit of gain values, the order in which we evaluate the variables is important. The choice of the variable evaluation order influences the structure and accuracy of the decision tree. If different variable evaluation orders are used, it can lead to different decision trees with different performance. The order of variable evaluation affects the splitting process at each node in the decision tree. Each split is based on selecting the variable that maximizes or minimizes the impurity measure (e.g., entropy or Gini index) or uses a different criterion (e.g., information gain). The selected variable becomes the splitting criteria for that node. Different variable evaluation orders can lead to different information gains or impurity reductions, which can result in different decision trees. Moreover, different variable evaluation orders can affect Stuvia.com - The study-notes marketplace Downloaded by: stuvia7feva | stuvia.7feva@simplelogin.com Distribution of this document is illegal Want to earn R13,625 per year? Stuvia.com - The study-notes marketplace the depth and size of the decision tree and can impact its interpretability, complexity, and computational efficiency. Therefore, selecting the optimal variable evaluation order is crucial to achieve better decision tree performance in terms of accuracy, interpretability, and efficiency. This selection is typically based on statistical measures or heuristics that evaluate the significance of each variable in predicting the target variable. QUESTION 4 4.1 To determine the root node of the decision tree, we need to calculate the information gain for each attribute in the dataset. The attribute with the highest information gain will be chosen as the root node. First, let's calculate the entropy of the dataset based on the target variable 'Play?'. Looking at the table provided, we can see that there are 9 instances where the pensioner plays golf and 7 instances where the pensioner does not play golf. Total instances: 16 Positive instances: 9 (Play = Yes) Negative instances: 7 (Play = No) The ratio of positive examples (p) is 9/16 = 0.5625. The corresponding entropy (E) is 0.99 (taken from the entropy table). Next, we calculate the entropy for each attribute by considering their values. For the attribute 'Outlook': - Sunny: 6 instances (4 Yes, 2 No) - Overcast: 4 instances (4 Yes, 0 No) - Rain: 6 instances (5 Yes, 1 No) Calculating the entropy for each value: Stuvia.com - The study-notes marketplace Downloaded by: stuvia7feva | stuvia.7feva@simplelogin.com Distribution of this document is illegal Want to earn R13,625 per year? Stuvia.com - The study-notes marketplace Entropy(Sunny) = E[4, 2] = 0.88 Entropy(Overcast) = E[4, 0] = 0.00 Entropy(Rain) = E[5, 1] = 0.81 To calculate the remainder for the 'Outlook' attribute, we need to consider the weighted average of entropies for each value: Remainder(Outlook) = (6/16) * Entropy(Sunny) + (4/16) * Entropy(Overcast) + (6/16) * Entropy(Rain) = (6/16) * 0.88 + (4/16) * 0.00 + (6/16) * 0.81 = 0.33 + 0 + 0.30 = 0.63 Now, let's calculate the information gain for the 'Outlook' attribute: Information Gain(Outlook) = Entropy(Dataset) - Remainder(Outlook) = 0.99 - 0.63 = 0.36 Similarly, we need to calculate the information gain for the remaining attributes - 'Temperature' and 'Windy'. I'll provide the calculations below: For the attribute 'Temperature': - Hot: 6 instances (4 Yes, 2 No) - Mild: 6 instances (3 Yes, 3 No) - Cool: 4 instances (2 Yes, 2 No) Entropy(Hot) = E[4, 2] = 0.88 Entropy(Mild) = E[3, 3] = 0.99 Entropy(Cool) = E[2, 2] = 1.00 Remainder(Temperature) = (6/16) * Entropy(Hot) + (6/16) * Entropy(Mild) + (4/16) * Entropy(Cool) = (6/16) * 0.88 + (6/16) * 0.99 + (4/16) * 1.00 Stuvia.com - The study-notes marketplace Downloaded by: stuvia7feva | stuvia.7feva@simplelogin.com Distribution of this document is illegal Want to earn R13,625 per year? Stuvia.com - The study-notes marketplace = 0.33 + 0.37 + 0.25 = 0.95 Information Gain(Temperature) = Entropy(Dataset) - Remainder(Temperature) = 0.99 - 0.95 = 0.04 For the attribute 'Windy': - Yes: 8 instances (5 Yes, 3 No) - No: 8 instances (4 Yes, 4 No) Entropy(Yes) = E[5, 3] = 0.9544 (approximated from the entropy table) Entropy(No) = E[4, 4] = 1.00 Remainder(Windy) = (8/16) * Entropy(Yes) + (8/16) * Entropy(No) = (8/16) * 0.9544 + (8/16) * 1.00 = 0.4772 + 0.5 = 0.9772 Information Gain(Windy) = Entropy(Dataset) - Remainder(Windy) = 0.99 - 0.9772 = 0.0128 Based on the information gain calculations, the attribute 'Outlook' has the highest information gain of 0.36. Therefore, 'Outlook' becomes the root node of the decision tree. The complete decision tree can be constructed by further splitting the data based on the chosen root node ('Outlook') and calculating information gains for the remaining attributes. 4.2 To determine the complete decision tree, the next step would be to calculate the information gain for each attribute in the dataset. Information gain measures the amount of information provided by an attribute in sorting the data based on the target variable. Stuvia.com - The study-notes marketplace Downloaded by: stuvia7feva | stuvia.7feva@simplelogin.com Distribution of this document is illegal Want to earn R13,625 per year? Stuvia.com - The study-notes marketplace The steps to calculate information gain are as follows: 1. Calculate the initial entropy of the dataset based on the target variable (Play?). 2. Calculate the entropy of the dataset for each attribute by considering the values it can take. 3. Calculate the information gain for each attribute by subtracting the entropy of the dataset for that attribute from the initial entropy. 4. Choose the attribute with the highest information gain as the root node of the decision tree. 5. Repeat the above steps recursively for each branch of the tree until all the data has been classified. By doing this, we can construct the complete decision tree based on the given dataset and its corresponding weather conditions. 4.3 The significance of an attribute becoming the root node of the decision tree lies in its ability to provide the most information gain and effectively classify the data. The root node acts as the starting point of the decision tree and determines the first split or division in the data based on the attribute's values. The attribute with the highest information gain is chosen as the root node because it can best differentiate the instances in the dataset with respect to the target variable. In this specific dataset, let's consider the Outlook attribute. If Outlook becomes the root node of the decision tree, it means that it has the highest information gain compared to the other attributes. This indicates that the Outlook attribute provides the most information in determining whether the old age pensioner will play golf or not. By choosing Outlook as the root node, the decision tree will start by splitting the data based on the different Outlook values (Sunny, Overcast, Rain). This split will help in categorizing instances where the Outlook is Sunny, Overcast, or Rain, and further splitting based on other attributes. The significance of an attribute becoming the root node is that it sets the foundation for subsequent splits and helps streamline the classification process by efficiently dividing the data into distinct subsets based on its values. Powered by TCPDF (www.tcpdf.org) Downloaded by: stuvia7feva | stuvia.7feva@simplelogin.com Distribution of this document is illegal Want to earn R13,625 per year?","libVersion":"0.2.3","langs":""}