{"path":"Subjects/COS3751 - Techniques of Artificial Intelligence/Telegram Notes/Materials/COS3751_2021_203_ALL_B.pdf","text":"BAR CODE Deﬁne Tomorrow. university of south africa Tutorial Letter 203/1/2021 Techniques of Artiﬁcial Intelligence COS3751 Semester 1 School of Computing This tutorial letter contains solutions to the self assessment assignment 03 COS3751/203/1/2021 Solution SELF ASSESSMENT ASSIGNMENT Study material: Chapters 7, 8, 9, and 18. You may skip sections 6.6, 8.4, 9.3, 9.4. You only need to study 18.1, 18.2, and 18.3 Question 1 Exercise 7.10 c,f and g. Part c: Let P represent the sentence: P : (smoke ⇒ ﬁre) ⇒ (¬smoke ⇒ ¬ﬁre) We draw the truth table for P, where ‘s’ stands for ‘smoke’ and ‘f’ stands for ‘ﬁre’: s f s ⇒ f ¬s ¬f ¬s ⇒ ¬f P 0 0 1 1 1 1 1 0 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 1 0 0 1 1 Thus, P is satisﬁable since there is at least one model in which it is true. Part f: Let Q represent the sentence (Smoke ⇒ Fire) ⇒ ((Smoke ∧ Heat) ⇒ Fire). We draw the truth table for P, where ‘s’ stands for ‘smoke’, ‘f’ stands for ‘ﬁre’, and ‘h’ stands for ‘heat’: s f h s ⇒ f s ∧ h (s ∧ h) ⇒ f Q 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 1 0 1 0 1 1 0 1 1 1 0 1 1 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 Thus Q is valid, since it is true in all models. Part g: Let R represent the sentence: big ∨ dumb ∨ (big ⇒ dumb). Then: R ≡ big ∨ dumb ∨ (¬big ∨ dumb) ≡ big ∨ ¬big ∨ dumb. R is valid because (big ∨ ¬big) is valid and so is (big ∨ ¬big ∨ dumb) Thus R is satisﬁable since there is at least one model in which it is true. Question 2 Exercise 7.21 a and b. 2 COS3751/203/1/2021 Part a: Suppose that [1, 1] is the lower-left corner square of the grid. The squares adjacent to [1, 1] are: [1, 2], [2, 2] and [2, 1]. The assertion that there are exactly two mines adjacent to [1, 1] is a dis- junction with three disjuncts, each one saying that two of the neighbours are true and the third neighbour is false. This is represented by the following sentence: (X12 ∧ X22 ∧ ¬X21) ∨ (X12 ∧ ¬X22 ∧ X21) ∨ (¬X12 ∧ X22 ∧ X21) (1) Conversion to CNF: sentence 1 is in Disjunctive Normal Form (DNF). In other words it is a disjunc- tion of conjunctions of literals. It can be converted to a CNF sentence as follows. We temporarily ignore the third term of the sentence, and combine the ﬁrst two terms, namely (X12 ∧ X22 ∧ ¬X21) and (X12 ∧ ¬X22 ∧ X21), by using the law of distribution of ∨ over ∧. This produces: (X12 ∨ X12) ∧ (X12 ∨ ¬X22) ∧ (X12 ∨ X21)∧ (X22 ∨ X12) ∧ (X22 ∨ ¬X22) ∧ (X22 ∨ X21)∧ (¬X21 ∨ X12) ∧ (¬X21 ∨ ¬X22) ∧ (¬X21 ∨ X21) (2) The ﬁrst term of 2 is equivalent to X12 . Because this sentence is in CNF form, any clause containing X12 can be eliminated. Tautologies, namely (X22 ∨ ¬X22) and (¬X21 ∨ X21), can also be eliminated. Therefore it reduces to: X12 ∧ (X22 ∨ X21) ∧ (¬X21 ∨ ¬X22) (3) Now, combining sentence 3 with the third term of 1, by using the law of distribution of ∨ over ∧, produces: (X12 ∨ ¬X12) ∧ (¬X12 ∨ X22 ∨ X21)∧ (¬X12 ∨ ¬X22 ∨ ¬X21)∧ (X22 ∨ X12) ∧ (X22 ∨ X21)∧ (X22 ∨ ¬X22 ∨ ¬X21)∧ (X21 ∨ X12) ∧ (X21 ∨ X22)∧ (X21 ∨ ¬X21 ∨ ¬X22) (4) Removing tautologies in sentence 4, such as (X22 ∨ ¬X22 ∨ ¬X21) , produces: (¬X12 ∨ X22 ∨ X21) ∧ (¬X12 ∨ ¬X22 ∨ ¬X21)∧ (X22 ∨ X12) ∧ (X22 ∨ X21)∧ (X21 ∨ X12) ∧ (X21 ∨ X22) (5) Because sentence 5 is in CNF form, the ﬁrst term can be removed due to the presence of the fourth term. The last term can be removed because it is identical to the fourth. This results in the ﬁnal sentence: (¬X12 ∨ ¬X22 ∨ ¬X21) ∧ (X22 ∨ X12)∧ (X22 ∨ X21) ∧ (X21 ∨ X12) (6) This sentence expresses the following idea. The ﬁrst term ensures that at most two of the three neighbours are true. The last three terms ensure that at least two of the three neighbours are true. Part b: The easiest way to reply to this question is to construct a DNF sentence as in Part a, and convert it into a CNF sentence. There will be (n k) = n! k!(n−k) disjuncts, each one expressing the fact that k of n symbols are true and the others false. 3 Question 3 Convert the following propositional sentence to conjunctive normal form (CNF): ¬[A ⇔ (C ∨ E)) ⇒ ¬((B ∧ F ) ⇒ ¬D)] ¬[(A ⇔ (C ∨ E)) ⇒ ¬((B ∧ F ) ⇒ ¬D)] ≡ ¬[¬(A ⇔ (C ∨ E)) ∨ ¬((B ∧ F ) ⇒ ¬D)] (eliminate main implication) ≡ ¬¬(A ⇔ (C ∨ E)) ∧ ¬¬((B ∧ F ) ⇒ ¬D) (De Morgan) ≡ (A ⇔ (C ∨ E)) ∧ ((B ∧ F ) ⇒ ¬D) (eliminate double negation) ≡ (A ⇒ (C ∨ E)) ∧ ((C ∨ E) ⇒ A) ∧ ((B ∧ F ) ⇒ ¬D) (eliminate biconditional) ≡ (¬A ∨ (C ∨ E)) ∧ (¬(C ∨ E) ∨ A) ∧ (¬(B ∧ F ) ∨ ¬D) (eliminate all conditionals) ≡ (¬A ∨ (C ∨ E)) ∧ ((¬C ∧ ¬E) ∨ A) ∧ ((¬B ∨ ¬F ) ∨ ¬D) (De Morgan) ≡ (¬A ∨ C ∨ E) ∧ ((¬C ∨ A) ∧ (¬E ∨ A)) ∧ (¬B ∨ ¬F ∨ ¬D) (Distributivity of ∨ over ∧) ≡ (¬A ∨ C ∨ E) ∧ (¬C ∨ A) ∧ (¬E ∨ A) ∧ (¬B ∨ ¬F ∨ ¬D) The ﬁnal sentence is in Conjunctive Normal Form (CNF) and it is composed of 4 clauses. Question 4 Convert the following ﬁrst-order logic sentence to clause form: (∀y)[(∀x)[P(x)] ⇒ (∃x)[(∀z)[Q(x, z)]∨)(∀z)[R(x, y, z)]]] • Eliminate implication: (∀y)[¬(∀x)[P(x)] ∨ (∃x)[(∀z)[Q(x, z)] ∨ (∀z)[R(x, y, z)]]] • Move ¬ inwards (∀y)[(∃x)[¬P(x)] ∨ (∃x)[(∀z)[Q(x, z)] ∨ (∀z)[R(x, y, z)]]] • Standardize variables: The variable name x is used in the scope of two different quantiﬁers, so we change the name of one of them to avoid confusion when we drop the quantiﬁers. The same applies for the variable name z. (∀y)[(∃x)[¬P(x)] ∨ (∃u)[(∀z)[Q(u, z)] ∨ (∀v )[R(u, y, v )]]] Note that the scope of (∀z) is the pred- icate Q(u, z), and the scope of (∀v ) is the predicate R(u, y, v ). On the other hand, the scope of (∀y ) is the whole formula. • Skolemize (∀y)[[¬P(f (y))] ∨ [(∀z)[Q(g(y), z)] ∨ (∀v )[R(g(y), y, v )]]] Here f and g are Skolem functions, and their arguments are all the universally quantiﬁed variables in whose scope the existential quantiﬁer appears. In this case, (∃u) appears only in the scope of (∀y). Note that the same variable u appears twice in the sentence, so we replace each occurrence of u with g(y). • Drop universal quantiﬁers: At this point, all remaining variables are universally quantiﬁed. Moreover, the sentence is equivalent to one in which all the universal quantiﬁers have been moved to the left: (∀y)(∀z)(∀v )[[¬P(f (y))] ∨ [Q(g(y), z)] ∨ [R(g(y), y, v )]] We can therefore drop the universal quantiﬁers:¬P(f (y)) ∨ Q(g(y), z) ∨ R(g(y ), y, v ) 4 COS3751/203/1/2021 The sentence is now in CNF and it has only one conjunct (one clause) consisting of a disjunction of three literals. Question 5 Consider the following English statements: • Anyone passing his history exam and winning the lottery is happy. • Anyone who studies or is lucky can pass his exams. • John did not study. • John is lucky. • Anyone who is lucky wins the lottery. (5.1) Translate the above English sentences to First Order Logic (FOL) statements. The sentences are translated as follows in FOL: 1. (∀x)[(pass(x, history) ∧ win(x, lottery)) ⇒ happy (x)] 2. (∀y)(∀z)[(study(y) ∨ lucky(y )) ⇒ pass(y, z)] 3. ¬study(john) 4. lucky(john) 5. (∀w)[lucky (w) ⇒ win(w, lottery)] It is important to standardize the variables (to use different variable names) to avoid confusion when dropping the universal quantiﬁers. (5.2) Convert the FOL statements obtained in 5.1 into clause form. The statements are converted as follows to clause form: 1. ¬pass(x, history) ∨ ¬win(x, lottery) ∨ happy (x) 2. ¬study(y) ∨ pass(y, z) 3. ¬lucky(y) ∨ pass(y, z) 4. ¬study(john) 5. lucky(john) 6. ¬lucky(w) ∨ win(w, lottery) Note that universal quantiﬁers have been dropped because all variables were univer- sally quantiﬁed. Skolem functions are introduced only to remove existential quantiﬁers. (See section 9.5 of R&N.) 5 (5.3) Use resolution refutation to prove that John is happy. In order to use resolution refutation, we negate the goal, convert the negated goal to clause form if necessary, and add the resulting clause(s) to the set of premises (sentences 1 to 6 in Part 2. above). Here the goal is: happy (john). Therefore the negation of the goal is: ¬happy (john). We then resolve the premises together with the negated goal until the empty clause (Nil) is generated. 1. ¬pass(x, history) ∨ ¬win(x, lottery) ∨ happy (x) assumption 2. ¬study(y) ∨ pass(y , z) assumption 3. ¬lucky(y) ∨ pass(y, z) assumption 4. ¬study(john) assumption 5. lucky(john) assumption 6. ¬lucky(w) ∨ win(w, lottery) assumption 7. ¬happy(john) negation of goal 8. ¬win(x, lottery) ∨ happy(x) ∨ ¬lucky (x) 1&3, {x/y}, {history/z} 9. ¬win(john, lottery) ∨ happy(john) 5&8, {john/x} 10. ¬win(john, lottery) 9&7 11. win(john, lottery) 5&6, {john/w} 12. Nil 10&11 We have shown that the negation of the goal together with the premises produce a contradiction (empty clause). Therefore the goal happy(john) is true, which translates to ‘John is happy’. 6 COS3751/203/1/2021 x1 x2 x3 f(x1, x2, x3) 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 1 1 1 Table 1: Boolean function table Question 6 (6.1) Convert the Boolean function in table 1 into a decision tree: No information gain values were given, so it becomes a matter of picking the sequence in which the variables are to be evaluated. The next step would be to simplify the decision tree by consolidating equivalent leaf nodes. Variable order x1, x2, x3: x1 x2 x3 1 0 0 1 0 x3 1 0 1 1 1 0 x2 x3 1 0 0 1 0 x3 0 0 1 1 1 1 This can be simpliﬁed to: 7 x1 x2 x3 1 0 0 1 0 1 1 0 x2 x3 1 0 0 1 0 x3 0 0 1 1 1 1 Variable order x1, x3, x2: x1 x3 1 0 x2 0 0 1 1 1 0 x3 x2 1 0 0 1 0 x2 0 0 1 1 1 1 Variable order x2, x1, x3: x2 x3 x2 1 0 0 1 0 x2 1 0 0 1 1 0 x3 1 00 x2 0 0 1 1 1 1 Variable order x2, x3, x1: 8 COS3751/203/1/2021 x2 x3 1 0 0 1 0 x3 x1 1 0 0 1 0 1 1 1 Variable order x3, x2, x1: x3 x2 1 0 x1 1 0 0 1 1 0 x2 0 0 1 1 1 Variable order x3, x1, x2: x3 x1 1 0 x2 1 0 0 1 1 0 x1 x2 0 0 1 1 0 x2 0 0 1 1 1 1 9 (6.2) When we construct a decision tree without the beneﬁt of gain values, the order in which we evaluate the variables is important. Why? It may be possible to consolidate leaf nodes with similar values to produce a smaller, more compact tree. Question 7 The National Credit Act introduced in South Africa in 2007 places more responsibility on a bank to determine whether the loan applicant will be able to afford it. Blue Bank has a table of information on 14 loan applications they have received in the past. (7.1) Use the information in this table to construct a decision tree that will assist the bank in determining the RISK associated with a new loan application. No. Credit history Debt Collateral Income RISK 1 Bad High No < R15k High 2 unknown High No R15k - R35k High 3 unknown Low No R15k - R35k Medium 4 unknown Low No < R15k High 5 unknown Low No > R35k Low 6 unknown Low Yes > R35k Low 7 Bad Low No < R15k High 8 Bad Low Yes > R35k Medium 9 Good Low No > R35k Low 10 Good High Yes > R35k Low 11 Good High No < R15k High 12 Good High No R15k - R35k Medium 13 Good High No > R35k Low 14 Bad High No R15k - R35k High Table 2: Risk information table In order to determine the information gain of any different attributes of a certain collec- tion of data, the calculation of entropy is important – the notion of information gain is deﬁned in terms of entropy. Entropy can be described as a measure of impurity of an arbitrary collection of exam- ples. Russell & Norvig describe entropy as a measure of the uncertainty of a random variable, and mention that the acquisition of information corresponds to a reduction in entropy. Namely a random variable with only one value has no uncertainty and thus its entropy is deﬁned as zero; thus we gain no information by observing its value. For any attribute A, if no information can be gained regarding the decision from at- tribute A, then the entropy of attribute A is equal to 0. If all the members of a class are split equally by any attribute A, then the entropy of A is 1. For any collection S the maximum entropy value is 1. Formally, the entropy of a random variable V with value vk , each with probability P(vk ) 10 COS3751/203/1/2021 is deﬁned as: H(V ) = n∑ k=1 −P(vk )log2 1 P(vk ) = − n∑ k=1 P(vk )log2 1 P(vk ) (7) Maybe a more comprehensible way of deﬁning the entropy of any collection S is as follows: Entropy(S) = − c∑ i=1 pilog2(pi) (8) This corresponds to what Russell and Norvig refer to as information content. If the possible answers vi have probabilities P(vi) then the information content I of the actual answer is given by: I(P(v1), ... , P(vn)) = n∑ i=1 −P(vi)log2P(vi) (9) You may see the negative as a way of making the values of pilog2(pi) positive because all pi < 1, i.e. each represents a probability of the sample set. In what follows we will use the ID3 algorithm to develop our decision tree. Our collection of 14 examples S, has four meaningful attributes (Credit History, Debt, Collateral, Income), and a 3-wise classiﬁcation (LOW, MEDIUM, HIGH). Since there are no repeat values in the No. attribute it plays no role in the classiﬁcation and can be ignored (although we will show the No. attribute in further tables so that we know which examples we are working with). For purposes of clarity in the formulae, we will shorten the attribute labels and values as follows: • Credit History = CH • Debt = D • Collateral = C • Income = I • BAD = B • UNKNOWN = U • GOOD = G • HIGH = H • LOW = L • <R15K = 15K • R15K-R35K = 15K-35K • >R35K = 35K 11 • MEDIUM = M We will also not be repeating calculations at every step: we show the full set of calcu- lations for the ﬁrst level of the tree, after that we will only show the ﬁnal result of the calculation. Summarise the example set, S = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14}, as: S = [5L, 3M, 6H] We start by calculating the Entropy for our set of examples: Entropy(S) = 3∑ i=1 −pilog2pi = −( 5 14)log2( 5 14 ) − ( 3 14 )log2( 3 14) − ( 6 14)log2( 6 14 ) = −( 5 14)(log25 − log214) − ( 3 14 )(log23 − log214) − ( 6 14 )(log26 − log214) = −( 5 14) ( log5 log2 − log14 log2 ) − ( 3 14) ( log3 log2 − log14 log2 ) − ( 6 14) ( log6 log2 − log14 log2 ) = 0.531 + 0.476 + 0.524 = 1.531(max = 1.585) We have to determine the information gain (IG) of the different attributes in order to select the best choice for the root node. The information gain measures the expected reduction of entropy – the higher the IG, the higher the expectation of reduction of entropy. In what follows we calculate the information gain for each of the four attributes. Credit History: Values (CH ) = B, G, U S = [5L, 3M, 6H] SB ← [0L, 1M, 3H] SG ← [3L, 1M, 1H] SU ← [2L, 1M, 2H] Entropy(SB) = −(0 4 )log2( 0 4) − ( 1 4)log2(1 4 ) − (3 4 )log2( 3 4) = 0.811 Entropy(SG) = −( 3 5)log2(3 5 ) − (1 5 )log2( 1 5) − ( 1 5)log2(1 5 ) = 1.371 Entropy(SU) = −( 2 5 )log2( 2 5) − (1 5 )log2( 1 5) − ( 2 5)log2(2 5 ) = 1.522 12 COS3751/203/1/2021 Gain(S, CH) = Entropy(S) − ∑ v ∈Values(A) |Sv | |S| Entropy(Sv ) = Entropy(S) − ( 4 14 )Entropy(SB) − ( 5 14 )Entropy(SG) − ( 5 14 )Entropy(SU) = 1.531 − ( 4 14 )0.811 − ( 5 14 )1.371 − ( 5 14)1.522 = 0.266 Debt: Values(D) = L, H S = [5L, 3M, 6H] SL ← [3L, 2M, 2H] SH ← [2L, 1M, 4H] Entropy(SL) = −( 3 7)log2(3 7 ) − ( 2 7 )log2( 2 7) − (2 7 )log2( 2 7 ) = 1.557 Entropy(SH) = −( 2 7)log2(2 7 ) − (1 7 )log2( 1 7) − ( 4 7)log2(4 7 ) = 1.379 Gain(S, D) = Entropy(S) − ( 7 14)Entropy(SL) − ( 7 14 )Entropy(SH) = 1.531 − ( 7 14 )1.557 − ( 7 14)1.379 = 0.063 Collateral: Values (C) = Y,N S = [5L, 3M, 6H] SY ← [2L, 1M, 0H] SN ← [3L, 2M, 6H] Entropy(SY ) = −( 2 3)log2(2 3 ) − (1 3 )log2( 1 3) − ( 0 3)log2(0 3 ) = 0.918 Entropy(SN) = −( 3 11)log2( 3 11 ) − ( 2 11 )log2( 2 11) − ( 6 11)log2( 6 11 ) = 1.435 13 Gain(S, C) = Entropy(S) − ( 3 14 )Entropy(SY ) − ( 11 14)Entropy(SN) = 1.531 − ( 3 14)0.918 − ( 11 14)1.435 = 0.207 Income: Values (I ) = 15 , 15-35, 35 S = [5L, 3M, 6H] S15 ← [0L, 0M, 4H] S15−35 ← [0L, 2M, 2H] S35 ← [5L, 1M, 0H] Entropy(S15) = −(0 4 )log2( 0 4) − ( 0 4)log2(0 4 ) − (4 4 )log2( 4 4) = 0 Entropy(S15−35) = −( 0 4)log2(0 4 ) − (2 4 )log2( 2 4) − ( 2 4)log2(2 4 ) = 1 Entropy(S35) = (5 6 )log2( 5 6) − (1 6 )log2( 1 6 ) − ( 0 6)log2(0 6 ) = 0.650 Gain(S, I) = Entropy(S) − ( 4 14)Entropy(S15) − ( 4 14 )Entropy(S15−35) − ( 6 14 )Entropy(S35) = 1.531 − ( 4 14 )0 − ( 4 14 )1 − ( 6 14)0.650 = 0.967 Attribute Income provides the highest information gain (0.967), i.e. the best prediction for our target attribute, Risk. Income becomes the root node of our decision tree. Income < 15K 15K − 35K > 35K The ID3 algorithm now performs a recursion with the three subsets of our exam- ples, based on the three possible values of the Income attribute (<R15K, R15K-R35K, >R35K). The ﬁrst subset, corresponding to Income = {< R15K }, is: 14 COS3751/203/1/2021 No. Credit History Debt Collateral Income RISK 1 BAD HIGH NO < R15k HIGH 4 UNKNOWN LOW NO < R15k HIGH 7 BAD LOW NO < R15k HIGH 11 GOOD HIGH NO < R15k HIGH We notice that the target attribute is the same for all four of our examples, i.e. all four examples produce the same target value, HIGH. We have our ﬁrst leaf node, labelled HIGH. Income HIGH < 15K 15K − 35K > 35K The second subset, corresponding to Income = {R15K − R35K }, is: No. Credit History Debt Collateral Income RISK 2 UNKNOWN HIGH NO R15K-R35K HIGH 3 UNKNOWN LOW NO R15K-R35K MEDIUM 12 GOOD HIGH NO R15K-R35K MEDIUM 14 BAD HIGH NO R15K-R35K HIGH Our subset collection of 4 examples S, has three meaningful attributes (Credit History, Debt, Collateral), and a binary classiﬁcation (MEDIUM, HIGH): S15K −35K = {2, 3, 12, 14} = [2M, 2H]. We start by calculating the Entropy for this subset: Entropy(S15K −35K ) = 2∑ i=1 −pilog2pi = −(2 4 )log2( 2 4) − ( 2 4)log2(2 4 ) = 0.5 + 0.5 = 1 Calculate the information gain for the attributes of the subset (entropy for each attribute is calculated in the same fashion as above). Credit History: Gain(S15K −35K , CH) = Entropy(S15K −35K ) − ( 1 4)Entropy(SB) − (1 4 )Entropy(SG) − (2 4 )Entropy(SU) = 1 − (1 4 )0 − ( 1 4 )0 − ( 2 4)1 = 0.5 15 Debt: Gain(S15K −35K , D) = Entropy(S15K −35K ) − (1 4 )Entropy(SL) − ( 3 4)Entropy(SH) = 1 − ( 1 4)0 − ( 3 4)0.918 = 0.312 Collateral: Gain(S15K −35K , C) = Entropy(S15K −35K ) − ( 0 4)Entropy(SY ) − (4 4 )Entropy(SN) = 1 − (0 4 )0 − (4 4 )1 = 0 Attribute Credit History provides the highest Information Gain in this subset (0.5). Thus it becomes our next decision node. Income HIGH < 15K Credit History Bad Good Unknown 15K − 35K > 35K The third subset, corresponding to Income = {> R35k}, is then used. No. Credit History Debt Collateral Income RISK 5 UNKNOWN LOW NO >R35K LOW 6 UNKNOWN LOW YES >R35K LOW 8 BAD LOW YES >R35K MEDIUM 9 GOOD LOW NO >R35K LOW 10 GOOD HIGH YES >R35K LOW 13 GOOD HIGH NO >R35K LOW Our subset collection of 6 examples S, has three meaningful attributes (Credit History, Debt, Collateral), and a binary classiﬁcation (LOW, MEDIUM): S35K = {5, 6, 8, 9, 10, 13} = [5L, 1M] 16 COS3751/203/1/2021 Start by calculating the Entropy for this subset: Entropy(S35K ) = 2∑ i=1 −pi log2pi = −( 5 6 ) log2 (5 6 ) − (1 6 ) log2 ( 1 6) = 0.65 We now proceed to calculate the information gain for each of the three attributes in the subset: Credit History: Values(CH) = B, G, U S35K = [5L, 1M] SB ← [0L, 1M] SG ← [3L, 0M] SU ← [2L, 0M] Entropy(SB) = −( 0 1)log2(0 1 ) − (1 1 )log2( 1 1) = 0 Entropy(SG) = −( 3 3)log2(3 3 ) − (0 3 )log2( 0 3) = 0 Entropy(SU) = −( 2 2)log2(2 2 ) − (0 2 )log2( 0 2) = = 0 Gain(S35K , CH) = Entropy(S35K ) − ( 1 6)Entropy(SB) − (3 6 )Entropy(SG) − (2 6 )Entropy(SH) = 0.65 − ( 1 6)0 − (3 6 )0 − (2 6 )0 = 0.65 Debt: Values(D) = L, H S35K = [5L, 1M] SL ← [5L, 1M] SH ← [2L, 0M] 17 Entropy(SL) = −( 3 4)log2( 3 4 ) − ( 1 4)log2(1 4 ) = 0.811 Entropy(SG) = −(2 2)log2( 2 2 ) − ( 0 2)log2(0 2 ) = 0 Gain(S35K , D) = Entropy(S35K ) − (4 6 )Entropy(SL) − ( 2 6)Entropy(SH) = 0.65 − (4 6 )0.811 − ( 2 6)0 = 0.109 Collateral: Values(D) = Y , N S35K = [5L, 1M] SY ← [2L, 1M] SN ← [3L, 0M] Entropy(SL) = −(2 3 )log2( 2 3) − ( 1 3)log2(1 3 ) = 0.918 Entropy(SG) = −(3 3 )log2( 3 3) − ( 0 3)log2(0 3 ) = 0 Gain(S35K , D) = Entropy(S35K ) − (3 6 )Entropy(SY ) − ( 3 6)Entropy(SN) = 0.65 − ( 3 6 )0.918 − (3 6 )0 = 0.191 In summary: Gain(S35K , CH) = 0.65 Gain(S35K , D) = 0.109 Gain(S35K , Collateral) = 0.191 Attribute Credit History provides the highest Information Gain in this subset and be- comes the decision node on this branch of the decision tree. 18 COS3751/203/1/2021 Income HIGH < 15K Credit History Bad Good Unknown 15K − 35K Credit History Bad Good Unknown > 35K The algorithm continues with recursion to the next level. The subset of Credit History that corresponds to the Income = R15K-R35K and Credit History = Bad provides a single example (14). No. Credit History Debt Collateral Income RISK 14 BAD HIGH NO R15K-R35K HIGH We have a single example, hence another leaf node: HIGH. Income HIGH < 15K Credit History HIGH Bad Good Unknown 15K − 35K Credit History Bad Good Unknown > 35K The subset of Credit History that corresponds to the Income = R15K-R35K and Credit History = Good is also a single example (12). No. Credit History Debt Collateral Income RISK 14 BAD HIGH NO R15K-R35K HIGH We have a single example, hence another leaf node: MEDIUM. 19 Income HIGH < 15K Credit History HIGH Bad MEDIUM Good Unknown 15K − 35K Credit History Bad Good Unknown > 35K The subset of Credit History that corresponds to the Income = R15K-R35K and Credit History = Unknown has two examples (2,3). No. Credit History Debt Collateral Income RISK 2 UNKNOWN HIGH NO R15K-R35K HIGH 3 UNKNOWN LOW NO R15K-R35K MEDIUM Our subset collection of 2 examples S, has two meaningful attributes (Debt, Collat- eral), and a binary classiﬁcation (MEDIUM, HIGH shortened for clarity to M,H): SUnknown = {2, 3} = [1M, 1H]. We again calculate the entropy for the subset: Entropy(S) = −( 1 2)log2(1 2 ) − ( 1 2)log2( 1 2) = 1 Debt Values(D) = L, H SUNKNOWN = [1L, 1M] SL ← [2L, 1M] SH ← [3L, 0M] 20 COS3751/203/1/2021 Entropy(SL) = −( 1 1)log2( 1 1 ) − ( 0 1)log2(0 1 ) = 0 Entropy(SH) = −(0 1)log2( 0 1 ) − ( 1 1)log2(1 1 ) = 0 Gain(SUNKNOWN, D) = Entropy(SUNKNOWN) − (1 2)Entropy(SL) − ( 1 2)Entropy(SH) = 1 − ( 1 2)0.918 − (1 2 )0 = 1 Collateral Values(D) = N SUNKNOWN = [1M, 1H] SN ← [1M, 1H] Entropy(SN) = −( 1 2)log2(1 2 ) − (1 2 )log2( 1 2) = 1 Gain(SUNKNOWN, C) = Entropy(SUNKNOWN) − ( 2 2)Entropy(SN) = 1 − (2 2 )1 = 0 In summary: Gain(SUNKNOWN, D) = 1 Gain(SUNKNOWN, Collateral) = 0 Debt gives us perfect information gain, and thus becomes the next decision node. 21 Income HIGH < 15K Credit History HIGH Bad MEDIUM Good DEBT HIGH LOW Unknown 15K − 35K Credit History Bad Good Unknown > 35K We return to credit history (we ﬁrst ﬁnish all the nodes on the same level). The subset of Credit History that corresponds to the Income = ¿R35K and Credit History = Bad contains only 1 example (8). No. Credit History Debt Collateral Income RISK 8 BAD LOW YES >R35K MEDIUM We thus have another leaf node: MEDIUM. Income HIGH < 15K Credit History HIGH Bad MEDIUM Good DEBT HIGH LOW Unknown 15K − 35K Credit History MEDIUM Bad Good Unknown > 35K 22 COS3751/203/1/2021 The subset of Credit History that corresponds to Income = >R35K and Credit History = Good provides three examples (9,10,13): No. Credit History Debt Collateral Income RISK 9 GOOD LOW NO >R35K LOW 10 GOOD HIGH YES >R35K LOW 13 GOOD HIGH NO >R35K LOW Our subset collection of 3 examples S, has two meaningful attributes (Debt, Collat- eral), and a single classiﬁcation (Low). Hence we again have a leaf node LOW. Income HIGH < 15K Credit History HIGH Bad MEDIUM Good DEBT HIGH LOW Unknown 15K − 35K Credit History MEDIUM Bad LOW Good Unknown > 35K The subset of Credit History that corresponds to the Income = >R35K and Credit History = Unknown is provides 2 examples (5,6). No. Credit History Debt Collateral Income RISK 5 UNKNOWN LOW NO >R35K LOW 6 UNKNOWN LOW YES >R35K LOW Our subset collection of 2 examples S, has two meaningful attributes (Debt, Collat- eral), and a single classiﬁcation (Low). Hence we again have a leaf node LOW. 23 Income HIGH < 15K Credit History HIGH Bad MEDIUM Good DEBT HIGH LOW Unknown 15K − 35K Credit History MEDIUM Bad LOW Good LOW Unknown > 35K We can now do the last set of calculations. With Debt = High we have only one exam- ple left (2), which makes this a leaf node. No. Credit History Debt Collateral Income RISK 2 UNKNOWN HIGH NO R15K-R35K HIGH Similarly, when Debt = Low we have one example left (3). This creates the ﬁnal leaf node. No. Credit History Debt Collateral Income RISK 3 UNKNOWN LOW NO R15K-R35K MEDIUM These last two steps complete our decision tree (note that Collateral plays no role in the decision based on this decision tree). 24 COS3751/203/1/2021 Income HIGH < 15K Credit History HIGH Bad MEDIUM Good DEBT HIGH HIGH MEDIUM LOW Unknown 15K − 35K Credit History MEDIUM Bad LOW Good LOW Unknown > 35K Copyright ©UNISA 2021 25","libVersion":"0.2.3","langs":""}