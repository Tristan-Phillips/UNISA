{"path":"Subjects/INF3720 - Human-Computer Interaction II/Unsorted/Assignment 3/Case study – Academic library.pdf","text":"Usability Testing of an Academic Library Web Site: A Case Study by Brenda Battleson, Austin Booth, and Jane Weintrop Usability testing is an invaluable tool for evaluating the effectiveness and ease of use of academic library Web sites. This article reviews major usability principles and explores the application of formal usability testing to an existing site at the University at Buffalo libraries. Brenda Battleson is Technical Services Librarian, University at Buffalo, Acquisitions Department, 134 Lockwood Library, Buffalo, New York 14260 ,blb@acsu.buffalo.edu.; Austin Booth is Humanities Librarian, University at Buffalo, Reference Department, 521 Lockwood Library, Buffalo, New York 14260 ,habooth@acsu.buffalo.edu.; Jane Weintrop is Reference Librarian, University at Buffalo, Business and Government Documents Department, Lockwood Library, Buffalo, New York 14260 ,ulcjanew@acsu.buffalo.edu.. L ibrary Web sites are evolving into information gateways, unlocking access to library resources and services as well as electronic indexes and databases, primary research materials, and the Internet at large. Although in- tended to ease the process of information access, the staggering amount of informa- tion available via these sites can produce a kind of ªinformation overloadº that can bewilder, confuse, and even discourage users. There is a fundamental need for ªusabilityº in library Web sites and us- ability testing is an invaluable tool for evaluating interfaces in terms of their ef- fectiveness and ease of use. Designed and implemented properly, usability testing becomes an integral part of the Web site's development and evolution. This article reviews the concept of usability and ex- plores the application of usability testing to library Web sites through a case study of usability testing at the University at Buffalo libraries. BACKGROUND ON USABILITY ENGINEERING AND HUMAN- COMPUTER INTERACTION Despite the explosion of information technology and a growing dependence on computers in all facets of society, only recently have ªuser needsº become part of software and interface development. Dur- ing the 1990s software companies began to address their customer needs seriously and to design ªusabilityº into their prod- ucts rather than focusing solely on func- tionality. 1 Usability engineering involves studying and designing ªease of useº into a product. Its major component, Human- Computer Interaction (HCI), the study of how people interact with computer tech- nology and how to make this interaction effective, provides the theoretical basis for applying usability concepts to soft- ware applications and computer inter- faces. Web sites are as well suited as soft- ware to the precepts of HCI; usability engineering can, therefore, impart a sys- temic approach to Web design. HCI dic- tates that interfaces should meet the fol- lowing goals: (1) provide task support, that is, enable users to achieve their goals and meet their particular needs; (2) be usable by making it possible for users to work easily, efficiently and with few errors; and (3) provide an aes- thetically pleasant interface design. A usable or ªuser-centeredº interface is one that effectively meets these goals. Task support is crucial to user-cen- tered interface design. Elements of task support include knowing who a li- brary's users are, what they wish to accomplish using the site, and what sup- port they need to complete their tasks successfully. First and foremost, an in- terface's primary users must be identi- fied. When a Web site caters to a variety of users performing different tasks, the needs of the defined primary user group take priority. Designers must ask, ªWho are our users and what tasks do we want them to be able to accomplish using this site?º Clearly defined priorities in terms of the ªwhoº and ªwhatº of a Web site are the bases for assessing whether or not the site provides sufficient task sup- port. ªClearly defined priorities in terms of the `who' and `what' of a Web site are the bases for assessing whether or not the site provides sufficient task support.º 188 The Journal of Academic Librarianship, Volume 27, Number 3, pages 188 ±198 The International Standards Organiza- tion (ISO) defines usability as ªthe extent to which a product can be used by spec- ified users to achieve specified goals with effectiveness, efficiency, and satisfaction in a specified context of use.º 2 Applied to Web technology, this means simply that, for the tasks it is designed to support, an interface must be easy to learn, remem- ber, and use, and must lead to few errors. The most effective means of assessing a site's usability is with usability testing, which will be discussed in more detail later in this article. To a somewhat lesser degree, HCI conveys the need for aesthetics in inter- face design. Given the graphics-intensive nature of the present information environ- ment, especially with regard to the World Wide Web, it is important that aesthetics be considered in usability testing since they can affect user satisfaction, as well as the ways in which users navigate a given interface. Aesthetics are, indeed, listed among the goals of HCI, but the impact of aesthetic variables on usability is ex- tremely difficult to measure. While a for- mal mechanism for evaluating aesthetics was not designed into the case study de- scribed below, the comments of the test participants regarding the interface's vi- sual appeal were certainly a factor in the overall evaluation of the site's usability. LITERATURE REVIEW Usability Engineering and HCI There are numerous guides and hand- books available concerning the design and maintenance of Web sites. While these publications vary in degree and so- phistication they tend to discuss the how- to's of Web site design solely from the site developer perspective. Few of these handbooks discuss usability engineering, HCI, or the concept of testing to see if users' needs are being met. This absence is not surprising since very few of these publications even acknowledge user needs as a factor in the Web site devel- opment process. Jakob Nielsen's Usability Engineering is the most comprehensive and practical discussion of usability engineering and testing, covering the usability engineering life cycle from product conceptualization to design and evaluation. 3 Usability test- ing, a component of usability engineering involving the testing of an interface/sys- tem to determine whether or not it meets the precepts of HCI, is specifically tar- geted in Jeffrey Rubin's The Handbook of Usability Testing and Joseph Dumas and Janice Redish's A Practical Guide to Us- ability Testing.4 Both of these books out- line detailed guidelines for conducting us- ability tests, but The Handbook of Usability Testing provides especially use- ful information about steps to take before undertaking usability testing, including developing a purpose statement, task list, and test plan. While these books give useful over- views of usability engineering and testing, they focus primarily on hardware and software development. Applying HCI and usability testing to the graphical user in- terface (GUI) has been a relatively recent concept, spurred on by the popularity of the Web. Nielsen addresses this in De- signing Web Usability, as well as in a series of Useit.com: Usable Information Technology ªAlertboxº articles. 5 Jared Spool's book, Web Site Usability docu- ments a test of nine popular Web sites for usability. 6 Spool's work is of interest to library Web site developers since it was one of the first studies to evaluate the usability of sites designed, at least in part, to support information retrieval. A useful and informative source for applying us- ability testing to Web-based interfaces is Alison Head's Design Wise, which de- scribes the application of task support, usability, and aesthetics to Web site de- sign as well as the importance of integrat- ing these HCI concepts into the evalua- tion of existing sites. 7 Usability Testing Usability testing can be divided into three categories: inquiry, inspection, and formal usability testing. While the first and last involve real users, the second does not. (The term ªusability testingº en- compasses numerous methods of evaluating site usability. This general term should not be confused with ªformal usability testingº which, as described below, is a specific usability test method.) The most comprehensive and practical outline of usability test methodology is James Hom's The Usability Methods Toolbox.8 In the context of Web site and interface assessment, inquiry involves requesting information about a particular site from the users. Methods of inquiry include fo- cus groups, interviews, questionnaires, and surveys. Interviews and focus groups are structured methods of inquiry which are used to gather information about us- ers' experiences and preferences. While surveys and questionnaires may also be used to gather such information, inter- views and focus groups allow for more interaction with the users and for imme- diate answers to questions raised during the interview or focus group. In addition, focus groups consist of multiple users whose interaction may raise additional is- sues of interest. Interviews and focus groups are generally conducted at early stages of product development, while sur- veys and questionnaires are generally used later in the product's life cycle. With inspection methods like heuristic evaluation and cognitive walkthrough, a site's designers and information special- ists serve as testers and subjects, often putting themselves in the place of the user to perform various tasks using the site. Unlike inquiry and formal usability test- ing, these forms of assessment do not enlist the participation of actual users. In cognitive walkthrough, experts attempt to accomplish typical user tasks with a given interface. Heuristic evaluation involves usability experts checking elements of an interface against a checklist of heuristics, or design principles. 9 While evaluations based on inspection are relatively inex- pensive to conduct, they are less useful in identifying usability errors than tests with actual users. 10 ªIn formal usability testing, users are observed using a site, or prototype, to perform given tasks or achieve a set of defined goals.º In formal usability testing, users are observed using a site, or prototype, to perform given tasks or achieve a set of defined goals. This method involves em- ploying experiments to gather specific in- formation about a design. Formal usabil- ity testing was first used in experimental psychology, and originally involved the gathering and analysis of large quantities of data. Today, however, it is more con- cerned with interpretation and rapid, use- ful results rather than amassing large bod- ies of quantitative data. Dumas and Redish discuss five facets of formal us- ability testing: (1) the goal is to improve the usability of the interface; (2) testers represent real users; (3) testers perform real tasks; (4) user behavior and commen- tary are observed and recorded; and (5) data are analyzed to recognize problems and suggest solutions. 11 Applied to Web May 2001 189 site interfaces, this test method not only results in a more usable site, but also allows the site design team to function more efficiently, since it replaces opinion with user-centered data. In terms of test design and implemen- tation, Rubin's Handbook of Usability Testing and Hom's Usability Methods Toolbox provide the most detailed discus- sion of the actual methodology of formal usability testing. A formal usability test typically involves introducing the inter- face, asking the user to attempt a set of tasks, observing the human-computer in- teraction that takes place and evaluating the results to identify design problems exposed by the interaction. There is a consensus in the literature that usability testing be an iterative pro- cess, preferably one built into a Web site's initial design. HCI concepts lay out interface design considerations, while proper testing techniques advocate testing for these design concepts as part of a Web site's development process. Site develop- ers should test for usability, redesign, and test againÐthese steps create a cycle for maintaining, evaluating and continually improving a site. 12 Usability Testing in Libraries Although libraries have a long history of studying and responding to user behav- ior through end-user studies, the relative lack of literature on the topic reveals that libraries are only beginning to apply us- ability testing to their Web sites. Karen Eliasen, Jill McKinstry, and Beth Mabel Fraser tested students' ability to navigate online menus to select correctly databases from the library home page at the Univer- sity of Washington. This test used a rela- tively simple testing mechanism where participants were observed as they at- tempted tasks using three alternate ver- sions of a low-fidelity prototype. The testers planned to do further testing using think-aloud protocol, timed-task analysis, and a post-test debriefing. 13 Janet Chis- man, Karen Diller, and Sharon Walbridge conducted formal usability testing of the Washington State University Libraries' online public access catalog and index- es. 14 A recent article by Jerilyn Veldof, Michael Prasse, and Victoria Mills de- scribed case studies and outlined much useful information about usability testing in general. The testing methods described include heuristic and formal usability test- ing with paper and online prototypes at the University of Arizona and OCLC. 15 The case study that follows is a de- scription of the use of HCI concepts to design and implement formal usability testing of an academic library's Web site. This study is notable in that the testing procedures were extremely effective, uti- lizing techniques that were neither expen- sive nor technically difficult to imple- ment. More importantly, this testing was applied to a Web-based environment typ- ical of most libraries, that is, the Web site was not new, but had evolved over a number of years. ACASE STUDY OF FORMAL USABILITY TESTING IN AN ACADEMIC LIBRARY At the onset of testing, in May 1999, the University at Buffalo Libraries Web site was well beyond the design stage. It had been launched four years prior and over the course of that period had expanded and changed dramatically in scope and purpose. Because this site was becoming an integral part of the library and univer- sity communities, the Web manager called for an ªoverall assessmentº of the site as the initial step in addressing the need for design enhancements. To this end, a committee comprised of librarians working in different areas of the univer- sity libraries (including a library school student) was charged to develop a com- prehensive evaluation program that would apply some method of testing. This test- ing would allow for the identification of problem areas and provide the data upon which to base design changes. Upon a literature review of testing methods and principles, it was decided that an ªoverall assessment of the library Web siteº was far too broad in scope. The Web manager and committee developed a more refined statement; ªDetermining whether or not the libraries' users could effectively use the Web site to perform specific tasksº clarified the purpose for testing and dictated that the test method would be formal usability testingÐ watching real users perform real tasks. Setting the Goals To design and implement an effective test, the usability testing committee needed to apply HCI design concepts to the library Web site. A framework was developed based on the key elements of task support: who are the users; what must they accomplish; and what support should the site provide? 16 Users of a library Web site typically comprise a very heterogeneous popula- tion. The University of Buffalo Libraries centralized Web site services a large com- prehensive public university with a stu- dent population of approximately 23,500 enrolled in a full range of undergraduate, graduate, and professional programs. Be- sides students, users of the libraries' Web site include faculty and staff, as well as members of the general public. Because a single usability test measuring task sup- port for all of these user groups was nei- ther practical nor feasible, the testing committee had to target the primary users of the libraries' Web site. Ideally, this user group would have been defined dur- ing the site's initial design stages. In fact, if there is no clear statement defining a target group, the Web manager, library administrators, or whatever decision- making body administers the Web site must identify the site's primary users be- fore any usability testing can proceed. In this case study, the Web manager defined the primary users as ªundergraduate stu- dents with little or no experience using the libraries' site.º The libraries' Web site accommodates numerous tasks. Among other things, un- dergraduates can search the library cata- log; access course readings and numerous electronic indexes; ask reference ques- tions via e-mail; renew books; and look up hours of operation for the various cam- pus libraries. For the purposes of this study, the committee and Web manager focused on tasks that would be key to evaluating whether or not the site was ªworking for the users.º What primary tasks could undergraduates expect to ac- complish using the site and what kind of support must the interface provide so that they might effectively complete those tasks? To ensure a user-centered ap- proach, site functionality was defined in terms of what the user needed to do, rather than all of the possible tasks the site could support, and ªconducting library re- searchº was defined as the most important activity for which the Web site should provide this task support. Thus, the goal of the usability test was set: to determine how effectively the li- braries' Web site ªworkedº when used for library research by undergraduates with little or no experience using the site. To further address this, the process of library research was broken into the broad tasks that typical undergraduates would be ex- pected to complete: c Using the site to identify an item/title that is part of the Libraries' collec- tions; 190 The Journal of Academic Librarianship c Using the site to locate the most ap- propriate resource for finding journal articles on a specific topic; and c Using the site to find an appropriate starting point for researching a topic without necessarily knowing the for- mat or sources of information. Designing the Test With the goal clearly stated and the test method identified, specific test questions representative of the broad tasks outlined above could be developed. Again, formal usability testing involves observing users as they perform given tasks or achieve a set of defined goals using a site. The challenge lay in presenting these tasks to the users in a way in which testers could learn from their responses and subsequent actions. The number of test questions devel- oped was limited to 11. In researching other usability tests, 17 the committee found that most were comprised of only 10 to 12 questions. This number was deemed sufficient since properly designed questions would provide the information needed without becoming tiresome to the students being tested. The committee sought questions that would be represen- tative of the broader tasks outlined above and still reflect real questions that an un- dergraduate might be faced with. To re- duce potential tester bias, the work of devising the questions was spread among the members of the committee, with each suggesting a number of test questions ad- dressing each task. All questions were then reviewed, selected and edited by the committee as a whole. Developing the questions presented a significant challenge. They had to be worded in a way that would test the site's design rather than the student's library skills. Also, there was concern that the multi-linked, cross-referenced structure of Web design would provide too many op- tions as students addressed the test's as- signed tasks, which would, in turn, com- plicate attempts to measure and identify the site's major usability problems. Thus, for this initial test, questions with only one or two ªcorrectº answers were select- ed. 18 Developing such questions was not an easy chore, given the size and scope of the libraries' Web site and its multiple internal and external links. Committee members tested each of the questions, try- ing every possible approach and identify- ing as many ways as possible to find each ªcorrectº answer. Questions with too many potentially ªcorrectº answers were thrown out while those with more readily apparent ªappropriateº and ªinappropri- ateº approaches were retained. By quickly determining whether or not a student could effectively navigate the site to com- plete a task, the committee was able to reduce not only the amount of time spent on each question but also the potential for fatigue and confusion. Finally, the committee considered the order in which the test questions would be presented. To determine whether or not the site was easy to learn and easy to remember, a number of questions were designed to build on tasks learned earlier in the test. The questions as they were presented were: 1. Do the University Libraries own the book Alias Grace? 2. Is the journal Northeast Anthropol- ogy available in the UB Libraries? 3. Can you find a journal article on gos- pel music? 4. How would you find a journal article on soap operas? 5. I am interested in investing in what are referred to as ªcallable securi- ties.º Where can I find recent articles about them? (You need to see the full article.) 6. Use the database SocioFile to look for an article about nursing homes and mental illness. What would you have to do to obtain this article? 7. How would you go about finding the author of this quotation: ªAnd we meet, with champagne and a chicken, at last?º 8. Assume you are taking a class in a subject you know very little about, for example, psychology, literature, environmental science, architecture or film. If you were assigned a re- search paper, how would you find information resources on that sub- ject? 9. Find an encyclopedia article about French wine. 10. Obtain background information about the ªethnic cleansingº that has taken place in Kosovo. 11. Does the Libraries' Web site have a guide to doing research in computer science? The same questions are organized below in terms of the broad tasks they were designed to test. Each question is fol- lowed by the user action deemed most appropriate for successfully completing the task: Task 1: Identify an item/title that is part of the Libraries' collections; c Question 1 3 link to library catalog for a given book c Question 2 3 link to library catalog for a given journal c Question 6 3 link to library catalog for a journal title given in a citation. Task 2: Locate the most appropriate re- source for finding journal articles on a topic; c Question 3 3 link to subject-specific or general index to full-text articles c Question 4 3 link to a general index to full-text articles c Question 5 3 link to a subject-spe- cific index to full-text articles c Question 6 3 link to a title index to citations Task 3: Find an appropriate starting point for research on a topic; c Question 7 3 link to ªReference Sourcesº [a listing of electronic refer- ence resources organized by subject] c Question 8 3 link to ªQuick Startº page [a listing of basic tools designed for novice researchers.] c Question 9 3 link to ªReference Sourcesº or ªQuick Startº c Question 10 3 link to ªQuick Startº c Question 11 3 link to ªNeed More?º [a series of subject specific pathfind- ers.] After the set of test questions was fi- nalized, the committee devised a method of presenting the questions to users and compiling the test results that would not only give insight into how effectively the Web site actually worked, but that was also within the committee's budget and level of expertise. Because the test em- phasized identifying gross usability prob- lems rather than making a detailed anal- ysis of the site, videotaping, eye tracking, and other elaborate techniques for gather- ing data were deemed unnecessary. In- stead, the committee utilized ªthink-aloud protocol,º which involves the user articu- May 2001 191 lating his/her thought processes and opin- ions while directly interacting with tech- nology to accomplish a given task. 19 Nielsen states that think-aloud protocol ªmay be the single most valuable usabil- ity engineering method.... One gets a very direct understanding of what parts of the [interface/user] dialog cause the most problems, because the thinking aloud method shows how users interpret each individual interface item.º 20 Given the scope of the University at Buffalo Librar- ies study and the wide support of this technique in the literature, think-aloud protocol was the obvious choice. Keith Instone refers to formal usability testing simply as ªwatch and learn.º 21 The ªwatchingº phase involves direct ob- servation of the users being tested. In the study two testers observed each partici- pant, but only one, the moderator, inter- acted with the student. The other, the scribe, kept a written log of what was taking place during the test. To promote consistency, the moderator worked from a script that suggested what and how much to say to the student. The script contained an introduction, during which time the moderator obtained consent and other in- formation about the student (experience with the site, year enrolled, etc.). As the test procedure was outlined, the modera- tor tried to impress upon students that the committee was testing the Web site, not them. It was also explained that there were no ªrightº or ªwrongº answers and that students would need to describe aloud what they were doing and why. For each question, the script stipulated the ideal response, the minimum require- ments for a satisfactory response, when to intervene and prompt the student to move on to the next question and when to end the test. For example, in Question 1, ªDo the libraries own the book Alias Grace?º the script stated that the ideal response would be for the user to select ªLibraries Catalogº from the main screen and then to search for the title. The moderator was instructed to end the test when the user reached the ªresults screenº in the cata- log. The script indicated how many at- tempts the student should make at suc- cessfully navigating from one page to the next. It also described when to let the subjects depend entirely on the site for direction and what instructions to give when intervention was necessary. Again, with Question 1, the moderator was di- rected to intervene ªwhen the user makes three incorrect selections from the main screen without reaching the catalog.º In this case the moderator was to clarify the term ªLibrary Catalogº if needed and then observe the results. Although the script was not designed to be used verbatim, it did suggest the wording of appropriate interaction, such as ªwhat do you under- stand by the phrase. . . º or ªI have learned enough from this questionÐlet's move on to the next.º The scribe recorded the user's move- ment through the Web site as well as his/her comments. A pre-printed log spe- cific to each question aided this process. The design of the log enabled the scribe to check off quickly the sequence of selec- tions made by the user and still have time to write down any additional observations or comments. Upon completing the actual test, stu- dents were asked to fill out a post-test evaluation asking for comments and opin- ions on both the Web site and the test. As soon as each student left the room, testers independently completed their own post- test evaluations, for which they reflected on what they had observed and com- mented on the obvious problems of the Web site, as well as features that appeared to work well. These comments proved to be an integral part of the ªlearningº phase of the watch and learn process. ªWith careful planning and design, implementing the tests were inexpensive, straightforward, and informative.º Conducting the Test With careful planning and design, im- plementing the tests were inexpensive, straightforward, and informative. Eleven tests were conducted, which is more than Nielsen advocates as necessary when test- ing a homogeneous group of users. (This is discussed below in depth.) To keep the time commitment to a minimum, each pair of testers was drawn from a pool of six committee members. Not only did this distribute the workload, but also reduced tester bias by allowing seven different people to observe the users and analyze the usability of the site. Testing required the use of a standard personal computer, with a Web browser and Internet access. The only limitation on the setting was that it be private enough so that the students would be comfortable talking aloud. Testers thought the privacy of an office would be intimidating and instead, chose a hands-on classroom. Volunteers were so- licited by instructors of a few sections of undergraduate English 101 classes, who handed out fliers and encouraged partici- pation. Although individual tests were conducted by appointment, it should be noted that despite agreed-upon meeting times, no-shows were inevitable. To fur- ther encourage participation and to help convey the importance of the test, partic- ipants were therefore compensated with gift certificates to a local music/video/ book store. Conducting the test was a relatively straightforward exercise. The committee tested the study's design by running two mock-tests using volunteers. This ªtesting the testº was well worth the effort be- cause it resulted in the addition of instruc- tions that greatly improved the effective- ness of the assessment process for both students and testers. For instance, results of the mock-test suggested that the stu- dents be instructed to read each question out loud, an exercise that actually encour- aged the ªthink aloudº process. In addi- tion, testers discovered that, without spe- cifically instructing students to return to the home page before beginning each question, they would start navigating from wherever they were on the Web site, thus masking what we were trying to learn. The lack of this simple instruction, ªclick `home' before beginning the next question,º would have seriously skewed the final test results. Evaluating the Results Just as the HCI element of ªtask sup- portº served as the framework for design- ing the test, ªmeasures of usabilityº were used to evaluate the results. Did students find that the Web site was easy to learn, easy to remember, pleasant to use, and caused few errors? 22 Much of this analy- sis was, of course, based on comments made by users during the ªthink aloud process.º However, as each student at- tempted to perform specific tasks using the site, testers based additional assess- ment on a number of observations: how quickly did the user work? Did choices seem obvious? How carefully did stu- dents read the information on the screens? Was there an increasing level of certainty and success as the test progressed? 192 The Journal of Academic Librarianship Testers carefully noted body language, fa- cial expressions, and reaction times in ad- dition to recording each user's comments. Both the scribe and the moderator, in their immediate post-test evaluations, assessed all observations and comments noted in the test log. These evaluations were then collected and reviewed by the larger com- mittee. Assessing the site with regard to the first two usability measuresÐis it easy to learn and easy to remember?Ðwas heavily based on observation and proved difficult to measure in certain respects. In a few instances, as students performed the assigned tasks, there was not always a clear distinction between a response re- sulting from ªlearningº and one that was simply a ªguess.º Also, even though a homogeneous user group was tested, in- dividual students obviously had different levels of technical proficiency and vary- ing learning abilities. Yet, the nature of the study allowed for these individual dif- ferences while quickly revealing the ma- jor usability problems with the site. Ascertaining whether or not students found the site ªpleasant to useº was chal- lenging given the subjective nature of such variables as aesthetics and the idio- syncrasies of individual users. The ideal method of measuring this element would have been to ask each student to complete a post-test questionnaire designed to mea- sure satisfaction with the site. Because the committee was as interested in the appli- cation of usability testing to the site as in the actual usability of the site, the com- mittee chose instead to design the post- test survey to measure satisfaction with the test process. Fortunately, tester obser- vations and user comments compiled dur- ing the test process were more than suf- ficient in revealing the extent of user satisfaction with the site. Of course, not all users are as ªtalkativeº or ªexpressiveº as the ones we tested. A post-test ques- tionnaire designed to measure satisfaction would have been useful and will be incor- porated into future test design. Assessing the fourth usability mea- sureÐªdoes the site cause errors?Ðwas a much more objective process, since for each question, there were pre-defined ªidealº and ªsatisfactoryº responses against which testers could measure per- formance. Using the pre-printed log, testers tracked a user's navigation through the site and tallied the number of attempts made when moving from one page to the next. These quantifiable measures of out- come in addition to observations relating the ease with which each student worked, provided more than just anecdotal evi- dence of usability. Because the students were encouraged to talk aloud as they were working, testers saw not only what they did and how easily they worked, but learned why they made the specific choices they did. Testers developed an almost immediate understanding of what site features were ineffective as well as why those features did not work. As test- ing progressed and different students en- countered the same problem areas, the committee was confident that these us- ability tests would successfully identify the major usability problems of the site. TEST RESULTS Main Screen Formal usability testing focused on those main screen links directly related to library research: c Online Resources (links to electronic information products to which the Li- brary subscribes); c UB Libraries Catalog (links to the on- line public access catalog); c Web Search (links to various aids for identifying information on the site and on the Internet); and c Need Help (links to guides for starting research). Emphasis was placed on assessing the students' experiences with the ªUB Li- braries Catalogº and ªOnline Resourcesº links, since ªWeb Searchº and ªNeed Helpº were viewed as secondary tools for supporting library research. However, the test revealed surprising data on the latter two links. From the main screen (see Fig- ure 1), the ªWeb Searchº and ªNeed Helpº links both failed the ªeasy to learnº criterion, creating enough confusion with users that they were rarely selected. Ac- cording to comments, students errone- ously assumed that the ªWeb Searchº link led only to Web search engines and the Internet, when, in fact, it included links to site-specific search features designed to help users. Also, while there was little direct feedback as to why ªNeed Helpº was not regularly selected, it was apparent that when this link was chosen, students were not at all satisfied with the informa- tion obtained. Testing clearly revealed a need to address usability problems of the main screen with regard to these two links. Students easily identified the link for the ªLibraries Catalogº and for tasks that involved identifying an item in the li- brary's collection, the main page met us- ability criteria. For the non-catalog re- search tasks (finding journal articles, researching a topic), the ideal choice was the ªOnline Resourcesº link, yet most stu- dents initially selected ªLibraries Cata- log.º There was obvious confusion with terminology 23 as well as a clear misun- derstanding of what the term ªOnline Re- sourcesº implied, so the screen initially failed the ªcauses few errorsº measure. Students quickly realized the error and upon returning to the main screen, usually selected ªOnline Resourcesº as the appro- priate choice. User comments indicated that, although they did not clearly under- stand the term ªOnline Resources,º most students selected it on their second at- tempts because they were able to elimi- nate the other choices as obviously inap- propriate. In short, they learned that ªLibraries Catalogº was inappropriate and used process of elimination to select ªOnline Resources.º Once they under- stood the distinction between ªLibraries Catalogº and ªOnline Resources,º the main screen was easy to use, easy to re- member and few errors were made. Second-level Screens The ªLibraries Catalogº link leads to a secondary page listing three options for connecting to the online catalog: one link allows users to connect to a Web-based graphical user interface, whereas the other two links provide access to the text ver- sion (see Figure 2). While users wishing to access the catalog were successful in connecting via the preferred Web-inter- face link, the process seemed unnecessar- ily unpleasant. The text-heavy presenta- tion of the connection options resulted in confusion and hesitation among the users and certainly detracted from the usability of the page. Students may have had difficulty in selecting the ªOnline Resourcesº link from the main screen, but they did find it to be an appropriate choice once they accessed the actual page (see Figure 3). Deciphering this page was another matter entirely, however. This page was very usable in some areas with regard to li- brary research tasks, but it proved to be surprisingly problematic in others. When looking for journal articles on a topic that could be easily associated with an academic discipline (test examples were gospel music and callable securi- May 2001 193 ties), the ªOnline Resourcesº page met all usability criteria. Students easily identi- fied the ªDatabases by Subjectº link as the correct choice and used it repeatedly to successfully complete a number of tasks. However, in the Question 6, which required that users find an article from the named database SocioFile, they did not find the ªOnline Resourcesº page easy to use since most were unable to readily identify the most appropriate choice, ªDa- tabases by Title.º Interestingly, students who did find the link to SocioFile often did so through indirect, often ªcreativeº routes. For finding articles on general topics (e.g., soap operas), the ªOnline Re- sourcesº page failed all usability criteria. Students did not identify ªQuick Start,º which was the only link that would have easily led them to such articles. Because they were unwilling to select this link, even as a second or third choice, students failed to learn that this link was useful. Indeed, observations of facial expressions confirmed that, when attempting to re- search a general topic, the site was not very ªpleasant to use.º The ªOnline Resourcesº page was not usable as a starting point for re- search. In addition to the users' unwill- ingness to select ªQuick Start,º they did not readily identify ªReference Re- sourcesº as a logical link to begin ex- ploration of a topic. Although not al- ways appropriate, ªDatabases by Subjectº was selected most often, while virtually every other link on the ªOnline Resourcesº screen was ignored. Usability Testing: What We Learned A great deal of time, effort and money had been invested in the ªOnline Re- sourcesº region of the library's Web site. That testing revealed a relative ineffec- tiveness of the presentation of informa- tion in this area was significant and sur- prising. Overall, it is fair to state that the usability problems discussed above would have not been considered, much less iden- tified, had formal usability testing of the Web site not been undertaken. Testers' observations and the comments of the stu- dents participating in the test were invalu- able in revealing where and why the site failed and helped evaluators to identify and prioritize the gross usability problems to be addressed. It is not an exaggeration to say that all parties involved were as impressed by the effectiveness of usabil- ity testing as they were by the results. Although the usability test supported the literature with regard to test design and implementation, the committee was surprised to find that the test experience also supported the use of a relatively small test group. The committee had ini- tially been skeptical of Nielsen's idea that when dealing with a homogeneous user group, watching as few as five users can identify a high percentage of the most critical errors: The most striking truth. . . is that zero users give zero insights.... As soon as you collect data from a single test user, your insights shoot up and you have already learned al- most a third of all there is to know about the usability of the design.... As you add more and more users, you learn less and less be- cause you will keep seeing the same things again and again.... After the fifth user, you are wasting your time by observing the same findings repeatedly but not learning much new.º 24 Nielsen outlines a mathematical formula developed with Tom Landauer, which supports his use of relatively small usabil- ity test groups: Figure 1 Main Page of the UB Libraries Web Site 194 The Journal of Academic Librarianship . . . the number of usability problems found in a usability test with n users is N(1-(1-L)n), where N is the total number of usability problems in the design and L is the propor- tion of usability problems discovered while testing a single user. The typical value of L is 31%, averaged across a large number of projects we studied. Plotting the curve for L531% gives the following result:25 As Nielsen stresses, this formula is only valid with a homogeneous group of users: ªYou need to test additional users when a Website has several highly distinct groups of users. The formula only holds for com- parable users who will be using the site in fairly similar ways.º 26 Even with a homo- geneous test group, however, Nielsen fur- ther challenges his own formula: ªThe curve (see Figure 4) clearly shows that you need to test with at least 15 users to discover all the usability problems in the design. So why do I recommend testing with a much smaller number of users? The main reason is that it is better to distrib- ute your budget for user testing across many small tests instead of blowing everything on a single, elaborate study.... º 27 As novice, cautious planners, the com- mittee tested 11 users only to find that Nielsen was indeed correct. The students in our test group committed the same mis- takes, with the most obvious errors and major usability problems apparent after the first few tests. Nielsen states that ª[e]laborate usability tests are a waste of resources. The best results come from testing no more than 5 users and running as many small tests as you can afford.º 28 Because there was very little new infor- mation obtained from our last few tests, they were essentially unnecessary and possibly wasteful, since the time and money invested may have been better spent on assessing the design changes we hoped to recommend as a result of this test. ªThe test process revealed a great deal about the site without being terribly complicated or expensive.º The test process revealed a great deal about the site without being terribly com- plicated or expensive. Careful test design was crucial however. In addition to the scripting instructions, the wording and or- der of the questions were key to obtaining accurate and useful data about the site as well as contributing to the efficient imple- mentation of the test. While administering the test was labor intensive, even with only 11 test subjects, breaking up the du- ties among numerous testers helped to minimized the work of each and provided numerous opportunities to observe differ- ent students using the site. This test revealed the validity and use- fulness of qualitative analysis in Web site evaluation. While the University at Buf- falo Libraries study was not elaborate, it did not need to be, given the goal of the testÐto determine whether or not under- graduate students could use the site effec- tively. This is not to say that more quan- tifiable data could not have been gathered using more sophisticated testing pro- cesses. Such techniques are very expen- sive, however, and it is doubtful that they would have been any more revealing. The informal comments and opinions of the students tested were as enlightening as their test performances. For instance, asking users their overall impressions of the Web site as they were preparing to leave resulted in very useful opinions and observations on their part. Because they Figure 2 ªUB Libraries Catalogº Page May 2001 195 assumed that the test had concluded, they seemed more relaxed and willing to give honest impressions regarding those dif- ficult-to-measure factors like aesthetics. In the future, the usability committee plans to build these questions into the test. Finally, this study reiterated the fact that usability testing be a continuous and integral part of Web site development. Initial testing is only the beginning. ªYou want to run multiple tests because the real goal of usability engineering is to im- prove the design and not just to document its weaknesses.º 29 Results of testing lead to recommended changes, which will, in turn, need to be evaluated, implemented and tested again. As Nielsen states, us- ability testing is an iterative process. 30 CONCLUSION Web sites have become an integral part of the information access mission of academic libraries. Although developing a usable and effective site is challenging in its own right, maintaining and redesigning that site to meet the constantly changing needs of users is a seemingly impossible task. Usability testing is perfectly geared to this task since it not only assists in identifying interface problems, but also in developing ways in which to attack those problems. The data compiled as a result of the usability test described above were justifi- cation for changes recently made in the overall design of the University at Buffalo Libraries' Web site. While formal usability testing was the method of choice in this case, it is not the only means of evaluating interfaces. Indeed, this test was supple- mented by a card-sort (inquiry method) test to evaluate the terminology of the library's site and the changes recently implemented will be evaluated using an appropriate us- ability test method. As the site continues to grow and change, testing will no doubt, be an important part of its evolution. Figure 3 ªOnline Resourcesº Page 196 The Journal of Academic Librarianship ªThe importance of usability testing and the applicability of usability testing to library Web sites cannot be understated.º The importance of usability testing and the applicability of usability testing to li- brary Web sites cannot be understated. Whether through inspection, inquiry or formal usability testing, usability test methods serve as both catalysts for design changes and as tools for evaluating those changes, especially as library sites strive to meet the increasing information de- mands of users. When developing a new site or evaluating an existing one, librar- ians, site designers and administrators must ask the following questions: Does (or will) the site provide task support? Is it usable? Is the interface pleasing in terms of ease of use and aesthetics? These questions are necessary elements of suc- cessful and usable Web site design. How- ever, these questions must be directed at real users, who are the key to successful usability testing. The case study above illustrates that carefully planned usability testing is not difficult to implement and the returns in terms of the information revealed about the effectiveness and us- ability of a site are well worth the time, effort, and money invested. Acknowledgment: The authors wish to express our appreciation to the additional members of the Usability Testing Com- mittee for their time, insights and dedica- tion to this study: Gayle Hardy, Lori Widzinski and Christopher Badurek. We also wish to thank the University Librar- ies Web Manager, Dr. Gemma DeVinney, who charged the committee and provided the administrative support that made this study possible. NOTES AND REFERENCES 1. Jakob Nielsen uses increasing corporate budget allocations for usability engineer- ing as well as ªthe increasing number of personal computer trade press magazines that include usability measures in their reviews,º as evidence of this increase. See Jakob Nielsen, ªUsability Laboratories: A 1994 Survey,º Useit.com: Usable Infor- mation Technology, (n.d.) available on- line: http://www.useit.com/papers/uselabs. html (accessed January 6, 2000). 2. ISO DIS 9241-11, Ergonomic Require- ments for Office Work with Visual Display Terminals. Part 11: Guidance on Usabil- ity (London: International Standards Or- ganization, 1994), p. 10. 3. Jakob Nielsen, Usability Engineering (Boston: Academic Press, 1993). Other use- ful books and Web resources on HCI and usability engineering in general include: Keith Instone, Usable Web, available online at http://usableWeb.com/(accessed July 15, 2000); Ben Shneiderman, Designing the User Interface: Strategies for Effective Human-Computer Interaction, 3rd ed. (Reading, MA: Addison Wesley Long- man, 1998); Bryce L. Allen, Information Task: Toward a User-Centered Approach to Information Systems (San Diego, CA: Academic Press, 1996). 4. Jeffrey Rubin, The Handbook of Usability Testing: How to Plan, Design, and Con- duct Effective Tests (New York: Wiley, 1994); Joseph C. Dumas & Janice C. Re- dish, A Practical Guide to Usability Test- ing (Norwood, NJ: Ablex Publishing Co., 1993). Since our testing, a 1999 edition of this title has been published by Intellect, Exeter. 5. Jakob Nielsen, Designing Web Usability: The Practice of Simplicity (Indianapolis, IN: New Riders Publishing, 1999); Jakob Nielsen, ªAlertbox,º column in Useit. com: Usable Information Technology, available online: http://www.useit.com/ alertbox/(accessed January 6, 2000). 6. Jared Spool, Web Site Usability: A De- signers' Guide (North Andover, MA: User Interface Engineering, 1997). This book documents a test of nine popular Web sites for usability. A 1999 edition of this title has been published by Morgan Kaufmann Publishers, San Francisco. 7. Alison Head, Design Wise: A Guide to Evaluating the Interface Design of Infor- mation Resources (Medford, NJ: Cyber- Age Books, 1999). 8. James Hom, The Usability Methods Tool- box, available online: http://www.best. com/;jthom/usability (accessed January 6, 2000). 9. Nielsen lists usability principles that should be followed by all interface de- signers: (1) use simple and natural dialog; (2) speak the user's language; (3) ensure that instructions are easily visible or re- trievable; (4) design consistency; (5) give user appropriate system feedback; (6) pro- vide clearly marked exits; (7) provide shortcuts; (8) display easily interpreted error messages; (9) design to prevent errors; and (10) provide help and docu- mentation (Usability Engineering, p. 20). 10. Useful sources for the discussion of these inspection methods include Jakob Nielsen & R. Mack, eds., Usability Inspection Methods (New York: Wiley, 1994), which introduces inspection methods, including heuristic evaluations and cognitive walk- throughs; and Keith Instone, ªUsability Heuristics for the Web,º webreview.com, available online: http://webreview.com/ wr/pub/97/10/10/usability/sidebar.html (accessed January 6, 2000), which shows how Nielsen's list of heuristics can be adapted for the Web environment. 11. Dumas & Redish, A Practical Guide to Usability Testing,p.22. 12. In addition to the sources cited above, there are numerous online periodicals and trade publications that discuss usability testing and it applicability to Web sites, The most useful are webreview.com, available at http://webreview.com and Useit.com: Usable Information Technol- Figure 4 Nielsen and Landauer's Curve Showing the Relationship between the Number of Users Tested and the Number of Problems Found in a Usability Test (From Nielsen, ªWhy You Only Need to Test with Five Usersº) May 2001 197 ogy, available at http://www.useit.com/. Useit.com includes Nielsen's ªAlertboxº column that discusses various usability topics and well as practical methods by which to implement usability testing. Both publications devote significant space to the discussion of usability. 13. Karen Eliasen, Jill McKinstry, & Beth Mabel Fraser, ªNavigating Online Menus: A Quantitative Experiment,º College & Research Libraries 58 (November 1997): 509 ±516. 14. Janet Chisman, Karen Diller, & Sharon Walbridge, ªUsability Testing: A Case Study,º College & Research Libraries 60 (November 1999): 552±569. 15. Jerilyn Veldof, Michael Prasse, & Victo- ria Mills, ªChauffeured by the User: Us- ability in the Electronic Library,º Journal of Library Administration 26 1999;115± 140. It should be noted that more and more academic libraries have begun to apply usability testing to their Web sites since the writing of this article. When the University at Buffalo Libraries began to design a usability study in May 1999, only a handful of libraries had actually imple- mented some form of testing. In July 2000, however, a quick, superficial search on the terms ª`usability testing' AND li- braryº using the search engine Google SM, listed links to recent and ongoing usability testing of academic library Web sites at, for instance, the University of Washing- ton, Indiana University-Bloomington, Yale, MIT, University of Nevada±Reno, Roger Williams University, and Washing- ton State University. 16. Head, Design Wise,p.48. 17. We modeled our test procedures after those used in a similar test conducted at the University of Arizona Library. See Michelle Clairmont, Ruth Dickstein, & Vicki Mills, ªLiving the Future 2: Testing For Usability in the Design of a New Information Gateway,º available: http:// www.library.arizona.edu/library/teams/ access9798/lft2paper.htm (accessed Janu- ary 6, 2000.) The information outlined in this paper was invaluable to us in that it conveyed the simplicity and effectiveness of conducting usability testing of a library Web site. 18. The student need not necessarily have an- swered the question to be considered ªcor- rect.º For the purposes of this test, a ªcor- rectº response was one in which the user could navigate to the appropriate index, database, resource, etc. in which the ques- tion could then be answered. 19. For more on think-aloud protocol, see Du- mas and Redish, A Practical Guide to Usability Testing and Rubin, The Hand- book of Usability Testing. 20. Nielsen, Usability Engineering, p. 185. 21. Keith Instone, ªUser Test Your Web Site,º webreview.com, available: http:// webreview.com/97/04/25/usability/index. html (January 6, 2000.). 22. Head, Design Wise, p.53. 23. Students did not immediately recognize the term, ªOnline Resourcesº which sug- gests that terminology also be subjected to usability testing. 24. Nielsen, Usability Engineering, pp.172± 174. See also Nielsen's ªWhy You Only Need to Test With 5 Users,º Useit.com: Usable Information Technology, (March 19, 2000) available: http://www.useit. com/alertbox/20000319.html (January 6, 2000). 25. Jakob Nielsen, and Thomas K Landauer, ªA Mathematical Model of the Finding of Usability Problems,º Proceedings of ACM INTERCHI'93 Conference (Am- sterdam, The Netherlands, 24 ±29 April 1993), pp. 206 ±213. Nielsen and Landau- er's formula and curve reproduced in Nielsen, ªWhy You Only Need to Test With 5 Users.º 26. Nielsen, ªWhy You Only Need to Test With 5 Users.º 27. Ibid. 28. Ibid. 29. Ibid. 30. Nielsen, Usability Engineering,p.21. 198 The Journal of Academic Librarianship","libVersion":"0.2.3","langs":""}