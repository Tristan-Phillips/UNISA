{"path":"Subjects/COS3751 - Techniques of Artificial Intelligence/Telegram Notes/Materials/Solution Self-assesment.pdf","text":"BAR CODE Deﬁne Tomorrow. university of south africa Tutorial Letter 203/1/2019 Techniques of Artiﬁcial Intelligence COS3751 Semester 1 Department of Computer Science School of Computing CONTENTS Model solutions for self-assessment assignment COS3751/203/1/2019 Solution SELF ASSESSMENT ASSIGNMENT Study material: Chapters 9, and 18. You may skip sections 9.3, and 9.4. You only need to study 18.1, 18.2, and 18.3 Question 1 Convert the following First Order Logic (FOL) sentence to Conjunctive Normal Form (CNF): ∀y(∀xP (x) ⇒ ∃x(∀zQ(x, z) ∨ ∀zR(x, y, z))) • Eliminate implication: ∀y(¬∀xP (x) ∨ ∃x(∀zQ(x, z) ∨ ∀zR(x, y, z))) • Move ¬ inwards: ∀y(∃x¬P (x) ∨ ∃x(∀zQ(x, z) ∨ ∀zR(x, y, z))) • Standardize variables: The variable name x is used in the scope of two diﬀerent quantiﬁers, so we change the name of one of them to avoid confusion when we drop the quantiﬁers. The same applies for the variable name z. ∀y(∃x¬P (x) ∨ ∃u(∀zQ(u, z) ∨ ∀vR(u, y, v))) Note that the scope of ∀z is the predicate Q(u, z), and the scope of ∀v is the predicate R(u, y, v). On the other hand, the scope of ∀y is the whole formula. • Skolemize: ∀y(¬P (f (y)) ∨ ∀zQ(g(y), z) ∨ ∀vR(g(y), y, v)) Here f and g are Skolem functions, and their arguments are all the universally quantiﬁed vari- ables in whose scope the existential quantiﬁer appears. In this case, ∃u appears only in the scope of ∀y. Note that the same variable u appears twice in the sentence, so we replace each occurrence of u with g(y). • Drop universal quantiﬁers: At this point, all remaining variables are universally quantiﬁed. Moreover, the sentence is equivalent to one in which all the universal quantiﬁers have been moved to the left: ∀y∀z∀v(¬P (f (y)) ∨ Q(g(y), z) ∨ R(g(y), y, v)) We can therefore drop the universal quantiﬁers: ¬P (f (y)) ∨ Q(g(y), z) ∨ R(g(y), y, v) The sentence is now in CNF and it has only one conjunct (one clause) consisting of a disjunction of three literals. Question 2 Consider the following English statements: 1. Anyone who passes their history exam and who wins the lottery is happy. 2. Anyone who studies or is lucky, passes their exams. 2 COS3751/203/1/2019 3. John did not study. 4. John is lucky. 5. Anyone who is lucky wins the lottery. (2.1) Provide a vocabulary for the statements. • P ass(p, s). Predicate. Person p passes subject s. • W in(p, g). Predicate. Person p wins game g. • Happy(p). Property. Person p is happy. • Study(p). Predicate. Person p studies. • Lucky(p). Property. Person p is lucky. • History. Constant for a subject. • Lottery. Constant for a game that can be won. • John. Constant for a person. (2.2) Translate the above English sentences to FOL statements using the vocabulary you de- ﬁned above. The sentences are translated as follows to FOL: 1. ∀p((P ass(p, History) ∧ W in(p, Lottery)) ⇒ Happy(p)) 2. ∀q∀r((Study(q) ∨ Lucky(q)) ⇒ P ass(q, r)) 3. ¬Study(John) 4. Lucky(John) 5. ∀s(Lucky(s) ⇒ W in(s, Lottery)) It is important to standardize the variables (to use diﬀerent variable names) to avoid confusion when dropping the universal quantiﬁers. 3 (2.3) Convert the FOL statements obtained in 2.2 into CNF. The statements are converted as follows to CNF: 1. ∀p((P ass(p, History) ∧ W in(p, Lottery)) ⇒ Happy(p)) ≡ ∀p(¬(P ass(p, History) ∧ W in(p, Lottery) ∨ Happy(p)) ≡ ∀p(¬P ass(p, History) ∨ ¬W in(p, Lottery) ∨ Happy(p)) 2. ∀q∀r((Study(q) ∨ Lucky(q)) ⇒ P ass(q, r)) ≡ ∀q∀r(¬(Study(q) ∨ Lucky(q)) ∨ P ass(q, r)) ≡ ∀q∀r((¬Study(q) ∧ ¬Lucky(q)) ∨ P ass(q, r)) ≡ (¬Study(q) ∧ ¬Lucky(q)) ∨ P ass(q, r) ≡ (¬Study(q) ∨ P ass(q, r)) ∧ (¬Lucky(q) ∨ P ass(q, r)) 3. No action required. 4. No action required. 5. ∀s(Lucky(s) ⇒ W in(s, Lottery)) ≡ ¬Lucky(s) ∨ W in(s, Lottery) We thus end up with the following clauses: 1. ¬P ass(p, History) ∨ ¬W in(p, Lottery) ∨ Happy(p) 2. ¬Study(q) ∨ P ass(q, r) 3. ¬Lucky(q) ∨ P ass(q, r) 4. ¬Study(John) 5. Lucky(John) 6. ¬Lucky(s) ∨ W in(s, Lottery) Note that universal quantiﬁers have been dropped because all variables were universally quantiﬁed. Skolem functions are only introduced to remove existential quantiﬁers. (See section 9.5 of R&N.) 4 COS3751/203/1/2019 (2.4) Use resolution refutation to prove that John is happy. In order to use resolution refutation, we negate the goal, convert the negated goal to clause form if necessary, and add the resulting clause(s) to the set of premises (clauses 1 to 6 in part (2.3) above). Here the goal is: Happy(John). Therefore the negation of the goal is: ¬Happy(John). We then resolve the premises together with the negated goal until the empty clause (Nil) is generated. Review the ground resolution theorem to make sure you understand why this ap- proach works. Also review section 7.5: Remember that α |= β only if (α ∧ ¬β) is unsatisﬁable. 1. ¬P ass(p, History) ∨ ¬W in(p, Lottery) ∨ Happy(p) premise 2. ¬Study(q) ∨ P ass(q, r) premise 3. ¬Lucky(q) ∨ P ass(q, r) premise 4. ¬Study(John) premise 5. Lucky(John) premise 6. ¬Lucky(s) ∨ win(s, Lottery) premise 7. ¬Happy(John) negation of goal 8. ¬W in(p, Lottery) ∨ happy(p) ∨ ¬Lucky(p) 1&3, {p/q}, {history/r} 9. ¬W in(John, Lottery) ∨ Happy(John) 5&8, {John/p} 10. ¬W in(John, Lottery) 9&7 11. W in(John, Lottery) 5&6, {John/w} 12. ∅ 10&11 We have shown that the negation of the goal together with the premises produce a contradiction (empty clause). Therefore the goal, namely Happy(John), is entailed by the premises. It is important to show which clauses form part of the resolution to produce the resolvent. You must also always show the substitutions. Remember: given a fact with a constant, one cannot simply replace the constant with a variable to derive a new (general) statement. For example, suppose we add Carol to the list of persons in the vocabulary for the above problem. We cannot derive: Lucky(John) {x/John} (resolving that John is lucky by replacing the variable x with the constant John), and then Lucky(Carol) {John/Carol}, deriving that Carol is lucky by substituting John with Carol. 5 x1 x2 x3 f(x1, x2, x3) 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 1 1 1 Table 1: Boolean function table Question 3 (3.1) Convert the Boolean function in Table 1 to a decision tree: No information gain values were given, so it becomes a matter of picking the se- quence in which the variables are to be evaluated. The next step would be to simplify the decision tree by consolidating equivalent leaf nodes. Variable order x1, x2, x3: x1 x2 x3 1 0 0 1 0 x3 1 0 1 1 1 0 x2 x3 1 0 0 1 0 x3 0 0 1 1 1 1 This can be simpliﬁed to: 6 COS3751/203/1/2019 x1 x2 x3 1 0 0 1 0 1 1 0 x2 x3 1 0 0 1 0 x3 0 0 1 1 1 1 Variable order x1, x3, x2: x1 x3 1 0 x2 0 0 1 1 1 0 x3 x2 1 0 0 1 0 x2 0 0 1 1 1 1 Variable order x2, x1, x3: x2 x3 x2 1 0 0 1 0 x2 1 0 0 1 1 0 x3 1 00 x2 0 0 1 1 1 1 Variable order x2, x3, x1: 7 x2 x3 1 0 0 1 0 x3 x1 1 0 0 1 0 1 1 1 Variable order x3, x2, x1: x3 x2 1 0 x1 1 0 0 1 1 0 x2 0 0 1 1 1 Variable order x3, x1, x2: x3 x1 1 0 x2 1 0 0 1 1 0 x1 x2 0 0 1 1 0 x2 0 0 1 1 1 1 8 COS3751/203/1/2019 (3.2) When we construct a decision tree without the beneﬁt of gain values, the order in which we evaluate the variables is important. Why? It may be possible to consolidate leaf nodes with similar values to produce a smaller, more compact tree. Question 4 The National Credit Act introduced in South Africa in 2007 places more responsibility on a bank to determine whether the loan applicant will be able to aﬀord it. Blue Bank has a table of information on 14 loan applications they have received in the past: No. Credit history Debt Collateral Income Risk 1 BAD HIGH NO < R15k HIGH 2 UNKNOWN HIGH NO R15k - R35k HIGH 3 UNKNOWN LOW NO R15k - R35k MEDIUM 4 UNKNOWN LOW NO < R15k HIGH 5 UNKNOWN LOW NO > R35k LOW 6 UNKNOWN LOW YES > R35k LOW 7 BAD LOW NO < R15k HIGH 8 BAD LOW YES > R35k MEDIUM 9 GOOD LOW NO > R35k LOW 10 GOOD HIGH YES > R35k LOW 11 GOOD HIGH NO < R15k HIGH 12 GOOD HIGH NO R15k - R35k MEDIUM 13 GOOD HIGH NO > R35k LOW 14 BAD HIGH NO R15k - R35k HIGH Table 2: Risk information table Use the information in Table 2 to construct a decision tree that will assist the bank in determining the risk associated with a new loan application. In order to determine the information gain of any diﬀerent attributes of a certain collection of data, the calculation of entropy is important the notion of information gain is deﬁned in terms of entropy. Entropy can be described as a measure of impurity of an arbitrary collection of examples. Russell & Norvig describe entropy as a measure of the uncertainty of a random variable, and mention that the acquisition of information corresponds to a reduction in entropy. Namely a random variable with only one value has no uncertainty and thus its entropy is deﬁned as zero; thus we gain no information by observing its value. For any attribute A, if no information can be gained regarding the decision from attribute A, then the entropy of attribute A is equal to 0. If all the members of a class are split equally by any attribute A, then the entropy of A is 1. For any collection S the maximum entropy value is 1. Formally, the entropy of a random variable V with value vk, each with probability P (vk) is deﬁned as: H(V ) = n∑ k=1 −P (vk)log2 1 P (vk) = − n∑ k=1 P (vk)log2 1 P (vk) (1) 9 Maybe a more comprehensible way of deﬁning the entropy of any collection S is as follows: Entropy(S) = − c∑ i=1 pilog2(pi) (2) This corresponds to what Russell and Norvig refer to as information content. If the possible answers vi have probabilities P (vi) then the information content I of the actual answer is given by: I(P (v1), . . . , P (vn)) = n∑ i=1 −P (vi)log2P (vi) (3) You may see the negative as a way of making the values of pilog2(pi) positive because all pi < 1, i.e. each represents a probability of the sample set. In what follows we will use the ID3 algorithm to develop our decision tree. Our collection of 14 examples S, has four meaningful attributes (Credit History, Debt, Collateral, Income), and a 3-wise classiﬁcation (LOW, MEDIUM, HIGH). Since there are no repeat values in the No. attribute it plays no role in the classiﬁcation and can be ignored (although we will show the No. attribute in further tables so that we know which examples we are working with). For purposes of clarity in the formulae, we will shorten the attribute labels and values as follows: • Credit History = CH • Debt = D • Collateral = C • Income = I • BAD = B • UNKNOWN = U • GOOD = G • HIGH = H • LOW = L • <R15K = 15K • R15K-R35K = 15K-35K • >R35K = 35K • MEDIUM = M We will also not be repeating calculations at every step: we show the full set of calculations for the ﬁrst level of the tree, after that we will only show the ﬁnal result of the calculation. Summarise the example set, S = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14}, as: S = [5L, 3M , 6H] We start by calculating the Entropy for our set of examples: 10 COS3751/203/1/2019 Entropy(S) = 3∑ i=1 −pilog2pi = −( 5 14)log2( 5 14) − ( 3 14)log2( 3 14) − ( 6 14)log2( 6 14) = −( 5 14)(log25 − log214) − ( 3 14)(log23 − log214) − ( 6 14)(log26 − log214) = −( 5 14) ( log5 log2 − log14 log2 ) − ( 3 14) ( log3 log2 − log14 log2 ) − ( 6 14) ( log6 log2 − log14 log2 ) = 0.531 + 0.476 + 0.524 = 1.531(max = 1.585) We have to determine the information gain (IG) of the diﬀerent attributes in order to select the best choice for the root node. The information gain measures the expected reduction of entropy – the higher the IG, the higher the expectation of reduction of entropy. In what follows we calculate the information gain for each of the four attributes. Credit History: Values (CH ) = B, G, U S = [5L, 3M , 6H] SB ← [0L, 1M , 3H] SG ← [3L, 1M , 1H] SU ← [2L, 1M , 2H] Entropy(SB) = −(0 4 )log2(0 4) − (1 4 )log2(1 4) − (3 4 )log2(3 4) = 0.811 Entropy(SG) = −(3 5 )log2(3 5) − (1 5 )log2(1 5) − (1 5 )log2(1 5 ) = 1.371 Entropy(SU ) = −(2 5 )log2(2 5) − (1 5 )log2(1 5) − (2 5 )log2(2 5) = 1.522 Gain(S, CH) = Entropy(S) − ∑ v∈V alues(A) |Sv| |S| Entropy(Sv) = Entropy(S) − ( 4 14)Entropy(SB) − ( 5 14)Entropy(SG) − ( 5 14)Entropy(SU ) = 1.531 − ( 4 14)0.811 − ( 5 14)1.371 − ( 5 14)1.522 = 0.266 11 Debt: Values(D) = L, H S = [5L, 3M , 6H] SL ← [3L, 2M , 2H] SH ← [2L, 1M , 4H] Entropy(SL) = −(3 7)log2(3 7 ) − (2 7)log2(2 7 ) − (2 7)log2(2 7 ) = 1.557 Entropy(SH) = −(2 7 )log2(2 7) − (1 7 )log2(1 7) − (4 7 )log2(4 7) = 1.379 Gain(S, D) = Entropy(S) − ( 7 14)Entropy(SL) − ( 7 14)Entropy(SH) = 1.531 − ( 7 14)1.557 − ( 7 14)1.379 = 0.063 Collateral: Values (C) = Y,N S = [5L, 3M , 6H] SY ← [2L, 1M , 0H] SN ← [3L, 2M , 6H] Entropy(SY ) = −(2 3 )log2(2 3) − (1 3 )log2(1 3) − (0 3 )log2(0 3) = 0.918 Entropy(SN ) = −( 3 11)log2( 3 11) − ( 2 11)log2( 2 11) − ( 6 11)log2( 6 11) = 1.435 Gain(S, C) = Entropy(S) − ( 3 14)Entropy(SY ) − (11 14)Entropy(SN ) = 1.531 − ( 3 14)0.918 − (11 14)1.435 = 0.207 Income: Values (I ) = 15 , 15-35, 35 S = [5L, 3M , 6H] S15 ← [0L, 0M , 4H] 12 COS3751/203/1/2019 S15−35 ← [0L, 2M , 2H] S35 ← [5L, 1M , 0H] Entropy(S15) = −(0 4 )log2(0 4) − (0 4 )log2(0 4) − (4 4 )log2(4 4) = 0 Entropy(S15−35) = −(0 4)log2(0 4 ) − (2 4)log2(2 4 ) − (2 4)log2(2 4 ) = 1 Entropy(S35) = (5 6 )log2(5 6) − (1 6 )log2(1 6) − (0 6 )log2(0 6) = 0.650 Gain(S, I) = Entropy(S) − ( 4 14)Entropy(S15) − ( 4 14)Entropy(S15−35) − ( 6 14)Entropy(S35) = 1.531 − ( 4 14)0 − ( 4 14)1 − ( 6 14)0.650 = 0.967 Attribute Income provides the highest information gain (0.967), i.e. the best prediction for our target attribute, Risk. Income becomes the root node of our decision tree. Income < 15K 15K − 35K > 35K The ID3 algorithm now performs a recursion with the three subsets of our examples, based on the three possible values of the Income attribute (<R15K, R15K-R35K, >R35K). The ﬁrst subset, corresponding to Income = {< R15K}, is: No. Credit History Debt Collateral Income Risk 1 BAD HIGH NO < R15k HIGH 4 UNKNOWN LOW NO < R15k HIGH 7 BAD LOW NO < R15k HIGH 11 GOOD HIGH NO < R15k HIGH We notice that the target attribute is the same for all four of our examples, i.e. all four examples produce the same target value, HIGH. We have our ﬁrst leaf node, labelled HIGH. 13 Income HIGH < 15K 15K − 35K > 35K The second subset, corresponding to Income = {R15K − R35K}, is: No. Credit History Debt Collateral Income Risk 2 UNKNOWN HIGH NO R15K-R35K HIGH 3 UNKNOWN LOW NO R15K-R35K MEDIUM 12 GOOD HIGH NO R15K-R35K MEDIUM 14 BAD HIGH NO R15K-R35K HIGH Our subset collection of 4 examples S, has three meaningful attributes (Credit History, Debt, Collateral), and a binary classiﬁcation (MEDIUM, HIGH): S15K−35K = {2, 3, 12, 14} = [2M , 2H]. We start by calculating the Entropy for this subset: Entropy(S15K−35K) = 2∑ i=1 −pilog2pi = −(2 4 )log2(2 4) − (2 4 )log2(2 4) = 0.5 + 0.5 = 1 Calculate the information gain for the attributes of the subset (entropy for each attribute is calculated in the same fashion as above). Credit History: Gain(S15K−35K, CH) = Entropy(S15K−35K) − (1 4 )Entropy(SB) − (1 4 )Entropy(SG) − (2 4 )Entropy(SU ) = 1 − (1 4)0 − (1 4 )0 − (2 4)1 = 0.5 Debt: Gain(S15K−35K, D) = Entropy(S15K−35K) − (1 4 )Entropy(SL) − (3 4)Entropy(SH) = 1 − (1 4 )0 − (3 4)0.918 = 0.312 14 COS3751/203/1/2019 Collateral: Gain(S15K−35K, C) = Entropy(S15K−35K) − (0 4)Entropy(SY ) − (4 4)Entropy(SN ) = 1 − (0 4 )0 − (4 4)1 = 0 Attribute Credit History provides the highest Information Gain in this subset (0.5). Thus it becomes our next decision node. Income HIGH < 15K Credit History BAD GOOD UNKNOWN 15K − 35K > 35K The third subset, corresponding to Income = {> R35k}, is then used. No. Credit History Debt Collateral Income Risk 5 UNKNOWN LOW NO >R35K LOW 6 UNKNOWN LOW YES >R35K LOW 8 BAD LOW YES >R35K MEDIUM 9 GOOD LOW NO >R35K LOW 10 GOOD HIGH YES >R35K LOW 13 GOOD HIGH NO >R35K LOW Our subset collection of 6 examples S, has three meaningful attributes (Credit History, Debt, Collateral), and a binary classiﬁcation (LOW, MEDIUM): S35K = {5, 6, 8, 9, 10, 13} = [5L, 1M ] Start by calculating the Entropy for this subset: Entropy(S35K) = 2∑ i=1 −pi log2pi = −(5 6) log2 (5 6 ) − (1 6) log2 (1 6 ) = 0.65 15 We now proceed to calculate the information gain for each of the three attributes in the subset: Credit History: V alues(CH) = B, G, U S35K = [5L, 1M ] SB ← [0L, 1M ] SG ← [3L, 0M ] SU ← [2L, 0M ] Entropy(SB) = −(0 1 )log2(0 1) − (1 1 )log2(1 1) = 0 Entropy(SG) = −(3 3 )log2(3 3) − (0 3 )log2(0 3) = 0 Entropy(SU ) = −(2 2 )log2(2 2) − (0 2 )log2(0 2) = = 0 Gain(S35K, CH) = Entropy(S35K) − (1 6 )Entropy(SB) − (3 6)Entropy(SG) − (2 6)Entropy(SH) = 0.65 − (1 6 )0 − (3 6)0 − (2 6 )0 = 0.65 Debt: V alues(D) = L, H S35K = [5L, 1M ] SL ← [5L, 1M ] SH ← [2L, 0M ] 16 COS3751/203/1/2019 Entropy(SL) = −(3 4 )log2(3 4) − (1 4 )log2(1 4) = 0.811 Entropy(SG) = −(2 2 )log2(2 2) − (0 2 )log2(0 2) = 0 Gain(S35K, D) = Entropy(S35K) − (4 6)Entropy(SL) − (2 6 )Entropy(SH) = 0.65 − (4 6)0.811 − (2 6 )0 = 0.109 Collateral: V alues(D) = Y, N S35K = [5L, 1M ] SY ← [2L, 1M ] SN ← [3L, 0M ] Entropy(SL) = −(2 3)log2(2 3 ) − (1 3)log2(1 3 ) = 0.918 Entropy(SG) = −(3 3)log2(3 3 ) − (0 3)log2(0 3 ) = 0 Gain(S35K, D) = Entropy(S35K) − (3 6)Entropy(SY ) − (3 6)Entropy(SN ) = 0.65 − (3 6)0.918 − (3 6)0 = 0.191 In summary: Gain(S35K, CH) = 0.65 Gain(S35K, D) = 0.109 Gain(S35K, Collateral) = 0.191 Attribute Credit History provides the highest Information Gain in this subset and becomes the decision node on this branch of the decision tree. 17 Income HIGH < 15K Credit History BAD GOOD UNKNOWN 15K − 35K Credit History BAD GOOD UNKNOWN > 35K The algorithm continues with recursion to the next level. The subset of Credit History that corresponds to the Income = R15K-R35K and Credit History = BAD provides a single example (14). No. Credit History Debt Collateral Income Risk 14 BAD HIGH NO R15K-R35K HIGH We have a single example, hence another leaf node: HIGH. Income HIGH < 15K Credit History HIGH BAD GOOD UNKNOWN 15K − 35K Credit History BAD GOOD UNKNOWN > 35K The subset of Credit History that corresponds to the Income = R15K-R35K and Credit History = GOOD is also a single example (12). No. Credit History Debt Collateral Income Risk 14 BAD HIGH NO R15K-R35K HIGH We have a single example, hence another leaf node: MEDIUM. 18 COS3751/203/1/2019 Income HIGH < 15K Credit History HIGH BAD MEDIUM GOOD UNKNOWN 15K − 35K Credit History BAD GOOD UNKNOWN > 35K The subset of Credit History that corresponds to the Income = R15K-R35K and Credit History = UNKNOWN has two examples (2,3). No. Credit History Debt Collateral Income Risk 2 UNKNOWN HIGH NO R15K-R35K HIGH 3 UNKNOWN LOW NO R15K-R35K MEDIUM Our subset collection of 2 examples S, has two meaningful attributes (Debt, Collateral), and a binary classiﬁcation (MEDIUM, HIGH shortened for clarity to M,H): SU nknown = {2, 3} = [1M , 1H]. We again calculate the entropy for the subset: Entropy(S) = −(1 2 )log2(1 2) − (1 2 )log2(1 2) = 1 Debt V alues(D) = L, H SU N KN OW N = [1L, 1M ] SL ← [2L, 1M ] SH ← [3L, 0M ] 19 Entropy(SL) = −(1 1 )log2(1 1) − (0 1 )log2(0 1) = 0 Entropy(SH) = −(0 1 )log2(0 1) − (1 1 )log2(1 1) = 0 Gain(SU N KN OW N , D) = Entropy(SU N KN OW N ) − (1 2)Entropy(SL) − (1 2 )Entropy(SH) = 1 − (1 2 )0.918 − (1 2 )0 = 1 Collateral V alues(D) = N SU N KN OW N = [1M , 1H] SN ← [1M , 1H] Entropy(SN ) = −(1 2)log2(1 2 ) − (1 2)log2(1 2 ) = 1 Gain(SU N KN OW N , C) = Entropy(SU N KN OW N ) − (2 2)Entropy(SN ) = 1 − (2 2 )1 = 0 In summary: Gain(SU N KN OW N , D) = 1 Gain(SU N KN OW N , Collateral) = 0 Debt gives us perfect information gain, and thus becomes the next decision node. 20 COS3751/203/1/2019 Income HIGH < 15K Credit History HIGH BAD MEDIUM Good Debt HIGH LOW UNKNOWN 15K − 35K Credit History BAD GOOD UNKNOWN > 35K We return to credit history (we ﬁrst ﬁnish all the nodes on the same level). The subset of Credit History that corresponds to the Income = >R35K and Credit History = BAD contains only 1 example (8). No. Credit History Debt Collateral Income Risk 8 BAD LOW YES >R35K MEDIUM We thus have another leaf node: MEDIUM. Income HIGH < 15K Credit History HIGH BAD MEDIUM GOOD Debt HIGH LOW UNKNOWN 15K − 35K Credit History MEDIUM BAD GOOD UNKNOWN > 35K 21 The subset of Credit History that corresponds to Income = >R35K and Credit History = GOOD provides three examples (9,10,13): No. Credit History Debt Collateral Income Risk 9 GOOD LOW NO >R35K LOW 10 GOOD HIGH YES >R35K LOW 13 GOOD HIGH NO >R35K LOW Our subset collection of 3 examples S, has two meaningful attributes (Debt, Collateral), and a single classiﬁcation (LOW). Hence we again have a leaf node LOW. Income HIGH < 15K Credit History HIGH BAD MEDIUM GOOD Debt HIGH LOW UNKNOWN 15K − 35K Credit History MEDIUM BAD LOW GOOD UNKNOWN > 35K The subset of Credit History that corresponds to the Income = >R35K and Credit History = UNKNOWN is provides 2 examples (5,6). No. Credit History Debt Collateral Income Risk 5 UNKNOWN LOW NO >R35K LOW 6 UNKNOWN LOW YES >R35K LOW Our subset collection of 2 examples S, has two meaningful attributes (Debt, Collateral), and a single classiﬁcation (LOW). Hence we again have a leaf node LOW. 22 COS3751/203/1/2019 Income HIGH < 15K Credit History HIGH BAD MEDIUM GOOD Debt HIGH LOW UNKNOWN 15K − 35K Credit History MEDIUM BAD LOW GOOD LOW UNKNOWN > 35K We can now do the last set of calculations. With Debt = HIGH we have only one example left (2), which makes this a leaf node. No. Credit History Debt Collateral Income Risk 2 UNKNOWN HIGH NO R15K-R35K HIGH Similarly, when Debt = LOW we have one example left (3). This creates the ﬁnal leaf node. No. Credit History Debt Collateral Income Risk 3 UNKNOWN LOW NO R15K-R35K MEDIUM These last two steps complete our decision tree (note that Collateral plays no role in the decision based on this decision tree). 23 Income HIGH < 15K Credit History HIGH BAD MEDIUM GOOD Debt HIGH HIGH MEDIUM LOW UNKNOWN 15K − 35K Credit History MEDIUM BAD LOW GOOD LOW UNKNOWN > 35K Copyright c⃝UNISA 2019 (v2019.1.3) 24","libVersion":"0.2.3","langs":""}