{"path":"Subjects/COS3712 - Computer Graphics/Unsorted/COS3712/Exam pack/Exam pack/Materials/4_5915801117861612035.pdf","text":"Terminology: • Affine: Line preserving • Nonuniform foreshortening: images of objects farther away from COP are reduced in size compared to closer images. Pipeline: • Vertex processing: each Vertex are processed independently. Per-vertex lighting calculation done here • Clipping and primitive assembly: Sets of vertices assembled into primitives. Output is a set of primitives whose projections should be appear in the image. • Rasterization: Primitives that emerge from clipper are still represented in terms of their vertices and must be converted to pixels in frame buffer. Output of rasterizer is a set of primitives whose projections should appear in the image. • Fragment processing: takes fragments generated by rasterizer and updates pixels in frame buffer. Hidden surface removal; texture mapping, bump mapping, and alpha blending can be applied here. • Advantages: o Increases performance when same sequence of concurrent operations on many, or large, data sets o Process on each primitive can be done independently • Disadvantages: o Latency of the system must be balanced against increased throughput. o Global effects may not be handled correctly. Sierpinski Gasket: • Immediate mode graphics: As vertices are generated, they are sent to graphics processor for rendering on display. No memory of geometric data • Retained mode graphics: All geometric data is computed and stored in some data structure. Then the scene is displayed by sending all data to graphics processor at once. Overheads are avoided from sending small amounts of data to graphics processor for each vertex generated. API Function groups: PAVTICQ • Primitive functions: Define low level objects or atomic entities that the system can display. • Attribute functions: Governs the way primitives appear on display. • Viewing functions: Allows us to specify various views. • Transformation functions: Allows us to carry out transformations of objects. (rotation, translation, and scaling) • Input functions: Deal with input devices • Control functions: Communicate with window system, initialise our programs, and deal with any errors taking place during program execution. • Query functions: Allow us to obtain info about operating environment, camera parameters, values in frame buffer etc. Primitives and attributes: • Well defined interior if it satisfies the following: o Simple: In 2D, as long as no 2 edges of a polygon cross each other o Convex: Object is convex if all points on the line segment between any two points inside the object, or on the boundary, are inside the object. Triangles, tetrahedral, rectangles, parallelepipeds, circles, and spheres. o Flat: All vertices specifying the polygon lie in the same plane. Forms of text: • Stroke text: Constructed as are other geometric objects. Vertices are used to specify line segments or curves outlining each character. Can be manipulated like any other graphical primitive • Raster text: Characters are defined as rectangles of bits, called bit blocks. Each block defines single character by pattern of 0 and 1 bits in the block. Colour: • Additive: 3 primary colours (RGB) add together to give perceived colour • Subtractive: Start off with white surface. Coloured pigments remove colour components from light striking a surface. CMY is used • Colour cube: View a colour as a point in a colour solid. • RGB System: Each pixel may consist of 24 bits (3 bytes), one for each colour. Specification is based on the colour cube. Numbers between 0.0 and 1.0 denote the saturation of each colour. • Indexed colour: Frame buffers limited in depth. 8 bits deep, not subdivided into groups, but the limited depth pixel interpreted as an integer value indexing to the colour lookup table. Only works with dynamic images that must be shaded. Types of events: • Mouse events: when mouse button is pressed or depressed, or mouse is moved with mouse button • Reshape events: When user resizes the window. • Keyboard events: Generated when mouse is in window and one of the keys is pressed or released • Idle callback: invoked when there are no other events. Double Buffering: • Distortion caused when a redisplay of the frame buffer can cause a partially drawn display. • Common solution is double buffering. o Hardware has 2 frame buffers o Front buffer: Buffer that is displayed o Back buffer: Available for constructing what to display next o Then buffers are swopped, and new back buffer is cleared and starts drawing next display. Homogenous Coordinates: • Advantages: o All affine transformations can be represented using matrix multiplications o We can carry out operations on points and vectors using homogenous coordinate representations and ordinary matrix algebra. o Uniform representation of all affine transformations make carrying out successive transformations far easier than in 3D space o Less arithmetic is involved o Modern hardware implements homogenous coordinate operations directly, using parallelism to achieve high speed calculations. Transformations: OWECNWS • Frames in WebGL: o 1. Object coordinates: Object details are kept in the model or object frame o 2. World coordinates: A scene may contain many objects. After the application program applies a sequence of transformations to each object, to display in the frame. This frame is called world frame – values in world coordinates. o 3. Eye coordinates: All graphics systems use a frame with origin is the centre of camera. Frame is called the camera or eye frame. o 4. Clip coordinates: Once objects are in clip coordinates, WebGL checks whether they lie within view volume. If not, clipped from scene prior to rasterization. o 5. Normalised device coordinates: Vertices still represented in homogenous coordinates, division by the 4th component, called perspective division, yields 3D representations in normalised device coordinates. o 6. Window coordinates: Final transformation take position in normalised device coordinates and taking the viewport into account, creates a 3D representation in window coordinates. o 7. Screen coordinates: if depth coordinate is removed, we work with 2D screen coordinates. Rotation, Translation, and scaling: o Translation: Displaces point by a fixed distance in a given direction. o Rotation: Rotates points by fixed angle about a point or line. o Both rotation and translation are known as rigid body transformations. No combination of rotations and translations can alter shape or volume of an object. Only location and orientation. o Scaling: Affine non rigid body transformation by which we can make objects bigger or smaller. Scaling has a fixed point, unaffected by transformation. ▪ Uniform: Scaling factor is identical in all directions. Shape of scaled object is preserved ▪ Non uniform: Scaling factor of each direction not be identical, shape is distorted. Transformation Matrices in GL: • Most used transformations: o Model-view transformation: Affine transformation matrix bringing representations of geometric objects from application or model frames to the camera frame. o Projection transformation: Projection matrix is usually not affine and is responsible for carrying out both desired projection, and also changes the representation to clip coordinates Projection normalisation: • Projection normalisation: o Technique converts all projections into simple orthogonal projections by distorting objects such that the orthogonal projection of distorted objects is same as desired projection of original objects. o Done by applying a matrix called normalisation matrix o Vertices transformed such that vertices within specified view volume are transformed to vertices within the canonical view volume. o Advantages: ▪ Both perspective and parallel views can be supported by same pipeline. ▪ Clipping process is simplified because sides of canonical view volume are aligned within coordinate axes o Shape of viewing volume for an orthogonal projection is a right parallelepiped: ▪ Perform translation to move centre specified view volume to centre of canonical view volume. ▪ Scale sides of specified view volume such that they have length of 2. Positioning of camera: • Model view matrix encapsulates relationship between camera frame and object frame o Modelling transformation: ▪ Takes instances of objects in object coordinates and brings them into world frame. o Viewing transformations: ▪ Transforms world coordinates to camera coordinates. • First method for construction: o Concatenating a carefully selected series of affine transformations. • Second approach: o Specify camera frame with respect to the world frame and construct matrix, called view-orientation matrix, taking us from world to camera coordinates. o 3 parameters need to be specified: ▪ View reference point VRP: Specifies location of COP in world coordinates ▪ View plane normal VPN: Also known as n, specifies normal to the projection plane ▪ View-up vector VUP: Specifies what direction is up from camera’s perspective. • Third method: o Called look at function and similar to second approach. o Only differs in way VPN is specified Frustum projection: Hidden Surface removal: • Algorithms removing surfaces that cannot be seen from the viewer. • Classes: o Object space: Attempt to order surfaces of objects in the scene such that rendering surfaces in a particular order provide correct image. o Image space: Work as part of projection process and seek to determine relationship among object points on each projector. • Z-Buffer: o Stores depth data of objects o Has same spatial resolution as colour buffer. o Advantages: ▪ Complexity is proportional to number of fragments generated by rasterizer ▪ Can be implemented with small number of additional calculations. ▪ Easy to implement ▪ Compatible with pipeline architecture. • Depth sort: o Orders all polygons by how far away from the viewer their max z- value is. o If none overlap, we paint the polygons back to front and we are done. o If they overlap, We may still be able to find an order to paint the polygons individually and yield correct image. o The algorithm runs increasingly more difficult tests, attempting to find such ordering. • Culling: For situations where back faces cannot be seen, Reduce work required for hidden surface removal by eliminating all back facing polygons before applying other hidden surface removal algorithms. o Usually performed after transformation to normalized device coordinates. Light and matter: • Classified into 3 groups: o Specular surfaces: Appear shiny, most light that is reflected or scattered in a narrow range of angles close to angle of reflection. Perfectly specular is when all light reflected emerges at a single angle. o Diffuse surfaces: Reflected light is scattered in all directions. Perfectly diffuse surfaces scatter light equally in all directions. o Translucent surfaces: Allow some light to penetrate and to emerge from another location on the object. Glass and water. Light sources: • Ambient light: Lights been designed and positioned to provide uniform illumination • Point sources: Ideal point source emits light equally in all directions. • Spotlights: Characterised by narrow range of angles through which light is emitted. • Distant light source: All rays parallel and we replace location of the light source with direction of the light. Reflection models: • Phong: o Four vectors used to colour arbitrary point p on a surfaces n – normal at p v – in direction from p to the viewer or COP l – in direction of a line from p to arbitrary point on light source r – in direction a perfectly reflected ray from l would take o Supports diffuse, ambient, and specular. o Lambert’s law: We only see the vertical component of incoming light ▪ 𝜃 is angle between normal at point of interest n and direction of light source l. ▪ ∝ increased: reflected light is concentrated in narrower region centred on angle of a perfect reflector. Meshes: • Set of polygons that share vertices and edges • Can be used to display height data. Polygonal shading: • Flat shading: constant shading. o Shading calculation only carried out once for each polygon, and each point on polygon is assigned the same shade o Will show differences among adjacent polygons • Gouraud shading: smooth shading o Lighting calculation done at each vertex using material properties and vectors n, v, and l. o Each vertex will have its own colour that the rasterizer can use to interpolate a shade for each fragment. • Phong shading: o Instead of interpolating vertex intensities, we interpolates normal across each polygon. o Independent lighting calculation for each fragment is made. Implementing lighting model: • Can be implemented in application, vertex shading, or fragment shader. • For efficiency sake, we almost always wat to do lighting calculations in shaders. • 3 steps to carry out implementation: o Choose lighting model o Write shader to implement model o Finally, transfer necessary data to the shader Global vs Local shading: • Global: Shadows, reflections, and blockage of light are global effects and require global lighting model. o Disadvantages: global models are incompatible with the pipeline architecture. • Local: Used when objects are shaded independently Mapping methods • Texture mapping: o Uses an image to influence the colour of a fragment. o Texture can be digitised image or generated by a procedural texture generation method. o WebGL: ▪ Texture maps rely heavy on pipeline architecture. ▪ 2 parallel pipelines – geometric and pixel pipeline. ▪ Pixel pipeline merges with fragment processing after rasterization ▪ Done by: • Form texture image and place in texture memory on GPU • Assign texture coordinates to each fragment • Apply texture to each fragment ▪ Key element in applying a texture in the fragment shader is the mapping between location of a fragment, and corresponding location with texture image where we get the texture colour for that fragment. ▪ Strategies for mapping texture coordinates to array of texels: • Point sampling: Use value of the texel closest to the texture coordinate output but the rasterizer. Most subject to aliasing errors • Linear filtering: Use a weighted average for a group of texels in the neighbourhood of texel determined by point sampling. Smoother texturing. ▪ Mipmaps can used to deal with minification problem • Bump mapping: o Distorts normal vectors during shading process to make surface appear to have small variations in shape. – such as bumps on a real orange. o Technique varies the apparent shape of the surface by perturbing normal vectors as surface is rendered. o Cannot be done in real time without programmable shaders. • Environment maps: o Allows us to create images that have the appearance of reflected materials without having to trace deflected rays. o Image of environment is painted onto the surface as that surface is being rendered. o Variant of texture mapping that can give approximate results that are visually acceptable. o 2 step rendering pass: ▪ 1st pass: render scene without reflecting object. Camera placed at the centre of the mirror pointed in the direction of the normal of the mirror. ▪ 2nd pass: Then use the image to obtain the shades to place on the mirror for the second rendering with the mirror placed back in the scene o Difficulties: ▪ Images we obtain in the first pass are not quite correct because they were formed without the mirror ▪ Mapping issues: onto what surface should we project the scene in the first pass and where should the camera be placed. – to solve project the environment onto a sphere at COP. Blending: • Alpha blending on RGB (RGBA) • The value of A controls how the RGB values are written into frame buffer. • Value of 1 is opaque. Value of 0 is transparent. • Difficulty: The order in which polygons are rendered affects final image. o In a scene containing both opaque and translucent polygons, any polygon behind an opaque polygon should not be rendered, but polygons should be composited. Z-Buffer will not composite polygons correctly if a translucent polygon is rendered first, and an opaque polygon is rendered afterwards behind it Z-buffer must be made read only when rendering translucent polygons, depth information will then be prevented from being updated when rendering translucent objects. • One major use is antialiasing. When rendering a line, instead of colouring a whole pixel with colour of line if it passes through it, amount of contribution of the line to the pixel is stored in pixels alpha value. Value then used to calculate intensity of a colour, avoiding sharp contrasts and steps of aliasing. • Rather antialiasing individual lines and polygons, we can use antialiasing on entire scenes using a technique, multisampling. Every pixel in the frame buffer contains number of samples, capable of storing colour, depth, and other values. When scene is rendered, the scene looks as if it is rendered at an enhanced resolution. When image must be displayed in the frame buffer, all samples for each pixel are combined to produce final pixel colour. Rasterization: Produces fragments from remaining objects. • Pixels have attributes that are colours in colour buffer. • Fragments are potentially pixels and each fragment has colour attribute and location in screen coordinates corresponding to location in colour buffer • DDA Algorithm: • Bresenham’s algorithm: o DDA requires a floating-point addition for each pixel generated. o Bresenham avoids all floating-point calculations and has become the standard algorithm used in hardware and software rasterizers. o Calculation of each successive pixel in the colour buffer requires only an addition and sign test • Polygon rasterization: o Crossing (odd even) test ▪ Any ray emanating from point outside polygon and entering the polygon crosses even number of edges before infinity ▪ Any ray emanating from point inside polygon and going off to infinity must cross an odd number of edges o Winding test: ▪ Considers the polygon as a knot being wrapped around a point or line. ▪ Start by traversing edges of polygon from any starting vertex and going around the edge in a particular direction until we reach the starting point. ▪ Next an arbitrary point is considered. ▪ Winding number for this point is the number of times it is encircles by the edges of the polygon. ▪ Count clockwise as positive and counter-clockwise as negative. ▪ Point is inside a polygon if its winding number is not zero Major tasks from Geometry to pixels: • Modelling: o Results are sets of vertices specifying group of geometric objects supported by rest of system • Geometry processing: o Determines which geometric objects can be appear on the display and assign shades or colour to object vertices o 4 processes: Projection, primitive assemble, clipping, and shading • Rasterization: o Rasterizer starts with vertices in normalised device coordinates and outputs fragments whose locations are in units of display o Line segments: Rasterization determines which fragments can be used to approximate a line segment between projected vertices. o Colours assigned to fragments are determined by vertex attributes or obtained by interpolating shades at vertices compound. • Fragment processing: o Per fragment shading, texture mapping, bump mapping, alpha blending, antialiasing, and hidden surface removal take place here Clipping: • Clipper decides which primitives, or parts of primitives, can possibly appear onto the display and be passed on to the rasterizer. • Those fitting within the specified view volume are accepted. • Those that fall outside are eliminated. • Those partially inside are clipped so anything outside the volume is removed. • Cohen-Sutherland clipping: o Algorithm extends sides of the clipping rectangle to infinity. o The algorithm includes, excludes or partially includes the line based on whether: ▪ Both endpoints are in the viewport region ▪ Both endpoints share at least one non-visible region, which implies that the line does not cross the visible region ▪ Both endpoints are in different regions: in case of this nontrivial situation the algorithm finds one of the two points that is outside the viewport region (there will be at least one point outside). The intersection of the outpoint and extended viewport border is then calculated (i.e. with the parametric equation for the line), and this new point replaces the outpoint. The algorithm repeats until a trivial accept or reject occurs. • Liang-Barsky Clipping: o Algorithm uses the parametric equation of a line and inequalities describing the range of the clipping window to determine the intersections between the line and the clip window. o More efficient that Cohen-Sutherland algorithm by avoiding multiple shortening of line segments and related re-executions of clipping algorithm o Idea of the Liang–Barsky clipping algorithm is to do as much testing as possible before computing line intersections Antialiasing: • Caused by 3 related problems with discrete nature of the frame buffer: o If we have an n x m frame buffer, number of pixels are fixed. Only can generate certain patters to approximate a line segment. o Pixel locations are fixed on uniform grid o Pixels have a fixed size and shape. • Scan-conversion algorithm forces us, for lines of slope less than 1, to choose exactly one-pixel value for each value of x. • Antialiasing by area: We can shade each box by the percentage of ideal line that crosses it, we get a smoother looking rendering. • If polygons share a pixel, and each polygon has a different colour, colour assigned to the pixel is one associated with the polygon closest to the viewer. We could obtain a much more accurate image if we could assign a colour based on an area weighted average of colours of these polygons.","libVersion":"0.2.3","langs":""}