{"path":"Subjects/COS3712 - Computer Graphics/Unsorted/Ex/cos3712_oct_12memo.pdf","text":"1 COS3712 OCT/NOV 2012 [TURN OVER] COS3712 OCT / Nov 2012 COMPUTER SCIENCE COMPUTER GRAPHICS Duration: 2 hours Total: 70 marks Examiners: First: Mr L Aron Second: Mr C Dongmo External: Prof P Marais (University of Cape Town) ........................................................................................................... ........................................................................................................... MEMORANDUM 2 COS3712 OCT/NOV 2012 [TURN OVER] QUESTION 1 [12] 1.1 A real-time graphics program can use a single frame buffer for rendering polygons, clearing the buffer, and repeating the process. Why do we usually use two buffers instead? [3] Double buffering is a technique for tricking the eye into seeing smooth animation of rendered scenes. One buffer (the front buffer) is displayed on the screen while the other (the back buffer) is currently being drawn to. During the vertical retrace period the buffers are swapped. Double buffering is needed because of shearing or flickering that occurs when we draw to the colour buffer that is currently being displayed.The front buffer is displayed while the application renders into the back buffer. When the application completes rendering to the back buffer, it requests the graphics display hardware to swap the roles of the buffers, causing the back buffer to now be displayed, and the previous front buffer to become the new back buffer. 1.2 Briefly explain what the accumulation buffer is and what it is used for in computer graphics. [3] (Angel, Section 8.12): The accumulation buffer has the same spatial resolution as the frame buffer, but has greater depth resolution. We can think of it as a special type of color buffer whose components are stored as floating-point numbers. We can use the additional resolution to render successive images into one location while retaining numerical accuracy. The accumulation buffer can be used for a variety of operations that involve combining multiple images. (Angel, Section 8.12.1): One of the most important uses of the accumulation buffer is for antialiasing. Rather than antialiasing individual lines and polygons, we can antialias an entire scene using the accumulation buffer. (Angel Section 8.12.3): Bump mapping and embossing 1.3 Define the term View Volume with reference to both perspective and orthogonal views. Provide the OpenGL functions that used to define the respective view volumes. [6] The view volume is analogous to the volume that a real camera would see through its lens (except that it is also limited in distance from the front and back). It is a section of 3D space that is visible from the camera or viewer between two distances. When using orthogonal (or parallel) projection, the view volume is rectangular. In OpenGL, an orthographic projection is defined with the function call glOrtho(left, right, bottom, top, near, far); 3 COS3712 OCT/NOV 2012 [TURN OVER] When using perspective projection, the view volume is a frustum and has a truncated pyramid shape. In OpenGL, a perspective projection is defined with the function call glFrustum(xmin, xmax, ymin, ymax, near, far); or gluPerspective(fovy, aspect, near, far); QUESTION 2 :Transformations [8] 2.1 Transformations are often carried out using a homogeneous co-ordinate representation. Give reasons as to why this representation is used? [2] When points and vectors are represented using 3-dimensional column matrices one cannot distinguish between a point and a vector, with homogeneous coordinates we can make this distinction. A matrix multiplication in 3-dimensions cannot represent a change in frames, while this can be done using homogeneous coordinates. All affine transformations can be represented as matrix multiplications in homogeneous coordinates. Less arithmetic work is involved when using homogeneous coordinates. The uniform representation of all affine transformations makes carrying out successive transformations far easier than in 3 dimensional space. Modern hardware implements homogeneous coordinates operations directly, using parallelism to achieve high speed calculations. they allow us to express perspective projection as a 4 x 4 projection matrix 2.2 Consider the following 4x4 matrices: A B C D E F Which of matrices reflect the following (write down the correct letter): 2.2.1 Identity matrix (no effect) D 2.2.2 Uniform scaling F 2.2.3 Non-uniform scaling E 2.2.4 Reflection C 2.2.5 Rotation about z A 2.2.6 Rotation B [6] 4 COS3712 OCT/NOV 2012 [TURN OVER] QUESTION 3 : Hidden surface removal [12] 3.1 Differentiate between depthsort and z-buffer algorithms for hidden surface removal. [6] 3.2 Briefly describe, with any appropriate equations, the algorithm for removing (or “culling\") backfacing polygons. Assume that the normal points out from the visible side of the polygon. [6] v θ n If θ is the angle between the normal and the viewer then the polygon is facing forward iff -90≤ θ≤90 or cos θ ≥ 0, using dot product n.v ≥ 0 Test can be simplified since it is applied after transformation to normalized device coordinates. All views are orthographic with DOP along z-axis Hence in homogenous coordinates v = [0010] thus if the polygon is on the surface Ax + by + cz + d =0 in normalized device coordinates then sign of c determines whether polygon is front or back facing. Z-buffer – rasterization is done polygon by polygon. For each fragment on the polygon corresponding to the intersection of the polygon with a ray (from the centre of projection) through a pixel we compute the depth from the COP. If depth is greater than depth currently stored in z-buffer it is ignored else z-buffer is updated and colour buffer is updated with new colour for fragment. Depth sort - all polygons are rendered with hidden surface removal as a consequence of back to front rendering of polygons. Depth sort orders the polygons by how far away from the viewer their maximum z-value(z-extent) is. If the minimum depth – the z-value – of a given polygon is greater than the maximum depth of the polygon behind the one of interest, we can render the polygons back to front 5 COS3712 OCT/NOV 2012 [TURN OVER] QUESTION 4 : Lighting and Shading [12] 4.1 The shading intensity at any given point p on a surface is, in general, comprised of three contributions, each of which corresponds to a distinct physical phenomenon. List and describe all three, stating how they are computed in terms of the following vectors: n - the normal at point p [9] v - from p to viewer l – from p to light source r – reflection of ray from l Ambient (a constant amount of light gets added to the scene, independent of the position or distance of the light source) I = kL where k is the reflection coefficient , L is ambient term Diffuse (a light hits a surface and gets scattered equally into all directions - dull or matt surfaces that appear equally bright from all viewing angles) I = k (l.n) L Specular (an incoming light gets reflected in a particular direction) I = k L Max((r.v)α ,0) α is the shininess coefficient 4.2 Describe the difference between Gouraud and Phong shading. [3] In smooth shading colour per vertex is calculated using vertex normals and then this colour is interpolated across the polygon. In Phong shading, the normals at the vertices are interpolated across the surface of the polygon. The lighting model is then applied at every point of within the polygon. QUESTION 5 : Discrete Techniques [12] 5.1 Describe the difference between bump mapping and texture mapping. [3] Whereas texture maps give detail by mapping patterns onto surfaces, bump maps distort the normal vectors during the shading process to make the surface appear to have small variations in shape, like bumps or depressions. 5.2 What does the value at each pixel in a bump map correspond to? How is this data used in rendering? [3] Like the texture map that maps a pattern (of colours) to a surface, we can create a mapping that alters the normals in the polygon so the shading model can create the effect of a bumpy surface. This is called a bump map, and like Phong shading the normal for each individual pixel is computed separately. Here the pixel normal is computed as the normal from Phong shading plus a normal computed from the bump map by the gradient of the colour. The colour of each individual pixel is then computed from the standard lighting model. The colours produced by the shading then give the 6 COS3712 OCT/NOV 2012 [TURN OVER] appearance of bumps on the surface. Note that the bump map itself can be defined simply as a 2D image where the height of each point is defined by the colour; this is called a height field. 5.3 Consider the texture map with U,V coordinates in the diagram on the left. Draw the approximate mapping if the square on right were textured using the above image. [2] Answer: 5.4 Discuss the difference between the RGB colour model and the indexed colour model with respect to the depth of the frame (colour) buffer. [4] Answer: In both models, the number of colours that can be displayed depends on the depth of the frame (colour) buffer. The RGB model is used when a lot of memory is available, eg 12 or 24 bits per pixel. These bits are divided into three groups, representing the intensity of red, green and blue at the pixel, respectively. The RGB model becomes unsuitable when the depth is small, because shades become too distinct/discreet. The indexed colour model is used where memory in the colour buffer is limited. The bits per pixel are used to index into a colour-lookup table where any shades of any colours can be specified (depending only on the colours that the monitor can show). 7 COS3712 OCT/NOV 2012 [TURN OVER] QUESTION 6 [6] Using diagrams describe briefly the Liang-Barsky clipping algorithm. [6] Two marks for diagram 8 COS3712 OCT/NOV 2012 [TURN OVER] QUESTION 7 [8] Consider the following simple animation program in OpenGL: /* This program demonstrates the use of double buffering for * flicker-free animation of the spinning motion of a square * and is adapted from the code provided in * E. Angel, Interactive Computer Graphics */ #include <GL/glut.h> //Line 1 #include <stdlib.h> //Line 2 GLfloat spin = 0.0; //Line 3 GLfloat size = 1.0; //Line 4 void display() { //Line 5 glClear(GL_COLOR_BUFFER_BIT); //Line 6 glColor3f(1, 0, 0); //Line 7 glRectf(-0.5, -0.5, 0.5, 0.5); //Line 8 glutSwapBuffers(); //Line 9 } void idle() { //Line 10 spin = spin + 1.0; //Line 11 if (spin > 360.0) //Line 12 spin = spin - 360.0; //Line 13 glLoadIdentity(); //Line 14 glRotatef(spin, 0.0, 0.0, 1.0); //Line 15 glScalef(size, size, 1.0); //Line 16 Suppose we have a line segment defined by two endpoints p (x1, y1) q(x1, y1). The parametric equation of the line segment gives x-values and y-values for every point in terms of a parameter α that ranges from 0 to 1. x(α) = (1 -α) x1 + α x2 y(α) = (1- α) y1 + α y2 There are four points where line intersects side of windows tB tL tT tR we can order these points and then determine where clipping needs to take place. If for example tL > tR , this implies that the line must be rejected as it falls outside the window. To use this strategy effectively we need to avoid computing intersections until they are needed. Many lines can be rejected before all four intersections are known. 9 COS3712 OCT/NOV 2012 [TURN OVER] glutPostRedisplay(); //Line 17 } void reshape(int w, int h) { //Line 18 if (w < h) //Line 19 glViewport(0, (h-w)/2, w, w); //Line 20 else //Line 21 glViewport((w-h)/2, 0, h, h); //Line 22 } int main(int argc, char** argv) { //Line 23 glutInit(&argc,argv); //Line 24 glutInitDisplayMode(GLUT_DOUBLE); //Line 25 glutCreateWindow(\"spinning square\"); //Line 26 glClearColor(1, 1, 1, 1); //Line 27 glutDisplayFunc(display); //Line 28 glutReshapeFunc(reshape); //Line 29 glutIdleFunc(idle); //Line 30 glutMainLoop(); //Line 31 return 0; //Line 32 } 7.1 How does the program rotate the square if it is always defined with the same co- ordinates (in line 8)? [2] Answer: The idle function changes variable spin (in lines 11, 12, and 14) and then transforms the objects in the graphics pipeline by multiplying them by an appropriate rotation matrix (in lines 14 and 15). Rotation is about the z-axis, by spin degrees. 7.2 What does line 16 do? [1] Answer: It applies a scaling transformation matrix to the objects in the graphics pipeline, adjusting the x and y coordinates only. 7.3 Answer the following questions with respect to the reshape function (lines 18 to 22) passed to glutReshapeFunc (in line 29): 7.3.1 What is the purpose of the reshape function? [1] 7.3.2 Why is the matrix mode switched from GL_PROJECTION to GL_MODELVIEW? [2] Answer: 7.3.1 To allow the user to change the size of the window (by maximising the window or dragging its borders) so that the contents are still displayed in a 10 COS3712 OCT/NOV 2012 [TURN OVER] meaningful way. 7.3.2 The gluOrtho function operates on the projection matrix (not the module-view matrix), so we have to swap to the projection matrix mode and back. (Most other transformations take place in model-view matrix mode.) 7.4 Identify the lines of the program that deal with double buffering and explain them briefly. [2] Answer: Line 25: The use of both front and back (colour) buffers is engaged. Line 9: Here the contents of the back buffer are copied into the front buffer.","libVersion":"0.2.3","langs":""}