{"path":"Subjects/COS3751 - Techniques of Artificial Intelligence/Telegram Notes/Materials/COS3751 Exam Preparation Notes.pdf","text":"© Sylvester Kuisis (45736901) & Cornelis Dubbelman (48269328) COS3751 EXAM PREPARATION NOTES STATE SPACES Agent function: maps any given percept sequence to an action. Agent program: a concrete implementation of a function. Fully observable: agent’s sensors have access to complete state of the environment at any given time. Partially observable: agent’s sensors have access to partial state of the environment at any given time. Stochastic environment: the next state of the environment is not determined by the current state or the action executed by the agent. Deterministic environment: the next state of the environment is completely determined by the current state and the action executed by the agent. Discrete environment: state-space is finite, and for each state, there are only finitely many percepts to be perceived, and finitely many actions to choose from (e.g. chess). Continuous environment: infinitely many distinct states, infinitely many percepts, and infinitely many actions to choose from at any state (e.g. taxi driving) INFORMED SEARCH For heuristics: An admissible heuristic is one that never overestimates the true cost of reaching the next goal. If a heuristic is consistent, the algorithm is guaranteed to be optimal, and complete when the branching factor is finite. For every node in the search tree, the following inequality must hold: h(n) <= c(n, a, n’) + h(n’) (where c(n, a n’) is the PATH-cost from the current node to the destination node, and n’ is the destination node) Useful synonyms: consider -> generate visit -> expand ALPHA-BETA SEARCH For determining whether or not to do a cut: On a MAX node: if v >= β, perform a β-cut On a MIN node: if v <= α, perform a α-cut For updating alpha-beta values: On a MAX node: compare v to current α, if v > α, replace it. On a MIN node: compare v to current β, if v < β, replace it. When passing up minimax values (v): When passing up to a MAX node: if vpassing is GREATER than vcurrent, replace it. When passing up to a MIN node: if vpassing is LESS than vcurrent, replace it. © Sylvester Kuisis (45736901) & Cornelis Dubbelman (48269328) Why ordering is important: The right order of nodes can result in pruning larger section of the search tree earlier. This benefits alpha/beta searching because it reduces the runtime of the algorithm. CONSTRAINT SATISFACTION PROBLEMS Arc consistency: A variable is arc-consistent of all the values in the variable’s domain satisfies the variable’s binary constraints. Forward checking: For a variable X, for each unassigned variable Y that is connected to X by a constraint, delete from Y’s domain any value that is inconsistent with the value chosen for X (establishing arc consistency). LOGIC Definite clause: a disjunction of literals of which exactly one is positive. Horn clause: a disjunction of literals of which at most one is positive. Why Horn clauses are useful: Deciding entailment with Horn clauses can be done in time that is linear in the size of the knowledge base (backward- and forward-chaining). Conjunctive Normal Form: every sentence of first-order logic can be converted into an inferentially equivalent CNF sentence. This gives us a basis for doing proofs by contradiction on the CNF sentences. Refutation-complete: if a set of sentences is unsatisfiable, then resolution will always be able to derive a contradiction. For forward-chaining, convert to implication form. For resolution refutation, convert to CNF/clausal form. When converting implication form to CNF (and vice versa): P -> Q becomes ¬P v Q P <-> Q becomes (P v ¬Q) ^ (¬P v Q) De Morgan: ¬(P v Q) becomes (¬P) ^ (¬Q) ¬(P ^ Q) becomes (¬P) v (¬Q) ¬¬P becomes P ¬(∀xP(x)) becomes ∃x¬P(x) ¬(∃xP(x)) becomes ∀xP(x) Moving OR’s inward: P v (Q ^ R) becomes (P v Q) ^ (P v R) MACHINE LEARNING Unsupervised learning: the agent learns patterns in the input even though no explicit feedback is supplied. Reinforcement learning: the agent learns from a series of reinforcements—rewards or punishments. © Sylvester Kuisis (45736901) & Cornelis Dubbelman (48269328) Supervised learning: the agent observes some example input–output pairs and learns a function that maps from input to output. Ockham’s Razor: prefer the simplest hypothesis consistent with the data. Entropy: the measure of the uncertainty of a random variable. Information gain: the expected reduction in entropy of a characteristic/attribute. Solutions to over-fitting: - decision tree pruning (eliminate irrelevant nodes). - limit the number of input attributes & the size of the hypothesis space. - increase the number of training examples.","libVersion":"0.2.3","langs":""}