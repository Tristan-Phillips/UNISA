{"path":"Subjects/COS3751 - Techniques of Artificial Intelligence/Telegram Notes/Past Exams/june 2018 memo.pdf","text":"Cos 3751 June 2018 Memorandum Question 1 a) A rational agent is an agent that Acts in order to achieve the best outcome, or where there is uncertainty, the best-expected outcome. Conceptually speaking, it does the “right thing”. b) A game of nim is Fully observable because player sensors give it access to the complete state of the environment at each point in time. c) i. Elements of the problem are the initial state, actions and transition model implicitly defines it, together ii. Initial State={ A1,A,2,A3,C1,C2, B} A1 representing Adult 1 A2 representing Adult 2 A3 representing Aduct 3 C1 representing Child 1 C2 representiong Child 2 B representing boat Question2 a) Breadth-First Search (Shortest First) Breadth-first search is a simple strategy in which the root node is expanded first, then all the successors of the root node are expanded next, then their successors, and so on. In general, all the nodes are expanded at a given depth in the search tree before any nodes at the next level are expanded. When applying it to a tree or graph, it expands the node with the shortest path from the frontier. Breadth First Search (BFS) algorithm with a branching factor of β at depth δ: o Has a complexity of O(β δ ) o This is an exponential growth Space/Complexity gets large quickly Eg =b^d =6^7 =279,94 Breadth-first-search uses a FIFO queue. Depth -first search always expands the deepest node in the current frontier of the search tree. Depth-first search uses a LIFO queue. The. search proceeds immediately to the deepest level of the search tree, where the nodes have no successors. As those nodes are expanded, they are dropped from the frontier, so then the search \"backs up\" to the next deepest node that still has unexplored successors. The space complexity is linear Eg bd 6x7=42 DFS has a clear space complexity advantage over BFS when a tree search is performed b) i. The property of heuristic depends on the conditions on h(n) which is called the heuristic function, and the basic condition of the heuristic is admissible ii. An admissible heuristic Never over estimates the cost to reach the goal, it is optimistic c) a) I,B, E, K,H,C ii) the frontier is C Question 3 i. A=5, B=5, C=2 , D=50 ii. B (α= -infinity, β=5) A (α= 5, β=+infinity) C (α= 5, β=+infinity) H (α= 5, β=+infinity) iii. Yes because a cu was performed b) The difference between the global and local maxima is that Local minima/maxima are local solutions in the search space that are optimal solutions to the problem in the immediate neighborhood (within a given range, there isn’t a solution that beats it). A global maxima/minimum is a solution to the problem that is the best solution in the solution space (no other solution beats it). Question 4 a) Forward checking It is a domain reduction technique which establishes arc consistency. (Or: It removes valuesfromthedomainoftheneighboursofthevariableforwhichtheforwardchecking is being done that are inconsistent with its value.) b) Define variables i) Variable X= {Abe, Bongi, Charlie, Dozi, Erica} ii) Dx = {0,1,2,3,4,5} (representing the order that they are picked up.) iii) The constraints are Abe≠Bongi Abe≠ Charlie Bongi ≠Charlie Abe≠Dozi Dozi≠Erica iv) Diagram Charlie Abe Dozi Eric Bongi v) 2 shuttles are needed: a: shuttle picks up Abe b: shuttle picks up Charlie c: shuttle picks up Dozi d: 2nd shuttle needed. Pick up Bongi. e: Pick up Erica Question 5 a) ¬C Ѵ A ¬AѴ BѴ C ¬ B Ѵ A ¬ A C→A (C→ B) Ѵ C B→ A ¬ A B assumption refute the conclusion ¬ B b) i. Poisonous x ii. Poison x→ ¬eats (Asale,x) iii. Eats (kapeni,X) → eats (Chiwa,x) iv. Eats (Chiwa,toadstools) ٨ sick from eating (Chiwa, toadstools) v. Eats (x,y) ٨ sick from eating (x,y) → poisonous Question 6 a) Entropy = 2/5 =0,4 =0,97 check 0,4 on the entropy table b) Calculating the information gain Fever entropy no =2/5 *[0:2] =0 Fever entropy yes=3/5*[2:1] =0,67 check entropy =0.91 then *0.6 =0,546 Information gain = Entropy Data-Entropy fever 0,97-0,546=0,424 Other symptoms entropy yes =4/5 * [2:2]=0,5 check entropy =1 then *0,8=0,8 Entropy no=1/5*[0:1]=0 Information gain = Entropy Data-Entropy other Symptoms 0,97-0,8=0,17 Abnormal entropy yes=2/5*[1:1]=0,5 check entropy =1 then * 0,4=0,4 No 3/5*[1:2]=0,33 check entropy=0.91 then * 0,6 =0,546 Total entropy=0,546+0,4=0,946 Information gain = Entropy Data-Entropy Abnomal 0,97-0,946=0,024 c) Fever ,because it has the higher information gain d) Other symptoms e) The decision tree Fv er At At Ab Ab Ab Ab 0 0 0 1 1","libVersion":"0.2.3","langs":""}