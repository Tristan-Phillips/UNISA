{"path":"UNISA/98906 - BSc Science in Computing/INF3703 - Databases II/Unsorted/INF 3703/Additional/inf3703-study-notesmeh.pdf","text":"Downloaded by Tristan Phillips (tristan@phillipsfamily.co.za) lOMoARcPSD|12886293 INF3703 Study Notes Chapter 10 \u0000 An information system is composed of people, hardware, software, databases, applications and procedures \u0000 Systems analysis is the process that establishes the need for and the scope of an information system \u0000 Systems development – the process of creating an information system \u0000 The performance of an information system depends on DB design and implementation; app design and implementation; and administrative procedures \u0000 DB Design objectives – create complete, normalised, non-redundant and fully integrated conceptual, physical and logical DB models SDLC Phases (PADIM) Planning (initial assessment and feasibility study) \u0000 Analysis (user requirements, existing system evaluation and logical system design) o Thorough audit of user requirements o Development of data models \u0000 Detailed systems design( detailed system spec) \u0000 Implementation (coding, testing and debugging, installation, fine-tuning) \u0000 Maintenance (Evaluation, Maintenance and Enhancement) o Corrective changes – in response to errors o Adaptive changes – changing business environment o Perfective maintenance –system enhancement Database Lifecycle (IDITOM) \u0000 Database Initial Study o Analyse the company situation (company objectives, operations and structure) o Define problems and constraints o Define objectives o Define scope and boundaries \u0000 Design (most critical phase) o 2 views of data – business view and designer’s view Database Design Flow a) Conceptual Design (DBMS-independent) - create an abstract DB structure that represents real-world objects DB analysis and requirements (user views, output and transaction-processing requirements) o ER modelling and normalisation (entities, attributes, relations, ER diagrams, table normalisation) o Data model verification – identify main processes, validate reports, queries, views, integrity, sharing and security Downloaded by Tristan Phillips (tristan@phillipsfamily.co.za) lOMoARcPSD|12886293 o Distributed DB design – define location of tables, access requirements and fragmentation strategy b) DBMS Software selection (costs, features, underlying model, portability and hardware requirements) c) Logical design (DBMS-dependent) – Translate conceptual model into definitions for tables, views, etc. Objective – to map the conceptual model into a logical model which can be implemented on an RDBMS. Create logical data model o Validate logical data model using normalisation o Assign and validate constraints o Merge various local models o Review logical model with user d) Physical Design (Hardware-dependent) – define storage structure and access paths o Analyse data and usage o Translated logical model relations into tables o Define indexes o Define user views o Estimate data storage requirements o Determine DB security for users \u0000 Implementation and Loading (create DB, Create views, relations, assign rights, load DB) o Also includes security, performance, integrity, standards; and backup and recovery Oracle encryption feature – Transparent Data Encryption – can decrypt DB columns. Circuit-level gateway blocks all incoming connections to any host but itself \u0000 Testing and Evaluation (testing and fine-tuning, modify physical design, modify logical design, upgrade DBMS) \u0000 Operation \u0000 Maintenance and Evolution o Preventative, corrective, adaptive, access permission assignment, DB stats, security audits and periodic summaries and reports DB Design Strategies (influenced by scope and size of system, company operations, mgmt. style and structure) \u0000 Top-Down – Identify entities/data sets and the elements within the entities – ER Models \u0000 Bottom-Up – Identify the elements and group then into entities/data sets – Normalisation Centralised vs. Decentralised DB Design Centralised \u0000 Normally suited for smaller companies or small databases where 1 DBA designer can define the problem, create the conceptual model, verify conceptual model, etc. \u0000 Illustration – (Conceptual model – (conceptual model verification (user views, system processes, data constraints)), data dictionary) Decentralised Downloaded by Tristan Phillips (tristan@phillipsfamily.co.za) lOMoARcPSD|12886293 \u0000 When the data component of a system requires many entities with complex relations on which complex operations are performed. \u0000 Teams of designers independently create the conceptual model for a particular component and each conceptual model is merged to form one overall conceptual model \u0000 Illustration – (Data Component – (conceptual models, verification, aggregation, conceptual model, data dictionary) DBA Roles \u0000 DA (Information Resource Manager) has higher authority than DBA DBA Managerial Role (Controlling and Planning) End-User Support – gather user reqs, build end-use confidence, resolve conflicts and problems, find solutions to information needs, ensure quality and integrity of applications and data; and manage the training and support of end users \u0000 Data backup and recovery \u0000 Data distribution and use – Ensure that data is distributed to the right people at the right time \u0000 Data security, privacy and integrity \u0000 \u0000 User Access Management o Define users, groups, roles o Manage user access privileges o Assign passwords o Control physical access o Define views o DBMS usage monitoring, access control DBA Managerial Roles include: \u0000 End-User Support \u0000 Policies, Procedures and Standards \u0000 Data security, Privacy and Integrity \u0000 Data Backup and Recovery \u0000 Data Distribution and Use Technical DBA Roles \u0000 Evaluating, Selecting and Installing the DBMS and utilities \u0000 Designing and Implementation of databases and applications \u0000 Testing and Evaluation of Databases and Applications \u0000 Operate the DBMS, Utilities and Applications \u0000 Training and Supporting Users \u0000 Maintenance of the DBMS, Utilities and Applications Critical success criteria for an Information Systems Architecture: \u0000 Management commitment \u0000 End-use involvement Downloaded by Tristan Phillips (tristan@phillipsfamily.co.za) lOMoARcPSD|12886293 \u0000 Thorough company situation analysis \u0000 Defined standards \u0000 Training \u0000 Small pilot project Chapter 11 DB Design stages – Conceptual, Logical and physical design \u0000 Module cohesivity – Must be high. High cohesivity indicates a strong relationship amongst entities \u0000 Module coupling - must be low. Indicates how independent modules are from each other CODD’S relational DB rules \u0000 Logical independence – if the logical structure of the database changes, the user should not be affected in any away \u0000 Physical independence – If the physical structure of the database changes, the user should not be affected in any way Conceptual Design Steps \u0000 Data analysis and requirements (info needs, users, sources, constitution) o Sources of information (developing and gathering use data views, observing current system and interface with the systems design team) o Business rules must be easy to understand and widely disseminated o Description of operations – a doc that provides precise, detailed, up-to-date and thoroughly reviewed description of business activities o Business Rules have the following benefits: \u0000 Communication tool between designers and users \u0000 They allow the designer to understand the nature of, role and scope of user data \u0000 They allow the designer to understand business processes \u0000 They allow the designer to develop appropriate relationship participation rules and foreign key constraints \u0000 Entity relationship modelling and normalisation (standard t be used for documentation must be communicated) o Identify, analyse and refine business rules o Identify main entities o Define relationships amongst entities o Define attributes, primary keys and foreign keys for each entity o Normalise the entities o Complete the initial ER diagram o Verify the model with end-users o Modify the ER diagram based on user feedback Downloaded by Tristan Phillips (tristan@phillipsfamily.co.za) lOMoARcPSD|12886293 \u0000 Data Model Verification o Identify the model’s central entity o Identify each module and it’s components o Identify each module’s transaction requirements o Verify all processes against the ER model o Make changes as suggested o Repeat steps 2 to 5 for all modules \u0000 Distributed Database Design (spreading the DB across multiple distributed DBs) Logical Database Design \u0000 Create the logical model o Create relations for strong entities o Create relations for weak entities o Map multivalued attributes o Map binary relations o Map ternary relations o Map supertype andsubtype relationships \u0000 Validate the model using normalisation o Normalise all entities to @least 3NF \u0000 Assign and validate integrity constraints (domain, entity and referential integrity) \u0000 Merge local models constructed for different parts together \u0000 Review logical model with users Physical Database Design \u0000 Analyse data volume and database usage (shown in composite usage or transaction usage map) o Identify most frequent and critical transactions o Determine which entities will be accessed by critical transactions \u0000 Translate logical relations into tables \u0000 Determine suitable file organisation o Heap Files – contain randomly ordered records. Used only when a large quantity of data needs to be added to a database for the first time. Slow searches and impractical. o Sequential file organisations – Records are usually sequenced based on primary key. Insertions are costly, deletions lead to unused space. Searches are very slow o Indexed file organisations – files that are sorted based on one more fields. Index is created to quickly locate records. Faster searches, access, aggregation. Indexes are logically and physically independent of the data in the associated table o Hashed file organisation – Uses a hashing algo to map primary key value to a specific record. Disadvantage – the uniqueness of the hash cannot be guaranteed o Types of indexes Downloaded by Tristan Phillips (tristan@phillipsfamily.co.za) lOMoARcPSD|12886293 \u0000 Primary index – placed on unique fields/primary key. One primary index per file. \u0000 Secondary index – can be placed on any field in the file that is unordered \u0000 Multi-level index – used when the index becomes large and is split into separate indexes Indexes can be sparse or dense. Dense indexes are faster. \u0000 Balanced trees (B-Trees) – more efficient in storing ordered data. Left node < parent node, parent node < right node \u0000 Bitmap indexes – usually applied to attributes which are sparse in their given domain. 2 dimensional arrays. Used in the following instances • Column has low cardinality • The table is not used for data manipulation • Specific queries reference a number of low cardinality values \u0000 Join Indexes (Bitmap Join Indexes) – used in data warehousing and applies to columns from 2 or more tables whose value come from the same domain. \u0000 Creating indexes o Primary Index - CREATE UNIQUE INDEX <INDEX_NAME> ON <TABLE>(<CLOUMN_NAME>) o Secondary Index – CREATE INDEX <INDEX_NAME> ON <TABLE>(<COLUMN_NAME>) \u0000 Define User Views \u0000 Estimate Data Storage Requirements o Estimate size of each row by adding the length of each data type o Estimate number of rows taking into consideration growth o Multiply the size by the estimated number of rows \u0000 Determine database security for users o Two sets of privileges (system and object) o Systems allows the executing of DDL commands like CREATE TABLE o Object allows the executing of DML commands (insert, update) o Roles (CREATE ROLE <Role_Name>, GRANT SELECT ON CUSTOMERS TO <Role Name>, GRANT <Role_Name> to <user_to_be_added_to_role> Chapter 12 \u0000 Concurrency control – managing concurrent transactions \u0000 Transaction – logical unit of work that must be entirely completed or entirely aborted \u0000 Consistent database state – a state in which all data integrity constraints are satisfied \u0000 Transactions must satisfy (ACIDS) – Atomicity, Consistency, Isolation, Durability and Serialisation o Atomicity – All transactions must be completed, otherwise the transaction will be aborted o Consistency – the permanence of the database’s consistent state o Isolation – Data used during the executing of a transaction cannot be used by a second transaction until the first one is completed o Durability – Once transactions are committed, they cannot be undone o Serialisability – Ensures that the concurrent execution of several transactions yields consistent results Downloaded by Tristan Phillips (tristan@phillipsfamily.co.za) lOMoARcPSD|12886293 \u0000 Scheduler – special DBMS program that establishes the order in which transactions are executed. Satisfies isolation and serialisation \u0000 A lock guarantees exclusive use of a data item to a current transaction. Controlled by a lock manager. \u0000 Lock levels – database, table, page or field(attribute) \u0000 Database lock – suitable for batch-processing but not for multi-user databases \u0000 Table lock – locks all rows in a table. Causes bottlenecks. Not suitable for multi-user databases \u0000 Page-level lock – an entire diskpage is locked. Most frequent locking option. \u0000 Row-level lock – Less restrictive. Improves availability of data. Adds management overhead. \u0000 Field-level lock – Allows concurrent transactions to access the same row as long as they require access to different fields in the row. Most flexible but rarely implemented due to overhead \u0000 Lock Types o Binary – either locked or unlocked. Too restrictive o Exclusive lock – Access is reserved for the transaction using the data. No other access (read or write) o Shared locks – concurrent transactions are granted read access as long as the transaction is read-only \u0000 2-Phase locking – define how transactions acquire and relinquish locks. Ensures serialisation but cannot prevent deadlocks. o Growing phase – transaction acquires all required locks without unlocking any data. Transaction is locked once all locks have been acquired o Shrinking phase – Transaction releases all locks and cannot obtain a new lock \u0000 Deadlock – When 2 transactions wait indefinitely for each one to release a lock – deadly embrace Deadlock Control techniques: Deadlock prevention – a transaction requiting a lock is aborted when there is a possibility for a deadlock \u0000 Deadlock detection – DBMS periodically tests the database for deadlocks, if found, one transaction is aborted \u0000 Deadlock avoidance – A transaction must obtain all the locks it requires before executing Database Recovery \u0000 Write-ahead logging – transaction logs are written before execution \u0000 Redundant transaction logs – redundant copies of logs are kept Wait/Die or Wound/Wait Schemes Wait/Die \u0000 Older transaction requesting lock waits until newer transactions releases the lock \u0000 Younger transaction requesting lock dies and rescheduled Wound/Die Downloaded by Tristan Phillips (tristan@phillipsfamily.co.za) lOMoARcPSD|12886293 \u0000 Older transaction requesting lock wounds the newer transaction. Newer transaction is rescheduled \u0000 Newer transaction requesting lock waits until the older transaction releases the lock Database Recovery using the deferred write/update process: \u0000 Identify the last checkpoint in the DB (when the transactions were last saved to a physical disk) \u0000 Transactions saved before the checkpoint need not be recovered \u0000 For transactions committed after the checkpoint, the DBMS uses transaction logs to redo the transactions in ascending order (oldest to newest) \u0000 Transactions that were rolled back or not committed after the checkpoint need not be restored Database Recovery using the write through/immediate update process: \u0000 Identify the last checkpoint \u0000 Transactions saved before the checkpoint need not be recovered \u0000 For transactions committed after the checkpoint, the DBMS uses transaction logs to redo the transactions in ascending order (oldest to newest) \u0000 Transactions that had a rollback or not committed after the last checkpoint are rolled back starting with the newest transactions Chapter 13 \u0000 SQL Performance Tuning – client-side \u0000 DBMS Performance Tuning – server-side \u0000 Table space – logical grouping of several data files that store data with similar characteristics \u0000 Data cache/buffer cache – reserved memory space that stores most recently accessed data blocks in RAM \u0000 SQL/Procedure cache – cache for recently executed SQL statements Create indexes in the following situations: \u0000 Single attributes used in WHERE, HAVING, ORDER BY clauses \u0000 Data sparsity for a column is high \u0000 On join columns other than the PK/FK Some guidance Numeric field comparisons are faster than character, date and NULL comparisons \u0000 Equality comparisons are faster than the inequality comparison \u0000 When using AND conditions, write the condition which is likely to be false first \u0000 When using multiple OR conditions, write the statement which is likely to be false first Downloaded by Tristan Phillips (tristan@phillipsfamily.co.za) lOMoARcPSD|12886293 Chapter 14 – Distributed Databases Changes that affected database development: \u0000 Globalisation of business operations \u0000 Customer and market demands favoured on-demand transaction style (web-based) \u0000 Rapid social and technological changes \u0000 Data realms are converging in the digital world Problems with centralised databases \u0000 Performance degradation as remote locations increase \u0000 High costs of maintaining centralised mainframe systems \u0000 Reliability problems create by SPOF at the central site \u0000 Scalability problems associated with physical limits – space, power, temperature, HVAC \u0000 Organisational rigidity imposed by the database – not agile Advantages of distributed Databases \u0000 Data is located near the greatest demand site \u0000 Faster data access \u0000 Faster processing \u0000 Growth facilitation \u0000 Improved communications \u0000 Reduced operating costs \u0000 User friendly interface \u0000 Less risk of SPOF \u0000 Processor independence Disadvantages of distributed Databases \u0000 Complexity of management and control \u0000 Potential reduction of security \u0000 Lack of standards \u0000 Increased storage requirements \u0000 Increased training costs \u0000 Increased costs \u0000 Distributed processing – database’s logical processing is shared among 2 or more physically independent sites. Uses only a single site database \u0000 Distributed database – stored a logically related database on 2 or more physically independent sites \u0000 Single site processing, single site data – processing is done on a central system. End users connect using dumb terminals \u0000 Multi-site processing single site data – processing at multiple sites, sharing a single repository Downloaded by Tristan Phillips (tristan@phillipsfamily.co.za) lOMoARcPSD|12886293 \u0000 Multi-site processing, multi-site data – Fully distributed databases with distributed processing \u0000 Homogeneous DDMBMS – Supports only one type (platform) of DBMS to be distributed \u0000 Heterogeneous DDBMS – supports many types (platforms) of DBMSes to be distributed. (With restrictions) DDBMS Transparency Features \u0000 Distribution transparency o Fragmentation transparency – highest form of transparency. User or programmer does not need to know that a database is fragmented o Location fragmentation – The user or programmer specifies the fragment names but doesn’t need to know where the data is located o Local mapping transparency – The end user or programmer must specify both the fragment names and their locations. \u0000 Transaction Transparency – a transaction can update at several sites. Ensures integrity and consistency. \u0000 Failure Transparency – system will continue to operate in the case of a node failure \u0000 Performance Transparency – system will perform if it is centralised \u0000 Heterogeneity Transparency – allows integration of multiple Data Fragmentation \u0000 Horizontal fragmentation – division of rows into subsets which are fragmented across nodes \u0000 Vertical fragmentation – division of relations into attribute subsets which are fragmented across nodes \u0000 Mixed fragmentation – a combination of horizontal and vertical fragmentation \u0000 Mutual consistency rule – requires that all copies of fragmented data are identical CAP Theorem In a highly distributed database there are three desired properties: Consistency, Availability and Partition Tolerance. Although it must be stated that any distributed database can only achieve two of these properties at a time. \u0000 Consistency – plays a big role. All nodes should see the data at the same time which means that replication must be in real-time. \u0000 Availability – A request must always be fulfilled by a database \u0000 Partition Tolerance – The system must continue to operate even following a node failure. CODD’s 12 Commandments \u0000 Local site independence, central site independence, Failure independence \u0000 Location transparency, Fragmentation transparency, replication transparency \u0000 Distributed query processing, hardware independence, operating system independence Downloaded by Tristan Phillips (tristan@phillipsfamily.co.za) lOMoARcPSD|12886293 \u0000 Network independence, Database independence Cloud Benefits \u0000 Cost-effectiveness \u0000 Scalability \u0000 Latest software \u0000 Mobile Access Cloud services support the Availability and Partial Tolerance properties of the CAP theorem Chapter 15 – Databases for Decision Support \u0000 Business Intelligence – a comprehensive, cohesive and integrated set of tools and processes used to capture, collect, integrate, store and analyse data with the purpose of generating and presenting information to support business decision-making. \u0000 Business intelligence provides a framework for: o Collecting and storing operational data o Aggregating the operational data into decision support data o Analysing the decision support data to generate information o Making business decisions o Monitoring results to evaluate outcomes of business decisions o Predicting future behaviours and outcomes with a high degree of accuracy \u0000 Master Data Management – Collection of concepts, techniques, and processes for proper identification, definition and management of data elements within an organisation \u0000 MDM’s main objective – a consistent definition of data throughout the organisation \u0000 Governance – the method or process of government Business Intelligence Benefits \u0000 Integrating architecture – integrates various solutions across platforms \u0000 Common user interface for data reporting and analysis \u0000 Common data repository fosters a single version of company data \u0000 Improved organisational performance (added efficiency, reduced waste, increase of the bottom line) Operational and DSS data can be differentiated by timespan, granularity and dimensionality Operational Data DSS Data Timespan Short Long Granularity Low High (drill-down/roll-up) Dimensionality Single focus Many dimensions \u0000 Data Warehouses are Integrated, Subject-oriented, Time-variant and Non-volatile Downloaded by Tristan Phillips (tristan@phillipsfamily.co.za) lOMoARcPSD|12886293 \u0000 Data Mart – small, single-subject data warehouse subset that provides decision support to a small group of people. Only difference between data mart and warehouse is the size and scope of the problem being solved. \u0000 Star Schema – data modelling technique used to map multi-dimensional decision support data into a relational database \u0000 Star schema consists of: o Facts – Numeric measurements that represent a specific business aspect or activity o Dimensions – Qualifying characteristics that provide additional perspectives to a given fact o Attributes – Each dimension table contains attributes o Attribute Hierarchies – Provides a top-down data organisation used for aggregation and drill down/roll-up \u0000 Study star schema diagram on page 764 \u0000 Techniques to enhance Data Warehouse design o Normalising dimensional tables \u0000 Normalising all dimensions into 3NF form. Each dimension table has its own table \u0000 Snowflake schema – type of star schema wherein each dimension table has its own dimension tables o Maintaining multiple fact tables to represent different aggregation levels \u0000 Aggregate tables are pre-computed at the data-loading phase rather than at run-time o Denormalising fact tables \u0000 Improves speed and reduces storage o Partitioning and replicating table \u0000 Partitioning splits a table into subsets of rows or columns and places the subsets close to the client computer to improve access time \u0000 Replication makes a copy of a table and places it in a different Chapter 16 \u0000 API – a set of routines, protocols and tools for building software applications ODBC Components ODBC API – Application through which applications access ODBC functionality \u0000 ODBC Driver manager – managed of database connections \u0000 ODBC Driver – connects directly to the DBMS DAO, RDO and ODBC do not provide support for non-relational data OLE-DB Components Downloaded by Tristan Phillips (tristan@phillipsfamily.co.za) lOMoARcPSD|12886293 \u0000 Consumers – objects (applications/processes) that request and use data \u0000 Providers - manage the connection with a data source and provides data to consumers \u0000 Data providers provide data to other processes \u0000 Service providers provide additional functionality to consumers \u0000 OLE-DB does not support scripting \u0000 Active X Data Objects (ADO) supports scripting Benefits of Internet Technology \u0000 Hardware and software independence \u0000 Common and simple user interface \u0000 Location independence \u0000 Rapid development at manageable costs \u0000 Server-side extension (Wweb-to-Database middleware) – program that interacts directly with the web server to handle specific types of requests \u0000 Study OLE-DB Diagram on page 809 Web servers use a common way to communicate to backend databases: \u0000 CGI o Main disadvantage – uses an external script that has to be executed for each request. Degrades performance \u0000 API o Implemented as a shared code or DLL. Loaded in memory and available for use on- demand. o Disadvantage – shares memory with web server. An API failure might lead to server failure o Webserver and operating system-dependent Cloud Computing \u0000 A computing model for enabling ubiquous, convenient, on-demand network access to a shared pool of configurable computer resources that can rapidly be provisioned and released with minimal management effort \u0000 Cloud computing removes financial and technological barriers so organisations can leverage database technologies in their business processes with minimal cost Cloud Implementation Types Downloaded by Tristan Phillips (tristan@phillipsfamily.co.za) lOMoARcPSD|12886293 \u0000 Public Cloud – Set up by a 3rd party service provider to provide cloud services to the general public. Most common. \u0000 Private Cloud – setup by an organisation for its sole use. Geographically dispersed organisations \u0000 Community Cloud – built by/for a specific group of organisations that share a common trade Cloud Services Characteristics \u0000 Ubiquous access via internet technologies \u0000 Shared infrastructure \u0000 Lower costs and variable pricing \u0000 Flexible and scalable services \u0000 Dynamic provisioning \u0000 Service orientation \u0000 Managed operations Cloud Service \u0000 SaaS – Google docs, Office Live – Turnkey apps that run in the cloud. Clients cannot change actual application. App shared amongst many customers \u0000 PaaS – provider provides capability to build and deploy consumer created applications in the cloud. Customers can build, deploy and manage applications but cannot change underlying infrastructure. Ms Azue \u0000 IaaS – Providers provide capability for customers to provision resources on demand (servers, storage, databases, processing units, virtualised desktops) Advantages \u0000 Low initial cost \u0000 Scalability \u0000 Ubiquitous computing \u0000 Reliability and performance \u0000 Fast provisioning \u0000 Managed infrastructure Disadvantages \u0000 Security, privacy, compliance \u0000 Hidden costs of implementation \u0000 Difficult and lengthy data migration \u0000 Complex licensing schemes \u0000 Loss of ownership/control \u0000 Difficult integration with internal systems \u0000 Semantic web – a mechanism for describing concepts in a way that computers can actually understand Downloaded by Tristan Phillips (tristan@phillipsfamily.co.za) lOMoARcPSD|12886293 Downloaded by Tristan Phillips (tristan@phillipsfamily.co.za) lOMoARcPSD|12886293","libVersion":"0.2.3","langs":""}