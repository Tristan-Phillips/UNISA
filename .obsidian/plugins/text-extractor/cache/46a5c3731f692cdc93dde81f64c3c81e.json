{"path":"UNISA/98906 - BSc Science in Computing/INF3703 - Databases II/Unsorted/INF3703/INF3703EndOfChapterSummaries.pdf","text":"INF3703 – DATABASES II End of Chapter Summaries Compiled From: Carlos Coronel, Steven Morris, Keeley Crockett and Craig Blewett (2020). DATABASE PRINCIPLES: fundamentals of design, implementation, and Management 3th edition CGMV Chapter 10: Database Development Process • An information system is designed to facilitate the transformation of data into information and to manage both data and information. Thus, the database is a very important part of the information system. Systems analysis is the process that establishes the need for, and the extent of, an information system. Systems development is the process of creating an information system. • The Systems Development Life Cycle (SDLC) traces the history (life cycle) of an application within the information system. The SDLC can be divided into five phases: 1. planning, 2. analysis, 3. detailed systems design, 4. implementation, and 5. maintenance. The SDLC is an iterative rather than a sequential process. CGMV • The Database Life Cycle (DBLC) describes the history of the database within the information system. The DBLC is composed of six phases: 1. database initial study, 2. database design, 3. implementation and loading, 4. testing and evaluation, 5. operation, and CGMV 6. maintenance and evolution. Like the SDLC, the DBLC is iterative rather than sequential. CGMV • Threats to database security include the loss of integrity, confidentiality and availability of data. An organisation should select the relevant security measures and develop a comprehensive data security plan to protect its data. • The conceptual portion of the design may be subject to several variations, based on two basic design philosophies: 1. bottom-up vs top-down CGMV 2. centralised vs decentralised. • The database administrator (DBA) is responsible for managing the corporate database. The internal organisation of the database administration function varies from company to company. Although no standard exists, it is common practice to divide DBA operations according to the database life-cycle phases. Some companies have created a position with a broader data management mandate to manage computerised and other data within the organisation. This broader data management activity is handled by the data administrator (DA). • The DA and the DBA functions tend to overlap. Generally speaking, the DA is more managerially orientated than the more technically orientated DBA. Compared to the DBA function, the DA function is DBMS-independent, with a broader and longer-term focus. However, when the organisation chart does not include a DA position, the DBA executes all of the DA’s functions. Because the DBA has both technical and managerial responsibilities, the DBA must have a diverse mix of skills. CGMV • The managerial services of the DBA function include at least: o Supporting the end-user community o Defining and enforcing policies, procedures and standards for the database function o Ensuring data security, privacy and integrity o Providing data backup and recovery services o Monitoring the distribution and use of the data in the database • The technical role requires the DBA to be involved in at least these activities: o Evaluating, selecting and installing the DBMS o Designing and implementing databases and applications o Testing and evaluating databases and applications o Operating the DBMS, utilities and applications o Training and supporting users o Maintaining the DBMS, utilities and applications • The development of the data administration strategy is closely related to the company’s mission and objectives. Therefore, the development of an organisation’s strategic plan corresponds to that of data administration, requiring a detailed analysis of company goals, situation, and business needs. To guide the development of this overall plan, an integrating methodology is required. The most commonly used integrating methodology is known as information engineering (IE). CGMV Chapter 11: Conceptual, Logical, and Physical Database Design • Conceptual database design is where the conceptual representation of the database is created by producing a data model that identifies the relevant entities and relationships within the system. This stage of the Database Life Cycle can be broken down in to four steps: 1. Data analysis and requirements, 2. entity relationship modelling and normalisation, 3. data model verification, and 4. distributed database design. CGMV The final conceptual model must embody a clear understanding of the business and its functional areas. • Data model verification is part of the conceptual database design phase where the ER model must be verified against the proposed system processes in order to corroborate that the intended processes can be supported by the database model. Verification requires that the model be run through a series of tests against end-user data views and their required transactions, access paths and security and business-imposed data requirements and constraints. • Logical database design is the second stage in the Database Life Cycle, where relations are designed based on each entity and its relationships within the conceptual model. Creating the logical data model involves the following stages: 1. creating the logical data model, 2. validating the logical data model using normalisation, 3. assigning and validating integrity constraints, 4. merging logical models constructed for different parts for the database, and 5. reviewing the logical data model with the user. • When creating the logical data model, the order in which the entities are translated into relations is important. Those with no dependents (e.g. that do not contain any foreign keys) should be translated first. To create the relations, the name of the relation is specified along with its associated attributes enclosed in brackets. Finally, the primary key attribute(s) is identified, followed by any foreign keys. • Physical database design is where the logical data model is mapped onto the physical database tables to be implemented in the chosen DBMS. The ultimate goal must be to ensure that data CGMV storage is used effectively, to ensure integrity and security, and to improve efficiency in terms of query response time. Physical database design comprises the following seven stages: 1. Analyzing data volume and database usage, 2. translating each relation identified in the logical data model into database tables, 3. determining the most suitable file organisation, 4. defining indexes to speed up data access, 5. designing user views, 6. estimating data storage requirements, and 7. determining appropriate database security for users. • Selecting a suitable file organisation is important for fast data retrieval and efficient use of storage space. The three most common types of file organisation are 1. heap files, which contain randomly ordered records; 2. indexed sequential files, which are sorted on one or more fields using indexes; and 3. hashed files, in which a hashing algorithm is used to determine the address of each record based upon the value of the primary key. Within a DBMS, indexes are often stored in data structures known as B-trees, which allow fast data retrieval. Two other kinds of indexes 1. are bitmap indexes and 2. join indexes. These are often used on multidimensional data held in data warehouses. • Indexes are crucial for speeding up data access. Indexes facilitate searching, sorting, and using aggregate functions and even join operations. The improvement in data access speed occurs because an index is an ordered set of values that contains the index key and pointers. Data sparsity refers to the number of different values a column could possibly have. Indexes are recommended in highly sparse columns used in search conditions. CGMV Chapter 12: Managing Transactions and Concurrency • A transaction is a sequence of database operations that access the database. A transaction represents real-world events. A transaction must be a logical unit of work; that is, no portion of the transaction can exist by itself. Either all parts are executed, or the transaction is aborted. A transaction takes a database from one consistent state to another. A consistent database state is one in which all data integrity constraints are satisfied. • Transactions have five main properties: 1. atomicity (all parts of the transaction are executed; otherwise, the transaction is aborted), 2. consistency (maintaining the permanence of the database’s consistent state), 3. isolation (data being used by one transaction cannot be accessed by another transaction until the first transaction is completed) 4. durability (the changes made by a transaction cannot be rolled back once the transaction is committed) and 5. serialisability (the result of the concurrent execution of transactions is the same as that of the transactions being executed in serial order). • SQL provides support for transactions through the use of two statements: 1. COMMIT (saves changes to disk) and 2. ROLLBACK (restores the previous database state). • SQL transactions are formed by several SQL statements or database requests. Each database request originates several I/O database operations. • The transaction log keeps track of all transactions that modify the database. The information stored in the transaction log is used for recovery (ROLLBACK) purposes. • Concurrency control coordinates the simultaneous execution of transactions. The concurrent execution of transactions can result in three main problems: 1. lost updates, 2. uncommitted data and 3. inconsistent retrievals. • The scheduler is responsible for establishing the order in which the concurrent transaction operations are executed. The transaction execution order is critical and ensures database integrity in multi-user database systems. Locking, time stamping and optimistic methods are used by the scheduler to ensure the serialisability of transactions. CGMV • A lock guarantees unique access to a data item by a transaction. The lock prevents one transaction from using the data item while another transaction is using it. There are several levels of locking: 1. database, 2. table, CGMV 3. page, 4. row and field. • Two types of locks can be used in database systems: 1. binary locks and ▪ A binary lock can have only two states: 1 (locked) or 0 (unlocked). 2. shared/exclusive locks. ▪ A shared lock is used when a transaction wants to read data from a database and no other transaction is updating the same data. Several shared or ‘Read’ locks can exist for a particular item. ▪ An exclusive lock is issued when a transaction wants to update (write to) the database and no other locks (shared or exclusive) are held on the data. CGMV • Serialisability of schedules is guaranteed through the use of two-phase locking. The two-phase locking schema has a growing phase, in which the transaction acquires all of the locks that it needs without unlocking any data, and a shrinking phase, in which the transaction releases all of the locks without acquiring new locks. • When two or more transactions wait indefinitely for each other to release a lock, they are in a deadlock, or a deadly embrace. There are three deadlock control techniques: 1. prevention, 2. detection and 3. avoidance. • Concurrency control with time stamping methods assigns a unique time stamp to each transaction and schedules the execution of conflicting transactions in time stamp order. • Two schemes are used to decide which transaction is rolled back and which continues executing: 1. the wait/die scheme and 2. the wound/wait scheme. CGMV • Concurrency control with optimistic methods assumes that the majority of database transactions do not conflict and that transactions are executed concurrently, using private, temporary copies of the data. At commit time, the private copies are updated to the database. • The ANSI standard defines four transaction isolation levels: 1. Read Uncommitted, 2. Read Committed, 3. Repeatable Read, and 4. Serialisable. • Database recovery restores the database from a given state to a previous consistent state. Database backups are permanent copies of the database; they are stored in a safe place and are to be used in case of a critical error in the master database. CGMV Chapter 13: Managing Database and SQL Performance • Database performance tuning refers to a set of activities and procedures designed to ensure that an end-user query is processed by the DBMS in the minimum amount of time. • SQL performance tuning refers to the activities on the client side designed to generate SQL code that returns the correct answer in the least amount of time, using the fewest resources at the server end. • DBMS performance tuning refers to activities on the server side orientated to ensure that the DBMS is properly configured to respond to clients’ requests in the fastest way possible while making optimum use of existing resources. • Database statistics refers to a number of measurements gathered by the DBMS that describe a snapshot of the database objects’ characteristics. The DBMS gathers statistics about objects such as tables, indexes, and available resources such as number of processors used, processor speed and temporary space available. The DBMS uses the statistics to make critical decisions about improving the query processing efficiency. • The DBMS processes queries in three phases: 1. Parsing. The DBMS parses the SQL query and chooses the most efficient access/execution plan. 2. Execution. The DBMS executes the SQL query, using the chosen execution plan. 3. Fetching. The DBMS fetches the data and sends the result set back to the client. CGMV • Indexes are crucial to the process that speeds up data access and should be carefully selected during physical database design in order to facilitate the searching, sorting and use of aggregate functions and join operations. • During query optimisation, the DBMS must choose which indexes to use, how to perform join operations, which table to use first, and so on. Each DBMS has its own algorithms for determining the most efficient way to access the data. The two most common approaches are 1. rule-based optimisation and ▪ A rule-based optimiser uses a set of preset rules and points to determine the best approach to execute a query. ▪ The rules assign a ‘fixed cost’ to each SQL operation; the costs are then added to yield the cost of the execution plan. 2. cost-based optimisation. ▪ A cost-based optimiser uses sophisticated algorithms based on the statistics about the objects being accessed to determine the best approach to execute a query. ▪ In this case, the optimiser process adds up the processing cost, the I/O costs and the resource costs (RAM and temporary space) to come up with the total cost of a given execution plan. • Hints are used to change the optimiser mode for the current SQL statement. Hints are special instructions for the optimiser that are embedded inside the SQL command text. • SQL performance tuning deals with writing queries that make good use of the statistics. In particular, queries should make good use of indexes. Indexes are very useful when you want to select a small subset of rows from a large table based on a condition. When an index exists for the column used in the selection, the DBMS may choose to use it. The objective is to create indexes with high selectivity. Index selectivity is a measure of how likely an index will be used in query processing. • Query formulation deals with how to translate business questions into specific SQL code to generate the required results. To do this, you must carefully evaluate which columns, tables and computations are required to generate the desired output. • DBMS performance tuning includes tasks such as managing the DBMS processes in primary memory (allocating memory for caching purposes) and the structures in physical storage (allocating space for the data files). CGMV Chapter 14: Distributed Databases • A distributed database stores logically related data in two or more physically independent sites connected via a computer network. The database is divided into fragments, which can be horizontal (a set of rows) or vertical (a set of attributes). Each fragment can be allocated to a different network node. CGMV • Distributed processing is the division of logical database processing among two or more network nodes. Distributed databases require distributed processing. A distributed database management system (DDBMS) governs the processing and storage of logically related data through interconnected computer systems. • The main components of a DDBMS are the 1. transaction processor (TP) and ▪ The transaction processor component is the software that resides on each computer node that requests data. 2. the data processor (DP). ▪ The data processor component is the software that resides on each computer that stores and retrieves data. CGMV • Current database systems can be classified by the extent to which they support processing and data distribution. Three major categories are used to classify distributed database systems: 1. (1) single-site processing, single-site data (SPSD); 2. (2) multiple-site processing, single-site data (MPSD); and 3. (3) multiple-site processing, multiple-site data (MPMD). CGMV • A homogeneous distributed database system integrates only one particular type of DBMS over a computer network. A heterogeneous distributed database system integrates several different types of DBMSs over a computer network. • DDBMS characteristics are best described as a set of transparencies: 1. distribution, ▪ A database transaction that accesses data in several remote data processors (DPs) in a distributed database. ▪ Three levels of distribution transparency: 1. Fragmentation (highest level) 2. Location (medium level) 3. Local mapping (lowest level) 2. transaction, ▪ A DDBMS property that ensures database transactions will maintain the distributed database’s integrity and consistency, and that a transaction will be completed only when all database sites involved complete their part of the transaction. 3. failure, ▪ A feature that allows continuous operation of a DDBMS, even if a network node fails. 4. heterogeneity and ▪ A feature that allows a system to integrate several centralised DBMSs into one logical DDBMS. 5. performance. ▪ A DDBMS feature that allows a system to perform as though it were a centralised DBMS. All transparencies share the common objective of making the distributed database behave as though it were a centralised database system; that is, the end user sees the data as part of a single logical centralised database and is unaware of the system’s complexities. • A transaction is formed by one or more database requests. An undistributed transaction updates or requests data from a single site. A distributed transaction can update or request data from multiple sites. • Distributed concurrency control is required in a network of distributed databases. A two-phase COMMIT protocol is used to ensure that all parts of a transaction are completed. • A distributed DBMS evaluates every data request to find the optimum access path in a distributed database. The DDBMS must optimise the query to reduce 1. access, 2. communications and 3. CPU costs associated with the query. • The design of a distributed database must consider the fragmentation and replication of data. The designer must also decide how to allocate each fragment or replica to obtain better overall response time and to ensure data availability to the end user. • A database can be replicated over several different sites on a computer network. The replication of the database fragments has the objective of improving data availability, thus decreasing access time. A database can be partially, fully, or not replicated. Data allocation strategies are designed to determine the location of the database fragments or replicas. CGMV • The CAP theorem states that a highly distributed data system has some desirable properties of 1. consistency, 2. availability and 3. partition tolerance. However, a system can only provide two of these properties at a time. CGMV Chapter 15: Databases for Business Intelligence • Business intelligence (Bl) is a term for a comprehensive, cohesive and integrated set of applications used to capture, collect, integrate, store and analyse data with the purpose of generating and presenting information to support business decision making. BI provides a framework for: 1. Collecting and storing operational data 2. Aggregating the operational data into decision support data 3. Analysing decision support data to generate information 4. Presenting such information to the end user to support business decisions 5. Making business decisions, which in turn generate more data that are collected, stored, and so on (restarting the process) 6. Monitoring results to evaluate outcomes of the business decisions, which again provides more data to be collected, stored, and so on 7. Predicting future behaviours and outcomes with a high degree of accuracy. CGMV • Decision support refers to a methodology (or a series of methodologies) designed to extract information from data and to use such information as a basis for decision making. A decision support system (DSS) is an arrangement of computerised tools used to assist managerial decision making within a business. • Operational data are not best suited for decision support. From the end-user point of view, DSS data differ from operational data in three main areas: 1. time span, ▪ Operational data cover a short time frame. In contrast, decision support data tend to cover a longer time frame. 2. granularity and ▪ DSS data must be presented at different levels of aggregation, from highly summarised to near-atomic. (Allowing drill down or roll up) CGMV 3. dimensionality. ▪ Operational data focus on representing individual transactions rather than on the effects of the transactions over time. • The data warehouse is an integrated, subject-orientated, time-variant, non-volatile collection of data that provides support for decision making. The data warehouse is usually a read-only database optimised for data analysis and query processing. A data mart is a small, single-subject data warehouse subset that provides decision support to a small group of people. CGMV • Online analytical processing (OLAP) refers to an advanced data analysis environment that supports decision making, business modelling and operations research. • Relational online analytical processing (ROLAP) provides OLAP functionality by using relational databases and familiar relational query tools to store and analyse multidimensional data. Multidimensional online analytical processing (MOLAP) provides OLAP functionality by using multidimensional database management systems (MDBMSs) to store and analyse multidimensional data. CGMV • The star schema is a data modelling technique used to map multidimensional decision support data into a relational database with the purpose of performing advanced data analysis. The basic star schema has four components: 1. facts, ▪ Facts are numeric measurements or values representing a specific business aspect or activity. 2. dimensions, ▪ Dimensions are general qualifying categories that provide additional perspectives to a given fact. Conceptually, the multidimensional data model is best represented by a three-dimensional cube 3. attributes and ▪ Attributes can be ordered in well-defined attribute hierarchies. 4. attribute hierarchies. ▪ The attribute hierarchy provides a top-down organisation that is used for two main purposes: 1. to permit aggregation and 2. to provide drill-down/roll-up data analysis. • Data analytics is a subset of Bl functionality that provides advanced data analysis tools to extract knowledge from business data. Data analytics can be divided into 1. explanatory and ▪ Explanatory analytics focuses on discovering and explaining data characteristics and relationships. 2. predictive analytics. CGMV ▪ Predictive analytics focuses on creating models to predict future outcomes or events based on the existing data. • Data mining automates the analysis of operational data with the intention of finding previously unknown data characteristics, relationships, dependencies and/or trends. The data mining process has four phases: 1. data preparation, 2. data analysis and classification, 3. knowledge acquisition and 4. prognosis. • SQL has been enhanced with analytic functions that support OLAP type processing and data generation. • Data visualisation provides visual representations of data that enhance the user’s ability to comprehend the meaning of the data. CGMV Chapter 16: Big Data and NoSQL • This chapter is not examinable. • Big Data is characterised by data of such volume, velocity and/or variety that the relational model struggles to adapt to it. Volume refers to the quantity of data that must be stored. Velocity refers to both the speed at which data is entering storage as well as the speed with which it must be processed. Variety refers to the lack of uniformity in the structure of the data being stored. As a result of Big Data, organisations are having to employ a variety of data storage solutions that include technologies, in addition to relational databases, a situation referred to as polyglot persistence. • 5 Vs of Big Data: 1. Volume, 2. velocity, 3. variety, 4. veracity and 5. value. However, these are not the only characteristics of Big Data to which data administrators must be sensitive. Additional Vs that have been suggested by the data management industry include 6. variability and ▪ Variability is the variation in the meaning of data that can occur over time. 7. visualisation. ▪ Further, visualisation is the requirement that the data must be able to be presented in a manner that makes it comprehensible to decision makers. Most of these additional Vs are not unique to Big Data. There are also concerns for data in relational databases. CGMV • The Hadoop framework has quickly emerged as a standard for the physical storage of Big Data. The primary components of the framework include the 1. Hadoop Distributed File System (HDFS) and ▪ HDFS is a coordinated technology for reliably distributing data over a very large cluster of commodity servers. 2. MapReduce. ▪ MapReduce is a complementary process for distributing data processing across distributed data. ▪ One of the key concepts for MapReduce is to move the computations to the data instead of moving the data to the computations. ▪ MapReduce works by combining the functions of map, which distributes subtasks to the cluster servers that hold data to be processed, and reduce, which combines the map results into a single result set. CGMV The Hadoop framework also supports an entire ecosystem of additional tools and technologies, such as Hive, Pig and Flume, which work together to produce a complex system of Big Data processing. • NoSQL is a broad term that refers to any of several non-relational database approaches to data management. Most NoSQL databases fall into one of four categories: 1. key-value databases, 2. document databases, 3. column-oriented databases, or 4. graph databases. Due to the wide variability of products under the NoSQL umbrella, these categories are not necessarily all-encompassing, and many products can fit into multiple categories. CGMV • Key-value databases store data in key-value pairs. In a key-value pair, the value of the key must be known to the DBMS, but the data in the value component can be of any type, and the DBMS makes no attempt to understand the meaning of the data in it. These types of databases are very fast when the data is completely independent, and the application programs can be relied on to understand the meaning of the data. • Document databases also store data in key-value pairs, but the data in the value component is an encoded document. The document must be encoded using tags, such as in XML or JSON. The DBMS is aware of the tags in the documents, which makes querying on tags possible. Document databases expect documents to be self-contained and relatively independent of one another. CGMV • Column-oriented databases, also called column family databases, organise data into key-value pairs in which the value component is composed of a series of columns, which themselves are key-value pairs. Columns can be grouped into super columns, similar to a composite attribute in the relational model being composed of simple attributes. All objects of a similar type are identified as rows, given a row key, and placed within a column family. Rows within a column family are not required to have the same structure, that is, they are not required to have the same columns. • Graph databases are based on graph theory and represent data through nodes, edges and properties. A node is similar to an instance of an entity in the relational model. Edges are the relationships between nodes. Both nodes and edges can have properties, which are attributes that describe the corresponding node or edge. Graph databases excel at tracking data that is highly interrelated, such as social media data. Due to the many relationships among the nodes, it is difficult to distribute a graph database across a cluster in a highly distributed manner. CGMV • NewSQL databases attempt to integrate features of both RDBMS (providing ACIDS-compliant transactions) and NoSQL databases (using a highly distributed infrastructure). • MongoDB is a document database that stores documents in JSON format. The documents can be created, updated, deleted and queried using a JavaScript-like language, named MongoDB Query Language. Data retrieval is done primarily through the find() method. • Neo4j is a graph database that stores data as nodes and relationships, both of which can contain properties to describe them. Neo4j databases are queried using Cypher, a declarative language that shares many commonalities with SQL, but is still significantly different in many ways. Data retrieval is done primarily through the MATCH command to perform pattern matching. CGMV Chapter 17: Database Connectivity and Web Technologies • Database connectivity refers to the mechanisms through which application programs connect and communicate with data repositories. Database connectivity software is also known as database middleware. • Microsoft database connectivity interfaces are dominant players in the market and enjoy the support of most database vendors. In fact, ODBC, OLE-DB and ADO.NET form the backbone of Microsoft’s Universal Data Access (UDA) architecture. • Native database connectivity refers to the connection interface that is provided by the database vendor and is unique to that vendor. ODBC is probably the most widely supported database connectivity interface. ODBC allows any Windows application to access relational data sources using standard SQL. Data Access Objects (DAO) is an older, object-oriented application interface. Remote Data Objects (RDO) is a higher-level, object-oriented application interface used to access remote database servers. RDO was optimised to deal with server-based databases such as Microsoft SQL Server and Oracle. CGMV • Object Linking and Embedding for Database (OLE-DB) is database middleware developed with the goal of adding object-oriented functionality for access to relational and non-relational data. ActiveX Data Objects (ADO) provides a high-level, application-oriented interface to interact with OLE-DB, DAO, and RDO. Based on ADO, ADO.NET is the data access component of Microsoft’s. NET application development framework. Java Database Connectivity (JDBC) is the standard way to interface Java applications with data sources. CGMV • Database access through the Web is achieved through middleware. To improve the capabilities on the client side of the Web browser, you must use plug-ins and other client-side extensions such as Java and JavaScript, or ActiveX and VBScript. On the server side, Web application servers are middleware that expand the functionality of Web servers by linking them to a wide range of services, such as databases, directory systems and search engines. CGMV • Extensible Markup Language (XML) facilitates the exchange of B2B and other data over the internet. XML provides the semantics that facilitates the exchange, sharing and manipulation of structured documents across organisational boundaries. XML produces the description and the representation of data, thus setting the stage for data manipulation in ways that were not possible before XML. XML documents can be validated through the use of Document Type Definition (DTD) documents and XML Schema Definition (XSD) documents. • Cloud computing is a computing model that provides ubiquitous, on-demand access to a shared pool of configurable resources that can be rapidly provisioned. SQL data services (SDS) refers to a cloud computing-based data management service that provides relational data storage, ubiquitous access and local management to companies of all sizes. This service enables rapid application development for businesses with limited information technology resources. SDS allows rapid deployment of business solutions using standard protocols and common programming interfaces. CGMV • The Semantic Web, often referred to as a Web of data, is a framework that allows all data on the WWW to be shared and reused across applications, without any boundaries.","libVersion":"0.2.3","langs":""}