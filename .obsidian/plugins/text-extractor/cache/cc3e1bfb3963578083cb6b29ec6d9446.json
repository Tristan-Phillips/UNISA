{"path":"UNISA/98906 - BSc Science in Computing/INF3703 - Databases II/Unsorted/INF 3703/Additional/summary.pdf","text":"Downloaded by Tristan Phillips (tristan@phillipsfamily.co.za) lOMoARcPSD|12886293 Explain the DATABASE INITIAL STUDY PHASE of the DBLS 1. Analyze the company situation 2. Define the problems and constraints 3. Define the objectives 4. Define the scope and boundaries Name the roles associated with a DBA in a technical role 1. Evaluate, select and install the DBMS 2. Design and implement DB and APPS 3. Testing and evaluating DB and APPS 4. Training and support user 5. Maintain the DBMS, DB and APPS Briefly explain the conceptual design phase of the DB design process 1. Data analysis and requirements 2. Entity relationship modelling and normalization 3. Data model verification 4. Distributed database design Name the steps required in the development of ER diagrams. 1. Identify, analyze and refine the business rules 2. Identify the main entities. 3. Define the relationships among the entities. 4. Define the attributes, PK and FKs 5. Normalize the entities Explain the flow of phases in a query. 1. Parsing phase DBMS parses the SQL query and chooses the most efficient execution plan 2. Execution phase The DBMS execute the SQL query using the execution plan 3. Fetching phase The DBMS fetches the data and sends the result set back to the client Name all the transaction properties 1. Atomicity 2. Consistency 3. Isolation 4. Reliability 5. Serializability Downloaded by Tristan Phillips (tristan@phillipsfamily.co.za) lOMoARcPSD|12886293 What is concurrency control and why is it necessary. It allows the simultaneous execution of transactions in a multi-user database system. Its objective is to yield consistent results Name and explain 3 additional issues when designing DDBMS 1. Data Fragmentation 2. Data replication 3. Data Allocation Data Security, privacy and integrity are important functions in authorization management. Name 7 activities that a DBA should do in his role as management. 1. Assign users to the DB 2. Assign each user a password 3. Define user groups 4. Define user views 5. Control physical access to the network 6. Control access to the DBMS 7. Monitor DBMS usage Identify the critical success factors in the development and implementation of a successful data system. 1. Management commitment 2. Thorough company situation analysis 3. End-User involvement 4. Defined Standards 5. Training 6. Start with a small pilot project Discuss the difference between the top down and bottom up strategies. The TOP DOWN strategy involves identifying the data sets first, then identifying the data elements for those sets. The BOTTOM UP strategy involves identifying the data elements first, then grouping them together in data sets. What is cloud computing and how does it relate to cloud services. 1. Cloud computing refers to the delivering of applications, storage and other resources to users over the web. Users can use the cloud to access SAAS, PAAS and IAAS. List 3 different types of implementations for cloud computing 1. Private Cloud 2. Public Cloud 3. Community Cloud Discuss the 4 important concepts that affect data recovery Downloaded by Tristan Phillips (tristan@phillipsfamily.co.za) lOMoARcPSD|12886293 1. Write ahead log protocol 2. Redundant transaction logs 3. Database Buffers 4. Database checkpoints What is an exclusive lock and when can it be granted 1. Exists when access is specifically reserved for the transaction that locked the object. It can only be granted if there is no locks on the data that is requested to be locked. What is the difference between rule based and cost based query optimizer modes 1. Rule based query optimization uses a fixed set of rules to optimize a query. This leads to a 'fixed cost' at which queries are optimized 2. Cost based query optimization uses a advanced set of algorithms based on database statistics and analytics to calculate the shortest possible execution path. The problem is that the overhead cost of the algorithms is much higher. Describe the difference between SQL and DBMS performance tuning SQL - Client Side. Objective is to generate a SQL query that returns the correct answer in the least amount of time using the minimum amount of resources at the server end. DBMS - Server Side. Activities and procedures designed to ensure that end-user queries are processed by the DBMS in the minimum amount of time, while making the best use of resources Describe the difference between Data cache and SQL cache 1. Data cache - used by the DBMS to store fetched data from the DB. 2. SQL cache - used to store the most recent SQL statements that were executed. Fully DDBMS must perform all the functions of a centralized DBMS. Name these functions 1. Receive application request 2. Validate, analyze and decompose the request. 3. Map the request's logical-to-physical data components 4. Search, locate, read and validate data 5. Ensure the DB consistency, security and integrity 6. Validate the dat for conditions What are the 4 star schema performance techniques? 1. Normalize the dimension tables 2. Denormalize the fact tables 3. Maintain multiple fact tables 4. Replicate and Partition tables Explain 6 measures you as DB Admin will take to ensure backup and recovery of the DB 1. Periodic backup of data and applications 2. Proper identification of what to back up 3. Convenient and Safe backup and storage 4. Physical protection of software and hardware Downloaded by Tristan Phillips (tristan@phillipsfamily.co.za) lOMoARcPSD|12886293 5. Limit personal access to control software and DB 6. Insurance coverage for data Explain in detail the 3 stages of DB design Conceptual design 1. Data analysis and requirements 2. ERM and normalization 3. Data model verification 4. DDBMS Logical design 1. Create the logical data model 2. Validate the logical data model 3. Assign integrity constraints 4. Merge logical models for different parts together 5. review the logical model with the user Physical design 1. translate each relation into tables 2. Determine a suitable file organization 3. Define indexes 4. Define user view 5. Estimate data storage 6. determine db security for users What is a deadlock It is when 2 transactions wait on each other to complete indefinitely. The one cannot continue without the other one finishing. thus there is a deadlock Name 3 techniques used to deal with deadlock 1. Prevention 2. Avoidance 3. Detection What is a consistent state and how is it achieved 1. A db is in a consistent state whenever all the constraints are satisfied. A successful transaction takes a db from one consistent state to another. What 3 levels of backup is used in DB recovery 1. Full back-up 2. Differential backup 3. Transaction log backup Describe the different types of DB requests and transactions 1. a request is a single SQL command 2. a transaction consists out of two or more requests. you could have a single request going to a single data source you could have multiple requests going to a single data source you could have multiple requests going to multiple data sources transactions include SQL statements like FROM, SELECT, UPDATE, COMMIT Downloaded by Tristan Phillips (tristan@phillipsfamily.co.za) lOMoARcPSD|12886293 Discuss a DDBMS transparency features 1. Transaction transparency 2. Distribution transparency 3. Failure transparency 4. Performance transparency 5. Heterogeinity transparency What is a data warehouse, and name its components It is a integrated, subject oriented, time-variant, non-volatile - once data enter it never leaves What is a star schema? It is a modeling technique used to map multidimensional decision support data into a relational database. Name the components of a Star schema 1. Fact Tables 2. Dimension tables 3. Attributes 4. Attribute hierarchy Name the differences between the two database design approaches Top-Down approach Identifies the data sets first and then identifies the data elements and attributes within the sets Bottom up approach identifies the data elements ant attributes first, an then group them together in data sets Name the 6 phases of the dblc 1. Initial feasibility study 2. DB Design 3. Loading and Implementation 4. Testing and evaluation 5. Operation 6. Maintenance DBMS Architecture 1. Files: All data in a database is stored in data files 2. Table Space or File group: Data files grouped here 3. Data Cache or buffer cache: Shared, reserved memory area that stores the most recently accessed data blocks in RAM 4. SQL cache or procedure cache: shared, reserved memory area that stores the most recently executed SAL statements or PL/SQL procedures Downloaded by Tristan Phillips (tristan@phillipsfamily.co.za) lOMoARcPSD|12886293 5. Input/Output (I/O) request: low level data access operations that reads/writes data to and from computer devices 6. Working with data in the data cache is much faster than working with data in data files because the DBMS doesn't have to wait for the HD to retrieve the data Star Schema performance tuning techniques 1. Normalizing dimensional tables 2. Maintaining multiple fact tables to represent different aggregation levels 3. De-normalizing fact tables 4. Partitioning and replicating tables What is a DDBMS? A Distributed Database Management System (DDBMS) governs the storage and processing of logically related data over interconnected computer systems in which both data processing are distributed among several sites. Cloud Computing A computing model that provides ubiquitous, on-demand access to a shared pool of configurable resources that can be rapidly provisioned. Cloud Services Allows any organization to quickly and economically add information technology services such as applications, storage, servers, processing power, databases, and infrastructure to its IT portfolio. Web-to-database middleware A database server-side extension that retrieves data from databases and passes them to the Web server, which in turn sends the data to the client's browser for display. Describe Object Linking and Embedding for Database (OLE-DB) and the main types of objects in the OLE-DB Model OLE-DB is a database middleware that adds object-oriented functionality for access to relational and non-relational data. The model has 2 main objects: 1. Consumers: Objects (application and processes) that request and use data. Consumers request data by invoking methods exposed by the data provider objects and passing the required parameters. 2. Providers: Objects that manage the connection with a data source and provide data to the consumers. Two types of providers, data providers that provide data to other processes and service providers which provide additional functionality to consumers. Downloaded by Tristan Phillips (tristan@phillipsfamily.co.za) lOMoARcPSD|12886293 Explain what a data mart is and provide reasons why some organizations prefer to create a data mart instead of a data warehouse A data mart is a small, single-subject data warehouse subset that provides decision support to a small group of people. Organizations may prefer data marts due to lower costs and shorter implementation time as well as technological advances and avoidance of associated 'people issues'. Discuss the three properties of the CAP theorem 1. Consistency: All nodes should see the same data at the same time, which means that replicas should be immediately updated. 2. Availability: A request is always fulfilled by the system and no received request is ever lost. 3. Partition Tolerance: The system continues to operate even in the event of a node failure. What is database recovery? Database recovery restores a database from a given state (usually inconsistent) to a previously consistent state. Explain the Atomic Transaction Property Database recovery techniques are based on the atomic transaction property: all portions of the transaction must be treated as a single, logical unit of work in which all operations are applied and competed to produce a consistent database Describe the use of deferred-write and write-through techniques (in database recovery) - Deferred-write technique: the transaction operations do not immediately update the physical database. Instead, only the transaction log is updated. The database is physically updated only with data from committed transactions, using info from the transaction logs. If the transaction aborts before it reaches a commit point, no changes (ROLLBACK) need to be made because it was never updated. - Write-through technique: The database is immediately updated by the transaction operations during the transaction's execution, even before the transaction reaches its commit point. If the transaction aborts before it reaches its commit point, a ROLLBACK operation needs to be done to restore the db to a consistent state. Database recovery process ( Deferred-Write Technique) 1. Identify the last checkpoint in the transaction log. This is the last time transaction data was physically saved to disk. 2. For a transaction that started and was committed before the last checkpoint, nothing needs to be done because the data is already saved. 3. For a transaction that performed a commit operation after the last checkpoint, the DBMS uses the transaction log records to redo the transaction and update the DB, using the 'after' values in the transaction log. The changes are made in ascending order, from oldest to newest. 4. For any transaction that had a ROLLBACK operation after the last checkpoint or that was left active before the failure occurred, nothing needs to be done because the database was never updated. SQL data services (SDS) Downloaded by Tristan Phillips (tristan@phillipsfamily.co.za) lOMoARcPSD|12886293 SDS refers to a cloud computing based data management service that provides relational data storage, access & management to companies of all sizes without the typically high-costs of in-house hardware, software, infrastructure & personnel. OLE-DB Object Linking and Embedding for Database. It was developed by Microsoft to answer the need to provide support for non-relational data and to simplify data connectivity. => Has object-oriented functionality Consists of: => Consumers: objects (applications or processes) that request and use data. Consumers request data by invoking methods exposed by the data provider objects and passing required parameters. => Providers: objects that manage the connections with data source and provide data to the consumers. Data providers provide data to other processes and Service providers provide additional functionality to consumers. Data Mart A small, single-subject data warehouse subset that provides decision support to a small group of people. Star Schema A data modeling technique used to map multidimensional decision support data into a relational database. The star schema represents data using a central table known as a fact table in a 1:M relationship with one or more dimension tables. Techniques to optimize data warehouse design => Normalizing dimensional tables => Maintaining multiple fact tables to represent different aggregation levels => Denormalizing fact tables => Partitioning and replicating tables Business Intelligence A comprehensive, cohesive, and integrated set of tools and processes used to capture, collect, integrate, store, and analyze data with the purpose of generating and presenting information to support business decision making. Advantages of DDBMS - data located near site of greatest demand - faster data access (using subset of data) - faster data processing (spread out work) - growth facilitation (add new sites easily) - improved communications - reduced operating costs - user-friendly interface Downloaded by Tristan Phillips (tristan@phillipsfamily.co.za) lOMoARcPSD|12886293 - less danger of single-point failure - processor independence Disadvantages of DDBMS - complexity of management and control - technological difficulty - security - lack of standards - increased storage & infrastructure requirements - increased training costs - costs Distribution Transparency - allows a distributed database to be managed as a single logical database - 3 levels: fragmentation, location, & local mapping transparency Transaction Transparency - allows a transaction to update data at more than one network site - either entirely completed or entirely aborted Failure Transparency - ensures that the system will continue to operate in the event of a node or network failure - lost functions picked up by other nodes in network Performance Transparency - allows the system to perform as if it were a centralized DBMS - no system degradation due to use of network - ensures most cost-effective path to remote data - able to \"scale out\" transparently Heterogeneity Transparency allows the integration of several different local DBMSs (relational, network and hierarchical) under a common, or global, schema What is a full backup A dump of the entire database. In this case all database objects are backed up in their entirety. What is Differential Backup Only the objects that have been updated or modified since the last full backup are backed up. Transaction Log Backup backs up only the transaction log operations that are not reflected in a previous backup copy of the database Incremental Database Produces a backup of all data since the last backup date Downloaded by Tristan Phillips (tristan@phillipsfamily.co.za) lOMoARcPSD|12886293 Concurrent backup Takes place while the user is working on the database What are the problems associated to an absence of concurrency control? - Lost updates - Uncommitted Data - Inconsistent Retrievals What is pessimistic locking? The use of locks based on the assumption that conflict between transactions is likely What is the optimistic approach to locking? A concurrency control technique based on the assumption that most database operations do not conflict. - Requires neither locking nor time stamping techniques - Transaction is executed without restriction until it is commited Concurrency control phases => Read Phase: reads the db, executes the needed computations and makes the updates to a private copy of the db values. Update operations are recorded in a temporary update file not accessed by remaining transactions. => Validation Phase: Transaction is validated to ensure that the changes made will not affect the integrity and consistency of the db. => Write Phase: The changes are permanently applied to the database. Atomocity Requires all operations of a transaction to be completed; if not, the transaction is aborted. Consistency Indicates the permanence of the db's consistent state. When transactions are completed, the db must be in a consistent state, if any transaction parts violates an integrity constraint, the entire transaction is aborted. Isolation The data used during the execution of a transaction cannot be used by a second transaction until the first one is completed. Durability Ensure that once transaction changes are done and committed, they cannot be undone or lost in the event of system failure Downloaded by Tristan Phillips (tristan@phillipsfamily.co.za) lOMoARcPSD|12886293","libVersion":"0.2.3","langs":""}