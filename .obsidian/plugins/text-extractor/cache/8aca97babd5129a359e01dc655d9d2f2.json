{"path":"UNISA/98906 - BSc Science in Computing/INF3703 - Databases II/Unsorted/Stuvia-869000-detailed-inf3703-summary-databases-ii.pdf","text":"DETAILED INF3703 Summary (Databases II) written by francoissmit www.stuvia.com Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Detailed INF3703 Summary Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Note: The box with the ‘red E’ means it is an exam question and the box with the ‘yellow A’ means it was an assignment question in 2019 1st semester. For each ‘red E’ it means it was asked in an exam, thus multiple of them mean it was asked in multiple exams. Chapter 10 - Transaction Management and Concurrency Control DB Transaction DB Transaction: Represent real-world transactions (instance of buying or selling) that are triggered by events such as buying a product, registering for a course ,etc. A scenario when a sale is made have the following parts: IN DB terms a transaction is any action that reads or writes to a DB. It may consist of the following: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Transaction Definition: A transaction is a logical unit of work that must be entirely completed or entirely aborted; no intermediate states are acceptable. e.g. in the previous scenario which is a multicomponent transaction must not be partially completed. All of the SQL statements in the transaction must be completed successfully. If any of the SQL statements fail, the entire transaction is rolled back to the original DB state. A consistent database state is one in which all data integrity constraints are satisfied. Every transaction must begin with the DB in a known consistent state. All transactions are controlled and executed by the DBMS to guarantee DB integrity. A DB request is the equivalent of a single SQL statement in an application program or transaction. Evaluating Transaction Results: Not all transactions update the DB. Remember a SELECT query access from the DB and an access of the DB is a transaction. So if the DB existed in a consistent (acceptable state – my own wording of consistent state)) state before the SELECT then after the SELECT it will still exist in a consistent state after the SELECT because the data inside DB hasn’t been altered. Transaction may consist of single or multiple SQL statement. Say a sale has been made, then to do all of this: The SQL statements will look like this: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace The result of the transaction are shown in red: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace E E The transaction is defined by the user or programmer and the DBMS cannot guarantee that the programmer coded in the transaction correctly. i.e. the DBMS cannot evaluate whether the transaction represents the real-world event correctly. Transaction Properties  Each individual transaction must have the following properties (also referred as the ACID test): In exem list and explain them and give examples where necessary. *have to alter this a bit to include examples and better explanation o Atomicity: requires that all operations (SQL statements) of a transaction be completed, if not the transaction must be aborted. o Consistency: A transaction takes a DB from one consistent state to another. (Indication of the permanence [permanent] of a DB’s consistent state) o Isolation: The data used during the execution of a transaction cannot be used by a 2nd transaction until the 1 st one is complete o Durability: ensures that once transaction changes are done and commited, they cannot be undone or lost  Another important property which applies when multiple transactions are executed concurrently is serializability which ensures that the schedule for the concurrent execution of the transaction yields consistent results. i.e. it ensures that concurrent transaction operations creates the same final DB state that would have been produces if the transactions had been executed in a serial fashion With single user DBMS serializability is auto. ensured. Think concurrent, happens at same time thus in series, which is serial Transaction Management with SQL: o Transaction support is provided by the SQL statements: COMMIT and ROLLBACK Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace o When a transaction is initiated, the sequence must continue through all succeeding SQL statements until one of the following happen: o A transaction begins implicitly when the 1st SQL statements is encountered, but some SQL implementation it must be coded e.g: The Transaction Log:  A DBMS uses a transaction log to keep track of all transactions that update the DB.  The DBMS uses the info. stored in this log for a recovery requirement triggered by a ROLLBACK statement, a program’s abnormal termination, or a system failure.  Transaction log stores the following:  Here is table of simple transaction consisting of 2 SQL statements. The transaction log is usually implemented as one or more files that are managed separately from the actual DB files. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace E E E E PTR points to the previous Transaction ID and to the next Transaction ID. Concurrency Control:  Coordinating the simultaneous execution of transactions in a multiuser DB system is known as concurrency control.  Objective of concurrency control is to ensure the serializability of transactions in a multiuser DB environment.  Concurrency control is important because the simultaneous execution of transactions over a shared DB can create several data integrity and consistency problems such as lost update, uncommitted data and inconsistent retrievals.  The 3 main problems cause by simultaneous execution of transactions are: Think LUI  Lost updates: Caused when two concurrent (simultaneously) transactions T1 and T2 are updating the same data element and one of the updates is lost (overridden by the other transaction. E.g.: Say we have Transaction T1 and T2. T1 updates PROD_QOH (table PRODUCT’s attribute) by 100 and T2 updates it by - 35. Also, say initially PROD_QOH is 35 Given by the table: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace A Suppose that a transaction can read a product’s PROD_QOH before a previous transaction has been committed. Now T1 has not been COMITTED when T2 is executed. Thus, T2 still operates on the value 35 and it subtraction yields 5 in memory. So T1 writes 135 to the disk, which is promptly overwritten by T2 which writes 5 to the disk. This table displays this: Note: T1 and T2 also reads.  Uncommitted data: Occurs when T1 and T2 are executed concurrently and the 1st transaction T1 is rolled back after the 2nd transaction T2 has already accessed the uncommitted data, thus violating the isolation property of transaction. Suppose T2 reads data that was updated by T1, but T1 was rolled back, thus the data was not supposed to be read. This is how it should be: Incorrect way (i.e. the roll backed or uncommitted data is read) : Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace  Inconsistent retrievals: Occur when a transaction accesses data before and after one or more other transactions finish working with such data. Here T2 updates values while T1 is calc. total. Example if T1 calculated some summary function over a set of data while another transaction T2 was updating the same data. Problem is T1 will read data before some data is changed and after some of the data is changed, thus yielding inconsistent results. Suppose T2 represents updating of ‘1546-QQ2’ by +10 and updating ‘1558-QW1’ by -10. So they are correcting mistakes that were made with these updates. This table represents the correct total: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Here the updates are made before the total was calculated. Here we have the incorrect one: Here 23 is added before the subtraction was made. The scheduler:  The scheduler is a special DBMS process/component that establishes the order in which the operations are executed within concurrent transactions.  The scheduler interleaves the execution of DB operations to ensure serializability and isolation of transactions.  Not all transactions are serializable, which means that they can occur at the same time and still yield results which would be the same as if they would have occurred one after each other. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace A  Scheduler is a DBMS's component that is in charge of concurrency control resulting from concurrent execution of several transactions. The scheduler bases its actions on concurrency control algorithms, such as locking or timestamping  Transactions that are not serializable are executed on a 1st come, 1 st serve basis by the DBMS.  Schedular’s main job is to create a serializable schedule of transaction’s operations, in which the interleaved execution of the transactions yields the same results as if the transactions were executed in a serial order (one after the other).  Also the scheduler make sure that the CPU and storage systems are used efficiently.  Also, the scheduler facilitates data isolation to ensure that two transactions do not update the same data element at the same time.  If two transactions T1 and T2 are executed concurrently over the same data, then two operations are in conflict when they access the same data and at least one of them is a WRITE operation. Given by this tabl 3 methods exist to schedule execution of conflicting operations in concurrent transactions: Locking, Time stamping and optimistic. Concurrency Control with locking methods  Most common technique.  A lock guarantees exclusive use of data item to a current transaction, i.e. T2 doesn’t have access to a data item currently being used by T1  A transaction acquires a lock prior to data access, the lock releases when the transaction is completed so that another transaction can lock the data item for its exclusive uses. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace A  The use of locks based on the assumption that conflicts between transactions is likely is referred to as pessimistic locking.  During a transaction the data is inconsistent (because transaction might be updating some data) thus locks are required to prevent another transaction from reading inconsistent data.  All lock info. is handles by a lock manager, which is responsible for assigning and policing the locks used by the transactions. Lock Granularity: Lock granularity indicates the level of lock use. Locking can take place at the following levels:  DB level: In a DB lock the entire DB is locked, thus preventing the use of any tables in the DB by T2 while T1 is executed. DB lock is good for batch process, but not good for multiuser DBMSs, because wait is too long.  Table Level: In a table-level lock, the entire table is locked, preventing access to any row by T2 while t1 is using the table. Here two transactions can access Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace the same DB if they accessing different tables. Also not suitable for multiuser DBMSs, because it causes traffic jams when many transactions want to access same tables.  Page Level: In a page-level lock, the DBMS locks an entire diskpage. A diskpage, or page, is the equivalent of a diskblock, which can be described as a directly addressable section of a disk. A table can span several disk. Most frequently used locking method for multiuser DBMSs. Two transactions can access the same table. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace  Row Level: A row-level lock is much less restrictive than the previous discussed locks. The DBMS allows concurrent transactions to access different rows of the same table. It improves availability if data, but its management requires high overhead.  Field Level: The field-level lock allows concurrent transactions to access the same row as long as they require the use of different fields (attributes) within that row. Field level yields the most flexibility, but it is rarely used. Lock Types:  Binary Lock: has only two states: locked (1) or unlocked (0). If object such as Db, table, page, etc. is locked then no other transaction can use that object. Every DB operation requires that the affected object be locked. Transaction must unlock the object after its termination. Thus every transaction requires a lock and unlock operation for each accessed data item and it is automatically done by the DBMS. E.g.: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Binary is not used since it is too restrictive because it doesn’t allow for two transactions to read the same object even though they are not updating the data thus not making any concurrency problems.  Shared/Exclusive lock: An exclusive lock exist when access is reserved specifically for the transaction that locked the object. Exclusive lock is used when the potential for conflict exists. exclusive lock: is issued when a transaction requests permission to update a data item and no locks are held on that data item by any other transaction. A shared lock exist when concurrent transactions are granted read access on the basis of a common lock. A shared lock produce no conflicts as long as all the concurrent transactions are read-only. Shared lock: A lock that is issued when a transaction requests permission to read data from a DB and now exclusive locks are held on the data by another transaction. Using a shared/exclusive locking concept there exist 3 states:  Unlocked Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace  Shared (read)  Exclusive (write) Shared lock allows several transaction to read data concurrently. Exclusive lock is granted if and only if no other locks are held on the data item and it is known as the mutual exclusive rule: Only one transaction at a time can own an exclusive lock on an object. If a shared lock is already held on a data item X by T1, then an exclusive lock cannot be granted to transaction T2. A shared/exclusive lock schema increases the lock manager’s overhead for these reasons:  Locks can lead to two major problems:  The resulting transaction schedule might not be serializable  The schedule might create deadlocks. A deadlock occurs when two transactions wait indefinitely for each other to unlock data. A deadlock occurs when two or more transactions wait for each other to unlock data. Both these problems can be solved: Serializability can be attained by locking protocol called Two-Phased Locking and deadlocks can be managed by using deadlock detection and prevention techniques. Two-Phase Locking protocol to Ensure Serializability: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace A  Two-phase locking (2PL) defines how transactions acquire and relinquish (give up) locks.  It guarantees serializability, but it does not prevent deadlocks.  The 2 phases are:  A growing phase: in which a transaction acquires all required locks without unlocking any data. Once all locks are acquired, the transaction is in its locked point.  A shrinking phase: in which a transaction releases all locks and cannot obtain a new lock. The 2PL are governed by the following rules: Deadlocks  Occurs when 2 transactions wait indefinitely for each other to unlock data. Think of transaction locking an object as transaction using an object Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace  Deadly embrace occurs when: So T1 locks data item X (i.e. uses it) and then is waiting for Y to be unlocked at the same time T2 locks data item Y and wait for X  Deadlocks only occur one of the transactions want to obtain an exclusive lock on a data item. 3 basic techniques to control deadlocks are: Think control is prevent, detect and avoid  Deadlock prevention: A transaction requesting a new lock is aborted when there is the possibility that a deadlock can occur. The transaction is later rescheduled for execution.  Deadlock detection: The DBMS periodically tests the Db for deadlocks.  Deadlock avoidance: The transaction must obtain all of the locks it needs before it can be executed. This technique avoids rolling back of conflicting transactions. concurrency Control with Time Stamping Methods Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace  The time stamping approach to scheduling concurrent transactions assigns a global unique time stamp to each transaction.  The time stamp produces an explicit order in which transactions are submitted to the DBMS.  Time stamping must have 2 properties: o Uniqueness: ensures no equal time stamp values can exist o Monotonicity: ensures that time stamp values always increase.  All DB within the same transaction must have the same time stamp  The DBMS executes conflicting operations in time stamp order, thus ensuring serializability of the transactions  If two transactions conflict, one is stopped, rolled back and rescheduled.  Disadvantage of time stamping is that each field requires new fields for the time stamp, thus increasing memory and processing resources. Wait/Die and Wound/Wait Schemes  These two schemes decide which transaction is rolled back and which continues executing.  Assume T1 has time stamp 11548789 and T2 has 19562545, thus T1 is oldest because it has lowest value (been alive longer been alive until 11548789).  Given the table: T1 is older transaction  In wait/die the older transaction wait for the younger transaction to complete and release the locks before requesting the locks itself.  In wound/wait an older transaction rolls back the younger transaction and reschedules it. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace  If transaction requesting lock is older of the two, it will roll back the younger transaction and the younger one is rescheduled  If transaction requesting lock is younger than younger waits until older is complete and locks are released. IF a transaction requests multiple locks than how long does a transaction have to wait for each lock request? It can cause transactions to wait indefinitely. To prevent this each lock request has an associated time-out value. Concurrency Control with optimistic methods:  Optimistic approach is based on the assumption that the majority of DB operations do not conflict. THINK: we are optimistic about it not conflicting.  This approach does not require locking nor time stamping. Instead a transaction is executed without restriction until it is committed.  In optimistic approach each transaction goes through 3 phases: ANSI Levels of transactions isolation o Transaction isolation refer to degree to which transaction data is protected or isolated from other concurrent transactions. o Transaction isolation levels are described based on what other transactions can see (read) during execution, more precisely by the type of “reads” that a transaction allows or not. The types of reads are:  Dirty read: a transaction can read data that is not yet committed.  Nonrepeatable read: a transaction reads a given row at time t1, an then it reads the same row at time t2, yielding diff. results. THINK: NON repeat thus can’t read the same row in same condition Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace  Phantom read: a transaction executes a query at time t1, and then it runs the same query at time t2, yielding additional rows that satisfy the query. o ANSI defined 4 levels of transaction isolation:  Read Uncommitted: will read uncommitted data from other transactions. At this level DB does not place any locks on the data which increases transaction performance, but at cost of data inconsistency.  Read committed: only committed data is read. Default mode for most DBs. Locks are used. Causes transactions to wait until the original transaction commits.  Repeatable read: ensures that queries return consistent results. Here shared locks are used to ensure other transactions do not update a row after the original query reads it. However, new rows are read since these rows did not exist when the 1st query ran.  Serializable: most restrictive level. Even here deadlocks are possible even though it is most restrictive. o The levels go from READ COMIITED (least restrictive) to SERIALIZABLE (most restrictive) with the higher the level having the most locks, but with the least transaction concurrency performance. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace A E E o The isolation level of a transaction is defined in the transaction statement for example: o Oracle and MS SQL servers use the statement Database Recovery Management  DB recovery is process of restoring a DB from a given state (usually inconsistent) to a previous consistent state; restoring data that have been lost, accidentally deleted, or corrupted or made inaccessible.  Recovery techniques are based on the atomic transaction property: all portions of the transaction must be treated as a single, logical unit of work in which all operations are applied and completed to produce a consistent DB.  Transaction recovery reverses all of the changes that the transaction made to the DB before the transaction was aborted.  Recovery also apply to DB and to the system after some critical error has occurred.  Example of critical events:  Hardware/software failures: hard disk failure, etc.  Human-caused incidents:  Natural disasters: earthquakes, etc. Transaction Recovery o DB transaction recovery uses data in the transaction log to recover a DB from an inconsistent state to a consistent state. o 4 concepts affecting recovery process: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace E - The write-ahead-log protocol ensures that transaction logs are always written before any DB data is actually updated. Ensures that in case of failures the DB can restore the data. - Redundant transaction logs (several copies of the logs) ensure that a physical disk failure will not impair the DBMS’s ability to recover data - DB buffers are temporary storage areas in primary memory (RAM) used to speed up disk operations. - DB checkpoints are operations in which the DBMS writes all of its updates buffers in memory (RAM) to disk. Describe to them the database recovery process you will follow to recover the database. [6] I would use transaction recovery procedures, which are based on the atomic transaction property. The transaction log contains data for database recovery purposes. Database transaction recovery uses data in the transaction log to recover a database from an inconsistent state to a consistent state.  Write-ahead-log protocol – Transaction logs are written before any data is actually updated.  Redundant transaction logs ensure that a disk failure will not impair the recovery.  Database checkpoints are performed regularly to write buffer content to disk and update the logs. Checkpoints play an important role in transaction recovery. Think DB recovery process: TRP based ATP, log; WRD o Transaction recovery procedures make use of deferred-write and write-through techniques. o When recovery procedure uses the deferred-write technique (also called a deferred(delayed<-synonym) update), the transaction operations do not immediately update the physical DB, instead only the transaction log is updated. The DB is physically updated only with data from committed transactions using the info. from the transaction log. The recovery process for deferred-write technique are: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace E o When the recovery procedure uses a write-through technique (also called an immediate update), the DB is immediately updated by transaction operations during the transaction’s execution, even before the COMMIT statement. If transaction aborts before COMMIT, a ROLLBACK operation need to be done to restore the DB to a consistent state. The ROLLBACK operation will use the transaction log “before” values. The recovery process is: Also describe to them the use of deferred-write and write-through techniques. [6]  Deferred-write technique (aka deferred update) – the transaction operations do not immediately update the physical database, only the transaction log is updated. The database is physically updated only with data from committed transactions, using information from the transaction log. If the transaction aborts before it reaches its commit point, no changes (no ROLLBACK or undo) need to be made to the database because it was never updated.  Write-through technique (aka immediate update) – the database is immediately updated by transaction operations during the transaction’s execution, even before the transaction Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace reaches its commit point. If the transaction aborts before it reaches its commit point, a ROLLBACK or undo operation needs to be done to restore the database to a consistent state. Think: Deferred wait until update, write through immediate update o Here is a DB recovery process that you need to understand Remember TRL ID is transaction line ID Explanation: Transaction 106 won’t be saved to disk since it has not been COMMITED before the CHECKPOINT. Also trace a DB recovery process using the deferred update for this table: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace * DML statement is Data manipulation statement, statement that updated data End Chapter Summary: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace A E Chapter 11 - DB performance Tuning and Query Optimization Db performance-Tuning Concepts  Main function of DB system is to provide timely answers to end users.  End users interact with the DBMS through the use of queries to generate info., using the following sequence:  Goal of DB performance is to execute queries as fast as possible, thus DB must be closely monitored and regularly tuned.  DB performance tuning refers to a set of activities and procedures designed to reduce the response time of the DB system, i.e. to ensure that an end-user query is processed by the DBMS in the minimum amount of time.  Performance of typical DBMS is constrained by these main factors: - CPU - RAM - input/output (hard disk and network) throughput  The most important DB design saying is: Good DB performance starts with good DB design.  Some general guidelines for better system performance table: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace A E E  Good Db design is to ensure that the design makes use of features in the DBMS that guarantee the integrity and optimal performance of the DB. Performance Tuning: Client and server Performance tuning can be divided into those of the client side and those of the server side. - on the client side: the objective is to generate a SQL query that returns the correct answer in the least amount of time, using the min. amount at the server end. This is called SQL performance tuning - On the server side: the DBMS environment must be properly configured to respond to client’ requests in the fastest way possible, while making optimal use of existing resources. The activities to achieve this goal is DBMS performance tuning. - Network plays critical role in delivering messages between clients and servers, this especially important in distributed DBs. before learning about performance tuning we learn about DBMS architecture: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace A E DBMS architecture:  The architecture of a DBMS is represented by the processes and structures (in memory and permanent storage) used to manage a DB.  Basic DBMS Architecture: Explanation of components of DBMS:  All data in a DB is stored in data files. A data file can contain rows from a single table, or it can contain rows from many different tables diff. tables. A DB admin (DBA) determine the initial size of the data files the make up the DB, however the files can automatically expand as required in predefined increments known as extents. Extent is increment in which data files can increase, e.g. 10 MB  Data files are grouped in file groups or table spaces which is logical grouping of several data files that store data with similar characteristics. E.g. a table space that stores info. from the data dictionary. Each time a Db is created, the DBMS auto. creates a min set of table spaces.  The data cache, or buffer cache, is shared, reserved memory area that stores the most recently accessed data blocks in RAM. The data read from the data files is stored in the data cache after the data has been read or before the data is written to the data files. This cache is in the RAM and it acts as RAM between hard disk and BBMS process Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace E in RAM. Data cache also caches system catalog data and contents if the indexes.  The SQL cache, or procedure cache, is shared, reserved memory area that stores the most recently executed SQL statements or PL/SQL (advanced SQL statements)procedures, including triggers and functions. It doesn’t store the SQL code, rather the processed version of the SQL that is ready for execution by the DBMS.  DBMS must retrieve the data from the permanent storage to work with the data and place it in RAM. i.e. the data is retrieved from the data files and placed in the data cache.  To move data from permanent storage to RAM, the DBMS issues I/O requests and waits for the replies. An input/output(I/O) request is a low level data access operation that reads or writes data to and from comp. devices.  Working with data in the data cache is many times faster than working with it in data files, because the DBMS does not have to wait for the hard disk to retrieve the data; no hard disk I/O operation needed.  Most performance-tuning activities focus on minimizing the number of I/O operations because using I/O operations is many times slower than reading data from the data cache. Typical DBMS processes: In the picture above o Listener: The listener process listens for clients’ requests and handles the processing of the SQL requests to other DBMS processes. Once request is received, the listener passes the request to the appropriate use process. o User: The DBMS creates a user process to manage each client session. When a user log on to the DBMS, the user is assigned a user process. This process handles all requests you submit to the server. o Scheduler: The scheduler process organizes the concurrent execution of SQL requests. o Lock manager: This process manages all locks placed on DB objects, including disk pages. o Optimizer: The optimizer process analyses SQL queries and finds the most efficient way to access the data. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace DB query optimization Modes: Most of the algorithms proposed for query optimization are based on 2 principles:  The selection of the optimal execution order to achieve the fastest execution time.  The selection of sites to be accessed to minimize communication costs. Within those 2 principles, a query optimization algorithm can be evaluated on the basis of its operation mode or the timing of its optimization (i.e. when the optimization is done) Operation modes can be classified as:  Automatic query optimization mean that the DBMS finds the most cost- effective access path without user intervention  Manual query optimization requires that the optimization be selected and schedules by the end user or programmer. This optimization is more desirable from the end user’s point of view, but at the cost of increased overhead that it imposes on the DBMS Timing of optimization (when the optimization is done)can be classified as:  Static query optimization takes place at compilation time, i.e. the best optimization strategy is selected when the query is compiled by the DBMS. Here when the program is submitted to the DBMS for compilation, it creates the plan to access the DB. When the program is executed, the DBMS uses that plan to access the DB.  Dynamic query optimization takes place at execution time. DB access strategy is defined when the program is executed. Thus access strategy is dynamically determined by the DBMS at run time, using the most up- to-date info. about the DB. The best strategy is determined several times in the same program. Query optimization techniques can be classified according to the type of info. that is used to optimize the query:  Statistically based query optimization algorithm uses statistical info. about the DB. The statistics provide info. about DB characteristics such as size, number of records, average access time, etc. These statistics is then used by the DB<S to determine the best access strategy. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace A The statistical info. is managed by the DBMS and is generated in one of the 2 different modes: - dynamic statistical generation modes: Here the DBMS automatically evaluates and updates the statistics after each data access operation. - Manual statistical generation mode: Here the statistics must be updated periodically through a user-selected utility.  Rule-Based query optimization algorithm is based on a set of user- defined rules to determine the best query access strategy. Rules are entered by user or DB admin. It’s a query optimization technique that uses preset rules and points to determine the best approach to executing a query. DB Statistics:  Gathering DB statistics play important role in query optimization  DB Statistics refers to a number of measurements about DB objects, such as number of processors used, processor speed, and temporary space available.  The DBMS uses these statistics to make critical decisions about improving query processing efficiency.  It can be gathered manually (by DBAdmin) or automatically by the DBMS.  Sample measurements that are gathered are shown in the table:  To generate the DB object statistically manually, each DBMS has its own commands: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace A E E  When you generate statistics for a table, all related index  DB statistics are stored in the system catalog.  The more current the statistic were generated the faster the DBMS can access something uses the statistic. Query processing A DMBS processes a query in 3 phases (Query processing phases): o Parsing: The DBMS parses (analyse) the SQL query and chooses the most efficient access/execution plan. o Execution: DBMS executes the SQL query using the chosen execution o Fetching: DBMS fetches the data and sends the result back to the client. Think: PEF Query processing described graphically: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace E Think: pSAeDDf; SD cache In Parsing: SNADGS (1st 3 is checks) In Execution: EARP (EE) Describing the phases:  SQL Parsing Phase: - Optimization process includes breaking down –parsing- the query into smaller units and transforming the original SQL query into slightly different version of the original SQL code, but one that is fully equivalent and more efficient. - Fully equivalent means that the optimized query results are always the same as the original query. - More efficient means that the optimized query will almost always execute faster than the original query - The SQL parsing activities are performed by the query optimizer. - Parsing a SQL requires several steps in which the SQL query is: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace  Once the SQL statement is transformed, the DBMS creates an access plan or execution plan.  An access plan is the result of parsing a SQL statement; it contain a series of steps a DBMS will use to execute the query and return the result set in the most efficient way.  1 st, DBMS checks to see if an access plan exists for the query in the SQL cache. IF it does, the DBMS reuse it, if not the optimizer evaluates various plans and then decides which indices to use and how to best perform join operations. If a new plan is used it is saved in the SQL cache for late use.  Access plans are DBMS specific and translate the client’s SQL query into the series of complex I/O operations required to read the data from the physical data files and generate the result set.  Some DBMS access plan I/O operations:  Execution Phase: o In this phase all I/O operations indicated in the access plan are executed. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace E E o When the execution is run, the proper locks-if needed-are required for the data to be accessed and the data is retrieved from the data files and placed in the DBMSs data cache. o All transaction (an input message to a computer system dealt with as a single unit of work) management commands are processed during the parsing and execution phases of query processing. All transaction management command parsing and execution  Fetching Phase:  In this phase all rows that match the specified condition(s) are retrieved, sorted, grouped, and aggregated (if required).  In this phase the resulting query set are returned to the client.  The Db server coordinates the movement of the result set rows from the server cache to the client cache. Query Processing Bottlenecks To do with processing I/O. makes delays  More complex the query the more complex the operations (that is broken up into) which means bottlenecks are more likely.  A query processing bottleneck is a delay introduced in the processing of an I/O operation that causes the overall system to slow down.  The more components a system have, the more interfacing, thus increasing likelihood of bottlenecks.  Within a DBMS, 5 components causes bottlenecks: - CPU: High CPU utilization might indicate that CPU processor is too slow. - RAM: There must be enough RAM - Hard disk - Network - Application Code Bottlenecks are the result of multiple DB transactions competing for the use of DB resources (CPU, RAM, hard disk, indexes, buffers, etc.) Because most transactions work with data rows in tables, one of the most typical bottlenecks is caused by transactions competing for the same data rows. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Indexes and Query Optimization  Indexes are crucial in speeding up data access because they facilitate searching, sorting, and using aggregate functions and even join operations.  An index speed up data access because an index is an ordered set of values that contains the index key and pointers.  Pointer are the row IDs for the actual table rows.  An index scan is more efficient than a full table scan because the index data is pre-ordered and the amount of data is usually much smaller.  Example: So if there is no index, the DBMS will perform full-table scan and read all 14786 rows. But if there is an index created like the one in the picture, i.e. STATE_NDX index of the column CUS_STATE, the DMBS will automatically use the index to locate the 1 st customer with state equal to FL and then proceed to read all subsequent rows which have FL as the key. Thus saving the DBMS to read 14776 I/O request  Not every column is indexed, because indexing every column in every table overtaxes the DBMS in terms of index-maintenance processing. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace I  A measure that determines the need for an index is the data sparsity of the column you want to index.  Data sparsity refers to the number of different values a column could have. e.g. STU_SEX (gender) can have 2 possible values M and F, thus we say the column has low sparsity.  So a column that have a small number of different values have low sparsity and a column that has many different values have high sparsity.  Knowing the sparsity helps you decide whether the use of an index is appropriate.  When performing search with column with low sparsity, you are likely to search high percentage of table anyway, thus index processing will be unnecessary work.  Thus we index columns that have high sparsity Most DBMS implement indexes using one of the following data structures: HBB indexes. Think HBO Example: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Optimizer Choices:  Query optimization is the central activity during the parsing phase in query processing.  In the parsing phase the DBMS must choose what indexes to use, how to perform join operations, which table to use first, and so on.  The query optimizer can operate in one of 2 modes: - A rule-based optimizer uses pre-set rules and points to determine the best approach to execute a query. The rules assign a “fixed cost” to each SQL operation; the costs are then added to yield the cost of the execution plan. E.g. full table scan has set cost of 10, while a table access by row ID has a set cost of 3. - A cost-based optimizer uses sophisticated algorithms based on statistics about the objects being accessed to determine the best approach to execute a query. Here the optimizer process adds up the processing cost, the I/O costs, and the resource costs to determine the total cost of a given execution plan.  The optimizer’s objective is to find alternatives ways to execute a query- to evaluate the cost of each alternative and then choose the one with the lowest cost.  To display the optimizer’s function look at this example: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Assume you want to list all products provided by a vendor based in Florida. To do this you write this query: Only the 1st 2 bullets (7000 rows in PRODUCT and 300 rows in VENDOR_ are available to the optimizer. The 2nd two are assumes to illustrate the choices that optimizer must make. Primary factor in determining the most efficient access plan is the I/O cost. This table show two sample access plans: Note: The TOTAL I/O COST column just keep on summing the values in the column for each plan. Also in both plans the Cartesian product must be taken, but in plan B it takes Cartesian product of table B1 which is much smaller. Plan B will be the best here. With the right conditions, some queries could be answered entirely using only an index. E.g. assume you using PRODUCT table and index P_QOH_NDX in the P_QOH attribute (column). Then query such as SELECT MIN(P_QOH) FROM Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace PRODUCT could be resolved by reading only the 1st entry in the index, since indexes are in ascending order. Using Hints to Affect Optimizer Choices:  In some instances optimizer might not choose best plan, because it uses statistics and if statistics is old it might choose the wrong plan.  So if the end user wants to change the optimizers mode for the current SQL statement.  To do this Optimizer hints are used which is special instructions for the optimizer that are embedded inside the SQL command text. Some of the most common optimizer hints used in standard SQL: SQL performance Tuning This is what can the user (client) do to write the code more efficient so that access can be faster. SQL performance tuning is evaluated from the client perspective and the goal is to use common practices used to write efficient code. A carefully written query almost always outperforms a poorly written one. Index Selectivity The key is to know when an index is used. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace In general indexes are likely to be used: Indexes are very useful when you want to select a subset of rows from a large table on a given condition. Objective is to create indexes with high selectivity which is a measure of the likelihood that an index will be used in query processing. General guidelines for creating and using indexes. - Create indexes for each single attribute used in a WHERE, HAVING, ORDER BY or GROUP BY clause. If this is done the DBMS will access table using index scan instead of full table scan. E.g. if you have index for P_PRICE attr. the condition P_PRICE>10.00 can be solved using index - Do not use indexes in small tables or tables with low sparsity. Small tables can be quickly scanned so no need. And low sparsity means many repeating values, thus search condition will return high percentage of rows, thus making indexing useless. - Declare primary and foreign keys so the optimizer can use indexes in join operations. Declaring PK, FK will auto. create an index for the declared column, thus all joins will benefit if you declare PK and FK. - Declare indexes in join columns other than PK or FK. If you perform join operation on columns other than FP or FK then it is good to declare indexes for those columns. You cannot always use an index to improve performance. E.g. an index for P_MIN will not help the search condition P_QOH > P_MIN *1.10 since it is using 2 attributes which is a function. Major DBs such as oracle, SQL server, etc. support function-based indexes. Function-based index is an index based on a specific SQL function or expression. It is very useful with derived attributes (attributes that was created Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace from other attributes in a function). Example on Function based index is an index on EMP_SALARY + EMP_COMMISSION. It is important to constantly evaluate the index usage-monitor, test evaluate and improve it if performance is not adequate. Conditional Expressions A conditional expression is normally placed within the WHERE and HAVING clauses of a SQL statement. It restricts the output of a query to only rows that match the conditional criteria. Conditional Criteria general forms: The following practices are used to write efficient conditional expressions in SQL code:  Use simple columns or literals as operands in a conditional expression – avoid use of conditional expressions with functions whenever possible for operands. e.g. P_PRICE > 10.00 is faster than P_QOH > P_MIN*1.10  Numeric field comparisons are faster than character, date and NULL comparisons. NULL conditions are the slowest of all conditional operands, because indexes do not store references to null values.  Equality comparisons are generally faster than inequality comparisons. E.g. P_PRICE = 10 is faster than P_PRICE > 10 because the DBMS can do a direct search using the index in the column.  Whenever possible, transform conditional expressions to use literals. E.g. if your condition is P_PRICE – 10 = 7, change it to P_PRICE = 17. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace  When using multiple conditional expressions, write the equality (=) conditions first. because equality conditions are faster to process than inequality conditions.  If you use multiple AND conditions, write the condition most likely to be false first. If you use this when DBMS get to the false it will stop evaluating the rest, thus saving time.  When using multiple OR conditions, put the condition most likely to be true first. Same as before, when DBMS get to the true condition it will stop evaluating the rest since all will be TRUE.  Whenever possible, try to avoid the use of the NOT logical operator. It is best to transform an expression containing a NOT operator into an equivalent expression. E.g.: Query Formulation  Queries are written to answer questions.  Here we focus on SELECT queries since they are found in most applications.  So if user gives you a sample output and tells you to match the output you must write it in SQL  To formulate a query these steps are followed: - identify what columns and computations (calculations) are required. - Identify the source tables. What tables must be in the SQL query. Once you know the columns are required, you can determine the source tables used in the query. - Determine how to join the tables. Once you know what tables should be used. i.e. natural join, outer join, etc. - Determine what selection criteria are needed. Determine what operands and operators are needed in your criteria. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace A - Determine the order in which to display the output. You will need to use ORDER BY clause. DBMS performance Tuning  DBMS performance tuning includes global tasks such as managing the DBMS processes in primary memory (allocating memory for caching purposes) and managing the structures in physical storage (allocating space for the data files) DBMS PT PM PS DBMS performance tuning at the server end focuses on setting the parameters used for:  Data cache: Data cache size must be set large enough to permit as many data requests as possible to be serviced from the cache. Each DBMS has settings that control the size of the data cache.  SQL cache: SQL cache stores the most recently executed SQL statements (after it has been parsed). Here statements that already have been parsed are saved so it can be used again.  Sort Cache: Used as a temporary storage area for ORDER BY or GROUP BY operations ,as well as for index-creation functions.  Optimizer mode: Either cost-based or rule-based optimization mode. If the entire DB could be stored in primary memory it would be useful to minimize costly disk access. In-memory DB systems are optimized to store large portions (if not all) of the DB in primary (RAM) storage rather than secondary (disk) storage. These DB systems are still subject to query optimization and performance tuning rules. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Most DBs are still stored on secondary memory. Thus the following are general recommendations for physical storage of DBs: o Use I/O accelerators: This type of device uses flash solid-state drives(SSD) to store the DB. SSDs are faster o Use RAID (Redundant Array of independent Disks) to provide both performance improvements and fault tolerance, and balance between them. Fault tolerance means that in case of failure, data can be reconstructed and retrieved. o Minimize disk contention (conflict). Remember DB consists of many table spaces, each with a particular function. Each table space is composed of several data files in which data is actually stored. A DB should have at least the following table spaces: o Put high-usage tables in their own table spaces so the DB minimizes conflict with other tables. o Assign separate data files in separate storage volumes for the indexes, system, and high-usage tables. This ensures that index operations will not conflict with end-used data or data dictionary table access operations. o Take advantage of the various table storage organizations available in the DB. E.g. in Oracle consider the use of index-organized tables. Indexed organized table is a table that stores the end-user data and the index data in consecutive locations on permanent storage. This storage organization provides an advantage. o Partition tables based on usage. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace o Use denormalized tables where appropriate. You can increase performance by taking tables form higher normal form to lower normal form. o Store computed and aggregate attributes in tables. i.e. use derived attributes in your tables. Query optimization Example: Go through the example in the book. p538 – 544 As a DBA, you should be aware that the main goal is to optimize overall Db performance-not just for a single query but for all requests and query types. Most DB systems provide advanced graphical tools for performance monitoring and testing. Summary: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace E E Chapter 12 - Distributed DB management Systems The evolution of distributed DB management Systems  A distributed DB management system (DDBMS) governs the storage and processing of logically related data over interconnected computer systems in which both data and processing are distributed among several sites.  To understand DDBMS let’s look at centralized DBMS. In centralized DBMS corporate data is stored in singe central site. It falls short when quickly moving events required faster response times and equally quick moving access info.  As large business units restructured to form leaner, quickly reacting, dispersed operations 2 DB requirements became obvious: - Rapid ad hoc(spur of the moment) data access became crucial in the quick-response decision making environment - Distributed data access was need to support geographically dispersed business units.  Centralized DB management is subject to problems such as: High cost because in the middle is mainframe computer which hold all the data and it is expensive maintaining this mainframe comp/  A multiple-source/multiple-location DB environment is best managed by a DDBMS. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace E DDBMS Advantages and Disadvantages The distributes Db is usually based on the relational DB model. The lack of standards Distributed Processing and Distributed DBs - In distributed processing, a DBs logical processing is shared among two or more physically independent sites that are connected through a network. E.g. the data I/O, data selection, and data validation might be Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace performed on one computer, and a report based on that data might be created on another computer. - Illustration of this concept: - The distributed processing system uses only a single-site DB, but shares the processing chores among several sites. Note: The DB is at 1 site, each site can access data and update the DB. The computer at the DB site (comp A) is known as the DB server. - A distributed DB, on the other hand, stores a logically related DB over 2 or more physically independent sites. The sites are connected via a computer network. - Here a DB is composed of several parts known as DB fragments which are located at different sites and can be replicated among various sites. - Each DB fragment is managed by its local DB process. - Illustrated by: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace A Note: DB is divided into 3 parts DB fragments (E1, E2, E3) located at diff sites. Note the following points about the 2: Characteristics of Distributed DBMSs In DDBMS both data and processing functions are distributed among several sites. A DBMS must have at least the following functions to be classified as distributed: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace A A A fully DDBMS must perform all the functions of a centralized DBMS, as follows: In addition, a fully DDBMS must also handle all necessary functions imposed by the distribution of data and processing, and it must perform those additional functions transparently to the end user. Tranparantly means that end users see only one logical DB although it exists out of 2 or more DBs. Illustrated by this picture: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace This picture shows the DDBMS’s transparent data access features. Here both Tom and Mary see only one logical DB and do not need to know the fragments. DDBMS components:  Computer workstations or remote devices (sites or nodes) that form the network system. DDBMS is independent of the comp. system hardware  Network hardware and software components that reside in each workstation or device. They allow all the sites to interact and exchange data.  Communications media that carry the data from one node to another. This is like the links between them and Network hardware and software is inside the computers.  Transaction processing (TP) is the software component found in each computer or device that requests data. The transaction processor receives and processes the applications remote and local data requests. The TP is also known as the application processor (AP) or the transaction manager (TM)  Data processor (DP) is the software component residing on each computer or device that stores and retrieves data located at the site. The DP is also known as the data manager (DM) 567 Illustration of the components: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Communication among TPs and DPs is made possible through a specific set of rules, or protocols, used by the DDBMS. The protocols determine how the distributed DB system will: A TP and DP reside on the same computer, allowing end user to access both local and remote data transparently. Levels of data and process distribution DB systems can be classified on the basis of how process distribution and data distribution are supported. E.g. DBMS may store data in a single site or multiple sites (data distribution) and it may support data processing at one or more sites. Illustrated by this table: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace To read this table. The 2nd column 1 st row with the entry of ‘Host DBMS’ is a Single-site process and single-site data and it’s advantage is Host DBMS. Single-Site Processing, Single-Site Data  Here all processing is done on single host computer and all data is stored on the host computer’s local disk.  DBMS is on the host computer, which is accessed by terminals connected to it.  Here TP and DP are embedded within the DBMS on the host computer. Multiple-Site Processing, Single-Site Data  Here multiple processes run on different computers that share a single data repository.  Requires a network file server running conventional applications that are accessed through a network. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Some properties of this setup: - All record- and file-locking activities are performed at the end-user location - All data selection, search, and update, functions take place at the workstation, this requiring that entire files travel through the network for processing at the workstation. Such requirements increases network traffic, slows response time and increases communication costs. A variant of the multiple-site processing, single-site data is the client/server architecture, except with the difference that all Db processing is done at the server site, thus reducing network traffic. Multiple-Site Processing, Multiple-Site Data o Describes a fully distributed DBMS with support for multiple data processors and transaction processors at multiple sites. o DDBMSs are classified as either homogenous or heterogeneous. - Homogenous DDBMSs integrate multiple instances of the same DBMS over a network- e.g. multiple Oracle 11g running on diff platforms - Heterogeneous DDBMSs integrate different typed of DBMSs over a network, but all support the same data model. - A fully heterogeneous DDBMS will support different DBMSs, each one supporting a different data model, running under different computer systems. o A DDBMS is subject to certain restrictions: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Distributed DB transparency features Db transparency is hiding complexities away from the user so that the user don’t have to worry about all the complexities.  A distributed DB system should provide some transparency features that make all the system’s complexities hidden to the end user.  The minimum desirable DDBMS transparency features are: All about distributing (spreading) the properties of the DBMS - Distributed transparency allows a distributed DB to be treated as a single logical DB. If a DDBMS has this feature that the user don’t have to worry about: - Transaction transparency allows a transaction to update data at more than one network site. Also ensures the transaction will be either entirely completed or aborted - Performance transparency allows the system to perform as if it were centralized DBMS. Also ensures that the system will find the most cost-effective path to access remote data. - Heterogeneity transparency allows the integration of several different local DBMSs (relational, network, hierarchical) under a common, or global, schema. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace A A E E E distribution Transparency This allows a physically dispersed DB to be managed as though it were a centralized DB. The level of transparency supported by the DDBMS varies from system to system. 3 levels of distribution transparency are recognized:  Fragmentation transparency highest level of distribution transparency. The end user or programmer does not need to know that the DB is partitioned. Thus fragment name neither fragment locations are specified prior to data access.  Location transparency exists when the end user or programmer must specify the DB fragments names, but doesn’t not need to specify where those fragments are located, i.e. the location.  Local mapping transparency exists when the end user or programmer must specify both the fragment names and their locations. Think local thus gives location and mapping gives fragment names. Think: FLLM, also since we dealing in fragment we give fragment names or location To illustrate Distribution transparency : Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Suppose this table is fragmented and each fragment is unique. Unique fragments condition indicates that each row is unique, regardless of the fragment in which it is located. Now suppose we want to list all the employees born before Jan 1, 1960: The 3 query cases:  CASE 1: The DB supports Fragmentation Transparency: Note: User did not specify fragment names or locations.  CASE 2: The DB supports location Transparency: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Note: We specify fragments names in the query, but not fragment locations  CASE 3: The DB supports local mapping Transparency: Note: Both fragment names and locations need to be specified. Distribution transparency is supported by a distributed data dictionary(DDD) or a distributed data catalog(DDC). The DDC contains the description (it describes it) of the entire DB as seen by the DB administrator. The DB description, known as the distributed global schema, is the common DB schema used by local TPs to translate user requests into subqueries (remote requests) that will be processed by different DPs. TP requests data and DPs stores and retrieves the data. The DDC itself is distributed, and it is replicated at the network nodes, thus it must contain consistency through updating at all the sites. Transaction transparency Transaction transparency is a DDBMS’s property that ensures DB transactions will maintain the distributed DB’s integrity and consistency. Transaction transparency ensures that the transaction will be completed only when all DB sites involved in the transaction complete their part of the transaction. To understand how the transactions are managed, you should know the basic concepts governing remote requests, remote transactions, distributed transactions, and distributed requests. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Distributed Requests and Distributed Transactions  Difference between nondistributed transaction and a distributed transaction is that the distributed transaction can update or request data from several different remote sites on a network.  A remote request lets a single SQL statement access the data that are to be processed by a single remote (distant) DB processor, i.e. the SQL statement can reference data at only one remote site.  Similarly, a remote transaction which is composed of several requests, accesses data at a single remote site note the following properties of a Remote transaction:  A distributed transaction can reference several different local or remote DP sites. Although each single request can reference only one local or Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace remote DP site, the transaction as a whole can reference multiple DP sites Note:  There may occur a problem where PRODUCT table is divided into two fragments and is located at site B and C. Given the following SQL statement will have problems since the request cannot access data from more than one remote site:  Thus the DBMS must be able to support a distributed request.  A distributed request lets a single SQL statement reference data located at several different local or remote DP sites.  Because each request (SQL statement) can access data from more than one local or remote DP site, a transaction can access several sites.  The ability to execute a distributed request provides fully distributed DB processing because you can: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Note: Note in this transaction a single SQL statement references tow tables that are located at different locations (sites)  Full-fragmentation transparency support is provided only by a DDBMS that supports distributed requests as illustrated here: CUSTOMER table if fragmented at two sites,  Transaction transparency ensures that distributed transactions are treated as centralized transactions, ensuring their serializability. That is the execution of concurrent transactions, whether they are distributed or not, will take the DB from one consistent state to another. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace E Distributed Concurrency Control Concurrency control very important in distributed DB, because multisite, multiple-process operations are more likely to create data inconsistencies and deadlocked transactions than single-site systems. The TP component of a DDBMS must ensure that all parts of the transaction are completed at all sites before a final COMMIT is issued to record the transaction. WE have a problem, say transaction updates data at 3 DP sites, 1st 2 DP sites complete transaction and commit the data, however the 3rd DP site cannot commit the transaction. PROBLEM because it yields inconsistencies because committed data cannot be uncommitted. THUS we have two-phase commit protocol. Two-Phase commit protocol  For distributed DBs a final COMMIT must not be issued until all sites have committed their parts of the transaction.  The two-phase commit protocol (2PC) guarantees that if a portion of a transaction operation cannot be committed, all changes made at the other sites participating in the transaction will be undone to maintain a consistent DB state. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace  Each DP maintains its own transaction log.  The two-phase commit protocol requires that the transaction log entry for each DP be written before the DB fragment is actually updated.  This protocol has a DO-UNDO-REDO protocol. The DO-UNDO-REDO protocol is used by the DP to roll transactions back and forward with the help of the system’s transaction log entries.  The DO-UNDO-REDO protocol defines 3 types of operations:  To ensure the DO, UNDO, and REDO operations can survive a system crash while they are being executed, a write-ahead protocol.  The write-ahead protocol forces the log entry to be written to permanent storage before the actual operations takes place.  The 2-phase commit protocol defines operations between two types of nodes: The coordinator and one or more subordinates. The coordinator role is assigned to the node that initiates the transaction. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace  Here is an illustration of the 2 phase protocol: Performance and failure transparency  Performance transparency allows a DDBMS to perform as if it were a centralized DB, i.e. no performance degradation should be incurred due to data distribution.  Failure transparency ensures that the system will continue to operate in the case of a node or network failure.  The cost associated with a request are a function of the following:  Resolving data requests in a distributed data environment must take the following points into consideration: - Data distribution: In DDBMS query translation is more complicated, because DDBS must decide which fragment to access. A TP executing a query must choose what fragments to access. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace E E - Data replication: Data may also be replicated at several diff sites. The system must ensure all copies of data are consistent. Query optimization in distributed DB system is that it must provide replica transparency. Replica transparency refers to the DDBMS’s ability to hide multiple copies of data from the user. A write request also involves “synchronizing” all existing fragments to maintain data consistency. The DDBMS’s must ensure consistency among all fragments, by a DP captures all changes and pushes the them to each remote replica. - Network and node availability: To achieve performance transparent, the DDBMS should consider issues such as network latency, the delay imposed by the amount of time required for a data packet to make a round trip from point A to point B, or network partitioning, the delay imposed when nodes become suddenly unavailable due to a network failure. - Distributed DB design The design of a distributed DB introduces 3 new issues: For exam be able to explain each issue The issues explained: Data fragmentation:  Data fragmentation allows you to break a single object into 2 or more segments or fragments.  The object might be a user’s Db, a system DB, or a table.  Each fragment can be stored at any site over a computer network.  3 data fragmentation strategies exist: - Horizontal fragmentation: division of a relation into subsets of tuples (rows). Fragments represents the equivalent of a Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace SELECT statement, with the WHERE clause on a single attribute. - Vertical Fragmentation: refers to the division of a relation into attribute subsets. Each fragment has unique columns. Equivalent to PROJECT statement in SQL. - Mixed fragmentation: refers to a combination of horizontal and vertical strategies. Just know up to here to explain Data fragmentation for issues that distributed DB introduces.  To illustrate fragmentation strategies: Horizontal fragmentation: Ways to portioning horizontally - Round-robin portioning: Rows are assigned to a given fragment in a round-robin fashion to ensure even distribution of rows among all fragments. Not a good strategy if you require location awareness- the ability to determine which DP node will process a query based on the geospatial location of the requester - Range partitioning based on a partition key: A partioning key is one or more attributes in a table that determine the fragment in which a row will be stored. e.g. use CUS_STATE as a partitioning key if you want location awareness. Here is what it would look like if we used it as the portioning key: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Vertical Fragmentation: Say we use this fragment it might look like this: Note the attributes are diff between fragments: Say there are 2 departments and each department is only interested in some of the attributed of the customers then this fragmentation can be used. Mixed Fragmentation: Suppose we want the CUSTOMER’s data be fragmented horizontally to accommodate the various company locations; within the locations, the data must be fragmented vertically to accommodate the two departments. Mixed fragmentation requires a two-step procedure. 1 st horizontal fragmentation is done. Then vertical fragmentation is done on the horizontal subsets. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Data Replication:  Data Replication refers to the storage of data copies at multiple sites served by a computer network.  Fragment copies can be stored at several sites to serve specific information requests.  Fragment copies can enhance data availability and response time and data copies can help to reduce communication and total query costs.  Replicated data: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Fragment A1 stored at S1 and S2, A2 is stored @ site S2 and S3  Replicated data is subject to the mutual consistency rule, which requires that all copies of data fragments be identical to maintain data consistency among replicas. i.e. DDBMS updates replicas  2 styles of replication: - Push replication: After a data update, the original DP node, sends the changes to the replica nodes thus data is immediately updates. This technique focuses on data consistency, but decreases data availability because data being updates can’t be accessed. - Pull replication: After a data update, the originating DP node sends “messages” to the replica nodes to notify them of the update. The replica nodes decide when to apply the updates to their local fragment. here the focus is on maintaining data availability, however temporary data inconsistencies do exist. Just know up to here to explain Data fragmentation for issues that distributed DB introduces( do not learn explanation for Push and pull replication) for this question. Just mention it.  Replication has the following benefits: improved data availability, better load distribution, improved data failure and reduced costs.  The negative part about replication is it imposes additional DDBMS processing overhead and increased transaction times. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Here are some processes that the DDBMS must perform when replica overhead is imposed on a DDBMS: 3 replication scenarios exist: Several factors influence the decision to use data replication: o DB size o Usage frequency o Costs Data Allocation Data allocation describes the process of deciding where to locate data. There are 3 data allocation strategies:  Centralized data allocation: entire DB is stored at one site  Partitioned data allocation: the DB is divided into 2 or more disjoint parts and stored at 2 or more sites  Replicated data allocation: Copies of one or more DB fragments are stored at several sites. Just know up to here to explain Data fragmentation for issues that distributed DB introduces Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace E Data allocation algorithms consider a variety of factors, including: The CAP theorem CAP stands for consistency, availability, and partition tolerance. In a highly distributed data system these are desirable properties. It’s impossible to have all 3.  Consistency – In a distributed database, all nodes should see the same data at the same time, which means that the replicas should be immediately updated. However, this involves dealing with latency and network partitioning delays.  Availability – Simply speaking, a request is always fulfilled by the system. No received request is ever lost. If you are buying tickets online, you do not want the system to stop in the middle of the operation.  Partition tolerance – The system continues to operate even in the event of a node failure. The system will fail only if all nodes fail. Some companies tend to forfeit the consistency and isolation components to achieve higher availability. This trade-off between consistency and availability has generated a new type of distributed data systems in which data is basically available, soft state, eventually consistent (BASE). BASE refers to a data consistency model in which data changes are not immediate but propagate slowly through the system until all replicas are eventually consistent. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace E Chapter 13 – Business Intelligence and data warehouses  A Sound BI strategy adds value to an organization by providing the right data, in the right format, to the right people, at the right time.  BI uses Master data management to manage data.  Master data management (MDM) is a collection of concepts, techniques, and processes for the proper identification, definition, and management of data elements within an organization. Stuff that identify, define and manage data elements in an organization. It’s main goal is to provide a comprehensive and consistent definition of all data within an organization.  Governance is a method if government (management). MDM approach helps the framework for business governance.  Monitoring business health is important and to do this BI makes use of key performance indicators (KPI). KPIs are quantifiable numeric or scale-based measurements that asses the company’s effectiveness in reaching its strategic and operational goals. Example of KPIs are: Companies monitor social media data to identify trends and quickly react to current or future threats or opportunities. Data visualization is abstracting (changing the form of the data) data to provide information in a visual format that enhances the user’s ability to effectively comprehend the meaning of the data. Goal of it is to allow the user to see the big picture in the most efficient way possible. It changes the data into a from that provides at-a-glance insight into overall trends and patterns. Techniques includes: pie charts, line graphs, bar graphs, scatter plots, Gantt charts, heat maps, etc. Excel can be used to make data visualization. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Heart of BI system is its advanced info. generation and decision support capabilities. Its advanced decision support function comes to life via user interface and its reporting capabilities. 3 reporting (comment) styles: BI benefits: Improved decision making is the main goal of BI, but BI provide the following benefits as well: o Integrated architecture: Integrate into the organization o Common user interface for data reporting and analysis: Common interface thus less training when using a common interface o Common data repository fosters single version of company data: One data store has its advantage o Improved organizational performance: Obvious BI evolution  A precursor (predecessor) of the modern BI environment was the 1st generation decision support system. A decision support system (DSS) is an arrangement of computerized tools used to assist managerial decision making. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Evolution explained:  Centralized reports running on mainframes and central server environments.  As spreadsheet emerged it emerged as dominant format for decision support systems (DSS). Data was downloaded from centralized data stores and manipulated with desktop spreadsheet.  BI flourished with introduction of data warehouses and online processing systems (OLAPs)  Changes in internet caused development of web-based dashboards  Social media development made way for development of Big data analytics, Hadoop and NoSQL. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace BI technology trends: Business intelligence technological trends: think of BI trends that has started to grow with increasing hardware technology o Data storage improvements: New SSD and SATA drives off better performance o BI appliances (uses): Vendors now offer plug and play appliances optimized for data warehouse and BI applications o BI as a service: Vendors offers BI as a service o Big data analytics: Organization is turning to social media (big data) as source for information o Personal analytics: OLAP brings data analytics to every desktop end user in an organization Decision support data BI’s effectiveness depends on the quality of data gathered at the operational level. Can’t only rely on operational data to decision support tasks, we also need decision support data Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Operational Data vs. Decision support Data  They serve diff. purposes, thus their formats and structures differ.  Operation Data:  Stored in relational DB in which structures tend to be normalized.  Optimized to support transactions that represent daily operations  It is not efficient for queries processing once stored in the DBs  Useful for capturing daily business transaction, e.g. like a sale.  Decision support data (DSD):  Gives tactical and strategic business meaning to operational data. Operational Data differs from DSD in the following 3 main ways: o Time span: Operation data cover short time span, but DSD covers long time frame. E.g. managers interested in sales in a month which is DSD, but single sale being generated is operational data. o Granularity (level of aggregation [in this sense it means transformed or summarised]): DSD must be presented at different levels of aggregation, from highly summarized to nearly atomic for different decisions that managers need to make. Drill down means to decompose data into more atomic components. Roll up the data means you are aggregating (summarizing/ combining) the data to a higher level. Operational data is at low level of aggregation. o Dimensionality: Operational data focuses on representing individual transactions rather than the effects of the transactions over time. DSD can be examined from multiple dimensions. Shown by Fig 3.14 Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace From the designer’s point of view the difference between Operational data and DSD are: - Operational data represents transactions as they happen in real time, DSD is a snapshot of the operational data at a given point in time, DSD data is historic (past tense data) - They differ in transaction type and transaction volume. Operational data is characterized by update transactions, DSD is characterized by read- only transactions. DSD also requires periodic updates. Transaction volume in operational data tends to be very high (data comes in a large rate) with low to medium for DSD. - Operational data stored in tables and stored data represents information about a given transaction only. DSD is stored in a few tables derived from the operational data. DSD represents transaction summaries, thus DSD stores data that is integrated, aggregated and summarized. - Degree to which DSD is summarized is very high. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace - Data models for the two are different. Operational data generally requires normalized structures to avoid data anomalies. DSS tend to be un-normalized. - Queries against operational data typically are narrow in scope and low in complexity. Think managers don’t use it to answer difficult questions. However, with DSD the queries are broad in scope and high in complexity. - DSD is characterized by very large amount of data, because many data is used more than once to be viewed in many diff. forms and it is stored in non-normalized structures that likely display many redundancies. Table summarizing diff. from the designers point of view. Decision Support DB Requirements A decision support Db is a specialized DBMS tailored to provide fast answers to complex queries. There are 3 main requirements for a Decision Support DB (DSDB):  DB schema: The DSDB schema must support complex (non-normalized) data representations. DSDB must contain data that is aggregated and summarized. Basically it must have DSD. The DSDB schema msut also be optimized for query(read-only) retrievals. To optimize query speed, the DBMS must support features such as bitmap indexes and data partitioning. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace  Data extraction and filtering: DSDB is created by extracting data from the operational DB and importing additional data from external sources. Thus the DSDB must support advanced extraction and data filtering tools. Data-filtering capabilities must include the ability to check for inconsistent data or validation rules.  DB size: DSDB tends to be very large. Thus the DBMS must be capable of supporting very large DBs (VLDBs). To support VLDBs the DBMS must support advanced storage technologies, such as symmetric multiprocessor (SMP) or a massively parallel processor (MPP). Demand for information requirements and for sophisticated data analysis have sparked the need for a new type of data repository – the data warehouse. The data warehouse(DW)  Repository (storehouse) that contains data in formats that facilitate data extraction, data analysis and decision making.  Definition: An integrated, subject-oriented, time-variant, non-volatile collection of data that provides support for decision making. Components: - Integrated: The data warehouse is a centralized, consolidated (combined) DB that integrates data derived from the entire organization and from multiple sources (external) with diverse formats. Data integration implies that all business entities, data elements, data characteristics, and business metrics are describes in the same way throughout the enterprise. e.g. same “sales performance” throughout the organization. All data is of the same format. - Subject-oriented: Data in DW is arranged and optimized to provide answers to questions from diverse functional areas within an organization. DW data organized into topics and then again into subjects of interest. E.g. Sales topic, then again into products. - Time-variant: DW data represents flow of data through time. E.g. its time sensitive so managers can make decisions as to what products sold best in Quarter 1. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace E - Non-volatile: Once data enters DW, it is never removed, because the data in the warehouse represents the company’s history. DW is always growing since new data is added and none is deleted. Summary of DW: DW is read-only Db optimized for data analysis and query processing. Data is extracted from various sources and then transformed and integrated (passed through a data filter). This process is known as ETL(extraction, transformation, loading) The ETL process: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace E E E Data warehouse to big and companies don’t always need to make decision on all the subjects, thus more companies are making use of data marts which are smaller data stores. Data Marts  A data mart is a small, single-subject data warehouse subset that provides decision support to a small group of people.  A data mart could be created from data extracted from a larger data warehouse for the specific purpose of supporting faster data access to a target group or function.  Data marts and DWs can coexist within an organization.  Only difference between data mart and DW is the size and scope of the problem being solved.  Reasons why some organization prefer to create data mart instead of DW:  Lower cost and shorter implementation time  A company’s employees are more likely to embrace minor changes that lead to improved decision support with a data mart rather than major changes that a data warehouse would impose. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace E  People at different organisational levels will likely require data with different summarization, aggregation and presentation formats.  Data marts can serve as a test vehicle for organisations exploring the potential benefits of a data warehouse. Think: LCSI, CEEMC, SAP, TV Star schemas  The star schema is a data-modeling technique used to map multidimensional decision support data (DSD) into a relational DB.  Yields an easily implemented model for multidimensional data analysis while preserving the relational structures on which the operational DB is built. Have 4 components: - Facts: are numeric measurements (values) that represent a specific business aspect or activity. e.g. sales figures. Facts are stored in a Fact Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace A table which are the centre of the star schema. Fact table contains facts that are linked through their dimensions. Computed or derived facts are called metrics. - Dimensions: are qualifying characteristics that provide additional perspectives to a given fact. Dimensions are of interest because decisions support data is almost always viewed in relation to other data. e.g. ‘sales’ (a fact) can be compared by products from region and from one time period to another time period. So sales have product, location and time dimensions. Dimensions are stored in dimension tables. - Attributes: each dimension table contains attributes. It is used to search, filter, or classify facts. Dimensions provide descriptive characteristics about the facts through their attributes. Thus, the data warehouse designer must define common business attributed that will be used by the data analyst to narrow a search, group information, or describe dimensions. The star schema, through its facts and dimensions, can provide the data in a format suited for data analysis. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace We can view the dimensions in a cube as in the next picture (don’t have to know the picture) Now the ability to focus on slices of the cube to perform a more detailed analysis is knowns as slice and dice. e.g.: - Attribute hierarchies: Attributed within dimensions can be ordered in a well-defined attribute hierarchy. It provides a top-down data organization that is used for 2 main purposes: aggregation and drill- down/roll-up data analysis. E.g. the following picture shows how the location dimension attribute can be organized in a hierarchy be region, state, city and store: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Attr. hierarchy provide the capability to perform drill-down and roll-up searches in a DW. E.g. Data analyst wants to look at sales of between Jan 2015 – Feb 2016. Suppose Oct 2015 shows a very low sales count and then the analyst wants to drill down inside this month to see what happened. The drill-down/roll up is possible because the attr. hierarchy allows data warehouse and BI systems to have a defined path that identifies how data is to be decomposed and aggregated for drill-down and roll-up operations. Attributes from different dimensions can be grouped to from a hierarchy. e.g. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace This scenario illustrates the data analyst with 3 different information paths. Data analyst can look up stuff like What ALL PRODUCTS has the highest sales in Q1 by Regions. here the analyst can drill-down into the location (e.g. the region) to see state, city and then store. Star Schema Representation  Facts and dimensions are normally represented by physical tables in the DWDB.  The fact table is related to each dimension table in a M:1 relationship. many side is on the Fact.  Using Sales example. Each product appears many times in sales.  Fact and dimension tables are related by foreign keys and are subject to the familiar PK and FK constraints. PK of the 1 side (dimension table) is stored as part of the PK on the many side (fact table).  because the fact table is related to many dimension tables, the PK of the fact table is a composite PK. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace E  e.g. in the SALES fact With this star schema a person can study SALES figures by year(time), location, customer and product. The PRODUCT, CUSTOMER, TIME and LCOATION are called dimensions So this table shows relationship among SALES fact table and the product, location and time dimension. Then we add Customer dimension to it. THINK about it: Sales can be aggregated (grouped/summarized) into store (location), customer, time, and product. Analyst can ask question such as what type of customers, bought some type of product at this location at the specified time. To add CUSTOMER table we simply have to add CUST_ID to SALES table.  notice the composite PK for SALES fact table is composed of TIME_ID, LOCK_ID, CUST_ID and PROD_ID.  In default the fact tables PK is always formed by combining the FK pointing to the dimension tables.  Fact tables are always the largest tables in the star schema and dimension tables are always the smallest since dimension tables contain non-repetitive information. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace E E  Each dimension record appears only once in dimension table, bit can appear many times in fact table. E.g. “widget” only appear once in product dimension, but thousands records in the SALES fact table. Performance-Improving techniques for the Star Schema Creating DB that provides fast and accurate answers to data analysis queries is the prime objective of data warehouse design. The 4 techniques are often used to optimize data warehouse design: Normalizing Dimensional Tables: Dimensional tables are normalized to achieve semantic (relationship) simplicity and facilitate end-user navigation through the dimensions. E.g. If the location dimension table contains transitive dependencies among regions you can revise those relationships to the 3NF. Transitive dependencies is: 1. A → B 2. It is not the case that B → A 3. B → C A->C is transitive dependency. E.g. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace This is known as a snowflake schema, which is a type of star schema in which the dimensions tables can have their own dimension tables. Snowflake schema is achieved by normalizing dimension tables. In this figure only the Location dimension is directly related to the SALES fact table. Maintaining multiple Fact tables that represent Different Aggregation Levels You can speed up query operations by creating and maintaining multiple fact tables related to each level of aggregation (region, state, and city). Illustrated by this figure in which it adds aggregate fact tables for region, state, and city to the initial sales example. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace E E Now, the aggregated table can be used to access some data instead if processing the SALES table. Thus increasing access time. Denormalizing Fact Tables: Doing this improves data access performance and saves data storage space. Denormalizing improves performance by using a single record to store data that normally takes many records. E.g. creating a YEAR_TOTALS table might have 1 record that exists out of many records. Partitioning and Replicating tables: Is a star schema performance improvement technique that splits and makes copies of DB tables. Partitioning splits table into subsets of rows or columns and places the subsets close to the client computer to improve data access time. replication makes a copy of a table or partition and places it in a different location, also to improve access time. Periodicity provides information about the time span of the data stored in the table. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace A BI uses Data warehouse and data marts for business decision making. Online analytical processing (OLAP) OLAP is widely used BI style. OLAP is a BI style whose systems share 3 main characteristics: Multidimensional Data Analysis Techniques:  In OLAP data is processed and viewed as part of a multidimensional structure.  It was previously discussed that multidimensional view of data (facts) are more helpful when it comes to making decisions. With operational (tabular view) view it is difficult to make decisions.  Multidimensional view allows end users to consolidate or aggregate data at different levels Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace So aggregation means different types of dimensions compared with fact. e.g. Sales by customer or sales by time. Multidimensional data analysis techniques are augmented (improved) by the following functions: - advanced data presentation functions: include 3D graphics, etc. - Advanced data aggregation, consolidation, and classification functions: These allows data analyst to create multiple data aggregation levels, slice and dice data, and drill down and drill down and roll up data across diff. dimensions and aggregation levels. - Advanced computational functions: These includes business-oriented variables such as market share, sales margins, statistical and forecasting functions, etc. - Advanced data-modeling functions: provide support for what-if scenarios ,etc. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Advanced DB Support: To deliver efficient support, OLAP tools must have the following advanced data access features: Easy-to-Use End-User Interfaces: The analytical interface permits the user to navigate the data in a way that simplifies and accelerates decision making or data analysis. Most OLAP vendors have closely integrated their systems with spreadsheets such as Excel. OLAP Architecture: OLAP has 3 main architectural components: Think it must Analyse data through the use of GUI and data must also be processed (managed). Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace OLAP uses both operational and data warehouse data as seen in picture. In this picture the OLAP components work on one computer, but a better implementation is if it works on multiple computers to share the processing. Better implementation is if OLAP data-processing logic (OLAP server) runs on shared server computer and OLAP GUI runs on client workstations. The OLAP analytical processing logic could be located on a client workstation, the OLAP server, or be split between 2 sides. OLAP server component acts as an intermediary between the OLAP GUI and the data warehouse. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace notice data marts are stored at the end-user workstations, thus increasing access. OLAP stores multidimensional data with 2 approaches: - In Relational DBs Specialized multidimensional DBs Chapter 14 - big data analytics and nosql Big Data Big data generally refers to a set of data that displays the characteristics of volume, velocity and variety to an extent that makes the data unsuitable for managements by a relational DBMS. The characters can be defined as follows: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace This is called the 3 V’s. Information regarding big data:  Web data, combination of text, graphics, video, and audio sources combined into a complex structure, is creating the new challenges for data management that involve all 3 characteristics (volume, velocity and variety)  Changes in technology have increased the opportunity for business to generate and track data so that Big Data has been redefined as involving any of the 3 V’s. They don’t have to include all 3 V’s, just 1 of them. Volume Summary of the units used for storing Big Data. o As quantity of data needed to be stored increases, the need for larger storage devices increases. When this occurs (needing larger storage devices) systems can either scale up or scale out. Scaling up is keeping the same number of systems, but migrating each system to a larger system and faster system. There are limits to how large and fast a single system can be and the costs increases more the better the system gets. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Scaling out means that when the workload exceeds the capacity of a server, the workload is spread out across a number of servers. Also referred to as clustering- creating a cluster of low-cost servers to share a workload. This is cheaper method than scaling up since cheaper to buy a lot of slower systems than 1 fast system. o A Distributed database can also be used to handle the volume of data. Remember the DBMS controlled all the distributed data and the DBMS can also be distributed. There are limits associated with the ability of distributed the DBMS due to increased costs of com. and coordination as the number of nodes grows. This limits the degree the which a relational DB to be scaled out as data volume grows, and it makes the RDBMs ill-suited for clusters. Velocity  Refers to the rate at which new data enters the system as well as the rate at which data must be processed.  The issued if velocity mirror those of volume.  Advances in technology such as RFID, GPS and NFC add new layers of data-gathering opportunities that often generate large amounts of data that must be stored in real-time.  RFID: Can be used to track inventory and warehouses management. Uses a tag which do not require line of sight.  The velocity of processing can be broken down into 2 categories: -Stream processing -Feedback loop processing Stream processing focuses on input processing, and it requires analysis of the data stream as it enters the system. Sometimes data is so large that all data cannot be stored. The data must be processed and filtered as it enters the system to determine which data to keep and which data to discard. Feedback loop processing refers to the analysis of the data to produce actionable results. Here the focus is on outputs. The feedback loop is: capturing data, processing it into useable information, and then acting on that information is a feedback loop. Example for providing recommendations for book purchases: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Here immediate results requires analysing large amount of data within a few seconds so that results of the analysis can become a part of the product delivered to the user in real time Variety Variety refers to the vast array of formats and structures in which the data may be captured.  Data can be -structured -unstructured -semistructured  Structured data is data that has been organized to fit a predefined data model.  Unstructured data is data that is not organized to fit into a predefined data model.  Semi-structured data combines elements of both – some parts of the data fit data model, other parts don’t.  Relational DB rely on structured data – a data model is created by the DB designer based on the business rules.  Most of the data in the world is semi-structured or unstructured (includes maps, satellite images, emails, text, etc. )  Big data requires that the data be captured in whatever format it naturally exists, without any attempt to impose a data model or Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace structure to the data. <- One of the key differences between processing data in a relational Db and big Data processing.  Relational DBs impose a structure on the data when the data is captured and stored whereas Big Data processing imposes a structure on the data as needed for applications as a part of retrieval and processing.  Advantage to providing structure retrieval and processing is the flexibility of being able to structure the data in different ways for different applications. Other characteristics Other characteristics than the 3 V is called the additional Vs. The other characteristics are: - Variability: refers to the changes in the meaning of the data based on context. Variety and variability although very similar means different things in Big Data. Variety is about differences in structure whilst variability is about difference in meaning of the data. - Sentiment analysis: is a method of text analysis that attempts to determine if a statement conveys positive, negative or neutral attitude about the topic. - Veracity: refers to the trustworthiness of the data. Can decision makers reasonably rely on the accuracy of the data and info. generated from it. - Value: Also called viability, refers to the degree to which the data can be analysed to provide meaningful information that can add value to the organization. only data that can form the basis for analysis that has the potential to impact organizational behaviour should be included in a company’s Big Data efforts - Visualization: is the ability to graphically present the data in such a way as to make it understandable. Visualization is a way of presenting the facts so that decision makers can comprehend the meaning of the information to gain insights. The additional Vs are not only characteristics of only Big Data, some of them are issues with all types of DBs. The decision to use a relational DB at all is a real question and has led to polyglot persistence – the coexistence of variety of data storage and management technologies within an organization’s infrastructure. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Hadoop  Big Data requires a different approach to distributed data storage that is designed for large-scale clusters.  Hadoop has become standard for most Big Data storage and processing.  Hadoop is not a DB, it is a Java-based framework for distributing and processing very large data sets across clusters of computers.  Hadoop was designed specifically to distribute and process enormous amounts of data across vast clusters of servers. Hadoop framework most important components are:  Hadoop Distributed File System (HDFS): HDFS is a low-level distributed file processing system, which means it can be directly used for data storage  MapReduce: Is a programming model that supports processing large data sets in a highly parallel, distributed manner. HDFS: This approach to distributing data is based on the following key assumptions:  High volume: Files in the HDFS will be extremely large.  Write-once, read-many: This simplifies concurrency issues and improves overall data throughput. Using this model, a file is created, written to the file system, and then closed. Once closed changes cannot be made to its content. This improves overall system performance and works well for the types of tasks performed by many Big Data applications.  Streaming access: Big Data applications typically process entire files (unlike transaction processing systems). Hadoop is optimized for batch processing of entire files as a continuous stream of data.  Fault tolerance: Hadoop is distributed across thousands of low-cost commodity computers. Some of these systems may experience hardware errors, thus Hadoop must include fault tolerance. HDFS is Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace designed to replicate data across many different devices. Hadoop uses replication factor of 3. Hadoop uses several types of nodes. A node is a computer that performs one or more types of tasks within the system. Within HDFS there are 3 types of nodes: The client node, the name node and data nodes. Data nodes store the actual file data. Each block (of data) are replicated on 1 or more data node. This figure shows the default replication factor of 3, so each block appears on 3 data nodes. The name node contains the metadata for the file system. Normally only 1 name node per HDFS cluster. Metadata is designed to be small, simple and easily recoverable. Keeping metadata small slows the name node to hold all of the metadata in memory to reduce disk accesses and improve system performance. The metadata is composed primarily of the name of each file, the block numbers that comprise each file, and the desired replication factor for each file. The client node makes requests to the file system, either to read files or to write files. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace  When a client node needs to create a new file, it communicates with the name node and the name node will: The client node contacts the 1st data node specified by the name node and begins writing the file on that data node. At the same time, the client node sends the data node the list of other data nodes that will be replicating the block. As data is received from the client node, the data node contacts the next data node in the list and begins sending the data to this node for replication. 2 nd data node then contacts next data node in the list and the process continues. Once the 1st block is written, the client node can get another block number and list of data nodes from the ‘name node’ for the next block. When the entire file has been written, the client node informs the name node that the file is closed. None of the data is transmitted to the name node.  If a client need to read a file similarly it contacts the ‘name node’ to request the list of blocks associated with that file and the data nodes that hold them. Given that each block may appear in many data nodes, for each block, the client attempts to retrieve the block from the data node that is closest to it in the network.  Periodically each data node communicates with the name node and send block reports and heartbeats. Block reports is sent every 6 hours and informs the name node of which blocks are on the data node. A Heartbeat is used to let the ‘name node’ know that the ‘data node’ is still available. If a data node fail then name node will not receive a heartbeat from the data node that failed. Thus the name node knows not to include that data node in lists to client nodes for reading or writing files. When fewer than desired replicated numbers exist due to a heartbeat then the block can be replicated on another node. MapReduce Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace  MapReduce is the computing framework used to process large data sets across clusters.  Follows the principle of divide and conquer – MapReduce takes a task, breaks it down into a collection of smaller subtasks, performs the subtasks all at the same time, and then combines the result of each subtask to produce a final result for the original task.  It is a combination of a map function and reduce function.  A map function takes a collection of data and sorts and filters the data into a set of key-value pairs. Map function is performed by a program called a mapper.  A reduce function takes a collection of key-value pairs, all with the same key value, and summarized them into a single result. This function is performed by a program called a reducer.  Example of MapReduce that determines the total number of units of each product that has been sold: Note: Original data is in key-value pairs (invoice num. is key and remainder of invoice data as a value. The map function produce a new list of key-value pairs in which product code(p_code) is the key and the line units(line_num) are the values. The reduce function then takes list of key-value pairs produces by the map functions and combines them by summing the values associated with each key to produce a summary result. Product example is ‘13-Q2/P2’ and the number of them is 1 after the ‘:’ Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace  Because the data is very big a central node does not process all the data. Instead copies of the program are “pushed” to the nodes containing the data to be processed. Each copy of the program produces results that are the aggregated(combined) across nodes and sent back to the client.  MapReduce compliments HDFS  MapReduce consists of (or uses) a -Job tracker (the actual name of the program is JobTracker): Acts as the central control for MapReduce processing and it normally exists on the same server that is acting as the name node. It will take care of locating data, determining which nodes to use, dividing the job into tasks for the nodes, and managing failures of the nodes. -Task Trackers (the programs are named TaskTrackers): Resides on the data nodes  When a user submits a MapReduce job for processing, the general process follows: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace The Hadoop system uses batch processing. Batch processing is when a program runs from beginning to end, either completing the tasks or halting with an error, without any interaction with the user. Hadoop Ecosystem  Hadoop requires considerable effort to create, manage and use.  Thus applications have been created to make it easier. In this figure we show some of the applications: Here are some of the application/more popular components in a Hadoop ecosystem:  MapReduce Simplification Applications: Here applications simplify the process of creating MapReduce jobs. Two application exist are Hive and Pig. Hive is a data warehousing system that sits on top of HDFS. It supports its own SQL-like language, called HiveQL that mimics SQL commands to run ad hoc queries. Pig is a tool for compiling a high-level scripting language, named Pig Latin, into a MapReduce jobs for executing in Hadoop. Primary difference between Hive and Pig is Pig is scripting language so it is procedural whilst HiveQL is declarative.  Data Ingestion Applications: here applications deal with problem of getting data from companies’ existing systems into the Hadoop cluster. They “ingest” data into Hadoop. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Flume is a component for ingesting data into Hadoop. Designed primarily for harvesting large sets of data from server log files (like clickstream data from web servers logs). Sqoop is a tool for converting data back and forth between a relational Db and the HDFS. Difference between them are Flume work with log files whilst Sqoop work with DBs and Flume work in 1 way whilst Sqoop work in both ways.  Direct Query Applications: These attempt to provide faster query access than is possible through MapReduce. These applications interacts directly with HDFS, instead of going through the MapReduce processing layer. HBase is a column-oriented NoSQL DB designed to sit on top of the HDFS. It’s characteristics is that it is highly distributed and designed to scale out easily. No SQL, use Java-like languages. Used for faster processing of smaller subsets of data. Impala is a query engine that supports SQl queries that pull data directly from HDFS. Here analysts can write SQl queries directly against the data while it is still in the HDFS. NoSQL  NoSQL is the name given to a broad array of nonrelational DB technologies that have developed to address the challenges by Big Data.  Actually many NoSQL products actually include query languages that mimic SQL. Here are the 4 NoSQL categories along with examples of Dbs of each category: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace The 4 NoSQL categories: - Key-Value DBs: Key-value (KV) DBs are conceptually the simplest of the NoSQL data models. It is a DB that stores data as a collection of key-value pairs. Key acts as identifier for the value. The DB does not try to understand the content of the value, it is the job of the applications that use the data to understand the meaning. There are no foreign keys, thus KV DBs are extremely fast and scalable for basic processing. Key-value pairs are organized into buckets which is the KV DB equivalent of a table. Bucket is a logical grouping of keys. Key values must be unique within a bucket. All data operations are based on the bucket plus the key. Only operations used are get, store and delete. get is used to retrieve the value component of the pair store is used to place a value in a key delete is used to remove a key-value pair. Example: Here we have customer bucket with 3 key-value pairs. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace It’s not possible to query the KV model based on data, e.g. can’t query based on LName, since KV does not know understand the data. Application could use get command to return data of key 10011 and then the application itself can get the LName from the data. - Document DBs: Conceptually same as KV DBs and is subtype of KV DBs. It is a NoSQL DB that stores data in tagged documents in key-value pairs. Unlike KV where value can contain any type of data, here the value component is of type document. Also, Document DB try to understand the values, whereas KV DBs don’t. Tags are named portions of a document. This DB is schema-less, which means they do not impose a predefined structure on the data that is stored. (So not all documents have to have the same tags) Here key-value pairs are grouped into logical groups called collections. Here querying is allowed based on the collection, key and the content of tags. Same Example: The DBMS is aware of the tags within the documents, it is possible to query based on the Document. e.g. allowed to query based on LName. Support for aggregate functions. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Document DBs do NOT store relationship and thus no support for join operations. - Column-oriented DBs: Refer to traditional, relational DB technologies that use column-centric storage instead of row-centric storage. In relational DBs data is presented as tables, however data is actually stored in data blocks containing rows of data. Many rows can be stored in the same data block. E.g. In the diagram below we have a relational table with 10 rows of data that is physically stored across many data blocks. Row-centric storage: As seen in diagram row-centric storage is a physical storage technique in which data is stored in blocks which holds data from all columns of a given set of rows. Entire rows are put in data blocks. Row-centric minimizes the number of disk reads necessary to retrieve a row of data. Negative thing about row-centric is it is not efficient at dealing with Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace queries that retrieve a small set of columns across a large set of rows. E.g. to retrieve cus_code column data 5 data blocks would have to be read. Column-centric storage: Is a physical data storage technique in which data is stored in blocks, which hold data from a single column across many rows. Here queries that retrieve a small set of columns across a large set of rows is more efficient than row-centric storage. E.g. retrieval of cus_code and cus_city means only 2 block have to be read. Column family DB: Other use of the term column-oriented DB is also called column family DB which is a NoSQL DB that organizes data in key-value pairs with keys mapped to a set of columns in the value component. The key is the name of the column, and the value component is the data that is stored in that column. E.g. “cus_lname: Ramas” is a column, cus_lname is the name of column and ‘Ramas’ is the data value in the column. A super column is a group of columns that are logically related. E.g. the columns cus_fname, cus_lname, and cus_initial which would logically group together to form a customer’s name. Super columns can be thought of as composite attributes. A column family is a collection of columns or super columns related to a collection of rows. A column family is theoretically similar to a table in the relational model. So Row keys are created from the column family Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace - Graph DBs: A graph DB is a NoSQL DB based on graph theory to store data about relationship-rich environments. Graph theory models relationships (or edges) between objects called nodes. Modeling and storing data about relationships is the focus of graph DBs. This DB shines when multiple relationships (layers deep) are involves, e.g. Facebook: friends, friends of friends, friends of friends of friends, etc. Primary components of Graph DBs are: Nodes, edges and properties Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Node: the representation of a single entity instance. E.g. in Fig each node is an Agent. Edges: Is a relationship between nodes. Thye can be one direction or bidirectional. Properties: are like the attributes, they are the data that we need to store about the node or an edge. All nodes or edges do not have to have the same properties. Example: Date on which Alfred Ramas liked agent Alex Alby is recorded in the graph DB. A query in the Graph DB is called traversal. Graph DBs do not force data to fit predefined structures, do not support SQL, and are optimized to provide velocity of processing. Also, it doesn’t scale out very well to clusters. NewSQL DBs Relational DBs concerned with supporting business transactions and NoSQL DBs are concerned with massive amounts of data. NewSQL DBs try to bridge the gap between RDBMS and NoSQL. NewSQL that attempts to provide ACID-compliant transactions over a highly distributed infrastructure. Like RDBMS, NewSQL DBs support: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Disadvantage of NewSQL: It has a heavy usage of in-memory storage. Also, can’t scale out very well. Data analytics  Data analytics is a subset of BI functionality that encompasses a wide range of mathematical, statistical, and modelling techniques with the purpose of extracting knowledge from data.  It is used at all levels within the BI framework.  Data analytics represents what business managers really want from BI: the ability to extract actionable business insight from current events and foresee future problems or opportunities.  It is knowledge acquisition that goes from discovery to explanation to prediction.  Outcome of data analytics become part of the information framework on which decisions are built.  Data analytics tools can be grouped into 2 separate areas: - Explanatory analytics: focuses in discovering and explaining data characteristics and relationships based on existing data. It uses statistical tools to formulate hypotheses, test them, and answer the how and why of such relationships. E.g. how do past sales relate to previous customer promotions? Think of explanatory analytics as explaining the past and the present - Predictive analytics: focuses on predicting future data outcomes with high degree of accuracy. E.g.: what would next month’s (future) sales be based on a given customer promotion? Think of predicting the future. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace A Data Mining:  Data mining refers to -analysing massive amounts of data to uncover hidden trends, patterns, and relationships; -to form computer models to simulate and explain the findings -and then to use such models to support business decision making.  So it focuses on the discovery and explanation stages of knowledge acquisition.  Look at the figure to see how knowledge is extracted from data: Data forms pyramid base. 2nd level contains information that represents the purified and processed data. Information forms the basis for decision making and business understanding. Knowledge is found on top and represents highly distilled information that provides concise, actionable business insight.  Data mining consists of the following general phases: In this phase the main data sets to be used by the data mining operation are identified and cleansed of any data impurities. Because the data in Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace the data warehouse is already integrated and filtered, the data warehouse is usually the target. Data do not have to be integrated In this phase the data is studied to identify common data characteristics or patterns. The data mining tool try to find the following: In this phase the data-mining tool selects the appropriate modelling or knowledge acquisition algorithms. This phases uses the result of the previous phase. In this phase the data-mining findings are used to predict future behaviour and forecast business outcomes. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace  Examples of data mining findings can be:  Data mining can be run in 2 modes: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Predictive Analytics  Refers to the use of advanced mathematical, statistical, and modelling tools to predict future business outcomes with high degrees of accuracy.  Both data mining and predictive analytics have predictive capabilities. What is the difference then? Data mining focuses on the how and what of past data, while predictive analytics focuses on creating actionable models to predict future behaviours and events. You can think of predictive analytics as the next logical step after data mining; once you understand your data, you can use the data to predict future behaviours.  Predictive analytics employs mathematical and statistical algorithms, neural networks, AI, and other advanced modelling tools to create actionable predictive models based on available data.  Data analytics is used to extract knowledge from all sources of data including NoSQL DBs, Hadoop data stores and data warehouses to provide decision support. Summary Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Chapter 15 - DB connectivity and Web technology You will learn how to connect applications to DBs. Also we examine web DB technologies. Also, we learn about cloud services. DB connectivity  DB connectivity refers to the mechanism through which application programs connect and communicate with data repositories.  DB store data in structures so it can be retrieved for processing.  DBMS functions as intermediary between data (stored in DB) and the end-user’s applications.  Take note of the fundamental you have already learned to continue with this section:  We use client/server concepts to better understand DB connectivity in which we break an application into interconnected layers. With DB connectivity you could break down its basic functionality into 3 layers: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace A E  DB connectivity software is also known as the DB middleware, because it provides an interface between the application program and the DB or data repository.  The data repository (also called data source) represents the data management application, such as Oracle, SQL server, etc.  The DB connectivity middleware could support multiple data sources at the same time. e.g. data source can be a spreadsheet, noSQL DB, relational DB The following interfaces are covered:  Native SQL Connectivity Refers to the connection interface that is provided by the DB vendor and is unique to that vendor. E.g. is Oracle RDBMS, to connect to this you must install and configure Oracle’s SQL*Net interface on the client computer.  Microsoft’s Open DB connectivity(ODBC), Data Access Objects(DAO), and Remote Data Objects(RDO) - ODBC is Microsoft’s implementation of a superset of SQL Access Group Call Level Interface(CLI) standard for DB access. - ODBC is Microsoft’s Middleware that provides a DB access API to Windows applications. - ODBC allows any Windows application to access relational data sources using SQL via a standard application programming interface(API). API: set of routines, protocols, and tools for building software applications. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace - After a while DBC lacked in providing functionality beyond the ability to execute SQL to manipulate relational-style data. Thus, Microsoft developed 2 other data access interfaces: - DAO is an object-oriented API used to access desktop DBs, such as MS Access. DAO can also be used to access other relational-style data sources. Gives optimized interface for Jet data engine which MS Access is based on. - RDO is a higher-level, object-oriented application interface used to access remote DB servers. It uses a lower-level DAO and ODBC for direct access to DBs. Used for server-based DBs such as MS SQL Server. - Figure of how windows apps use ODBC, DAO and RDO to access local and remote data sources: Think: manager manages both so it’s in between API and Driver & Driver drives DMBS Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace E The RDO and DAO are like extensions of ODBC, thus they provide more functionality than ODBC. They make use of ODBC underlying services. ODBC, DAO and RDO are implemented as shared code that is dynamically linked to the Windows operating environment through dynamic-link libraries(DLLs) which are stored as files with a .dll extension. The basic ODBC architecture have 3 main component - Defining a data source is the 1st step in using ODBC. To do this you must create a data source name(DSN) for it. To create a DSN name you need to provide the following:  an ODBC driver: driver used to connect to the data source which normally is provided by the DB vendor.  name: Unique name by which data source will be known to ODBC. ODBC offers 2 types of data sources: user(available to user only) and system(available to all users)  ODBC driver parameters: ODBC drivers require parameters to establish a connection to DB, e.g. like username and password - The ODBC API standard defines 3 levels of compliance: Core, Level-1 and Level-2, which provide increasing levels of functionality. A vendor can chose which levels to provide.  Microsoft’s Object Linking and Embedding for DB(OLE-DB) - Is DB middleware that adds object-oriented functionality for access to relational and non-relational data. - provide support for nonrelational data unlike ODBC, DAO and RDO. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace E - adds object-oriented functionality for access to relational and nonrelational data. - It is based on COM(Component object Model) objects that provide low-level DB connectivity for applications. - It’s functionality is divided into 2 objects/main types of objects in the OLE-DB model:  Consumer: objects that request and use data. Consumers request data by invoking methods exposed by the data provider objects and passing the required parameters  Providers: objects that manage the connection with a data source and provide data to the consumers. 2 categories:  Data providers: provide data to other processes. DB vendors create data provider objects that expose the functionality of the underlying data source.  Service providers: provide additional functionality to consumers. It is located between the data provider and the consumer. Service provider requests data from the data provider, transforms the data, and then provides the transformed data to the data consumer. i.e. the service provider acts like a data consumer of the data provider and as a data provider for the data consumer. - Vendors provide OLE-DB objects to augment(enhance) their ODBC support, thus introducing object-oriented functionality to ODBC. Each object implements a specific task, thus by using these objects a vendor can choose what functionality to implement. - Sample OLE-DB classes and interfaces Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace - OLE-DB does not provide support for scripting languages. Script is written in a programming language that is not compiled but is interpreted and executed at run time. To provide support for scripts an object framework was designed called ActiveX Data Object(ADO) which provides a high-level, application oriented interface to interact with OLE- DB, DAO and RDO. - ADO/OLE-DB architecture: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace ADO provides a unified interface to access data from any programming language that uses the underlying OLE-DB objects.  Microsoft’s ActiveX Data Objects(ADO.NET) - This is based on ADO - ADO.NET is the data access component of Microsoft’s .NET application development framework. - Microsoft .NET framework is a component-based platform for developing distributed, heterogeneous, interoperable applications aimed at manipulating any type of data using any combination of network, OS and programming language. - The .NET framework extends and enhances the functionality provided by the ADO/OLE-DB duo. - ADO.NET introduced 2 new features that is critical for the development of distributed applications: DataSets and XML support. A DataSet is a disconnected, memory-resident representation of the DB, i.e. it contains tables, columns, rows, relationships, etc. Once data is read from a data provider, it is placed in a memory-resident DataSet, which is then disconnected from the data provider. The data consumer application interacts with the data in the DataSet object to make inserts, updates, and deleted in the DataSet. Once the processing is done, the DataSet data is synchronized with the data source and the changes are made permanent. Dataset is internally stored in XML format. A DataSet is a XML-based, in memory database that represents the persistent data stored in the data source. So it keep data from data source permanently Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace - Main components of ADO.NET object model:  ADO.NET framework consolidates all data access functionality under one integrated object model.  In this model several objects interact with one another to perform specific data manipulations.  All these objects can be grouped as data providers and consumers.  ADO.NET comes with 2 standard data providers: one for OLE-DB sources and one for SQL server. In this way ADO.NET can work with any previously supported DB. Descriptions of some of the ADO.NET components/objects: o Connection: This object defines the data source used, the name of the server, the database, etc. Enables the client application to open and close a connection to a DB o Command: This object represents a DB command to be executed within a specific DB connection. Contains the actual SQL code or store procedure call to the run by the DB. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace o DataReader: This object is a specialized object that creates a read-only session with the DB to retrieve data sequentially and very quickly. DataReader creates read-only session o DataAdapter: Is in charge of managing a DataSet object, and it is the most specialized object in ADO.NET. Contains the following objects that aid in managing the data in the DataSet: SelectCommand, InsertCommand, UpdateCommand. TheDataAdapter uses these objects to populate and synchronize the data in the DataSet with the permanent data source data. o DataSet: This object is the in-memory representation of the data in the database. It contain 2 main objects: DataTableCollection & DataRelationCollection. The DataTableCollection object contains a collection of DataTable objects that make up the “in-memory” database. The DataRelationCollection object contains a collection of objects that describe the data relationships and ways to associate one row in a table to the related row in another table. o DataTable: Represents the data in tabular form. Think table in tabular form. Has 1 important property: PK, which allows the enforcement of entity integrity. Has 3 main objects: Think: 3 collection and table have datacolumns, datarows and constraints.  DataColumnCollection: Contain column description  DataRowCollection: contain rows  ConstraintCollection: Constraint the definition of the constraints. 2 types are supported: FK & unique - So DataSet is simple DB with tables, rows and constraint and what makes it special is it does not need permanent connection to the data source. - DataAdapter uses SelectCommand object to populate the DataSet from a data source. - DataTable objects in a DataSet can come from different data sources. - ADO.NET framework is optimized to work in disconnected environments.  Oracle’s Java Database Connectivity(JDBC) Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace - Java is object-oriented programming language that runs on top of web browser software. It is one of the most-common programming languages for web development. - Java DB connectivity (JDBC) is an application programming interface that allows a Java program to interact with a wide range of data sources, including relational DBs, tabular data sources, etc. - JDBC allows a Java program to establish a connection with a data source, prepare and send the SQL code to the DB server and process the result set. - JDBC allows direct access to a DB server or access via DB middleware. - JDBC provide way to connect to DB through ODBC driver. - JDBC architecture: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace - One advantage of JDBC over other middleware is that it requires no configuration on the client side. The JDBC driver is automatically downloaded and installed as part of the Java applet download. - Since Java is web-based technology, application can connect to a DB directly using a URL. DB internet connectivity Internet DB connectivity opens the door to the following innovative services: Table that shows Internet technology and what benefits they provide” Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace The effects of bad DB design, implementation, and management are magnified in an environment in which transactions might be measured in hundreds of thousands per day rather than hundreds. Web-to-DB Middelware: Server-Side Extensions  Web server is the main hub through which all Internet services are accessed.  User uses web browser to dynamically query a DB and then this browser request a webpage from the web server. Then web server looks for page on hard disk and when it finds it the server sends it back.  In Dynamic webpages, the web server generates the webpage contents before it sends the page to the client web browser. Problem with this is that web server must include the DB query result on the page before it sends that page back to the client. Neither web browsers knows how to connect and read data from the DB. Thus the server’s capability must be extended so it can understand and process Db requests.  This job is known as server-side extension which is a program that interacts directly with the web server to handle specific types of requests. So the server-side extension program retrieves the data from the DBs and passes the retrieved data to the web server, which in turn sends the data to the client’s browser for display. The server- side extension provides its services to the web server in a way that is totally transparent to the client browser.  The DB server side extension program is also known as a web-to-DB middleware.  Def: Web-to-DB middleware: A DB server-side extension that retrieves data from DBs and passes them to the web server which in turn sends the data to the client’s browser for display.  Interaction between the browser, the web server and the web-to- DB middleware: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace E E Web-To-DB Middleware allows end-users to interact with DBs over the web Trace the web-to-DB middleware actions: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Web Server Interfaces  In previous section we discussed importance of web-to-DB middleware that must communicate with the web server.  Extending web server functionality implies that the web server and the web-to-DB middleware will properly communicate with each other.  A web server interface (boundary across which 2 objects communicate) defined a standard way to exchange messages with external programs.  2 web server interfaces exist: Think: Uses common gateway to communicate and API is an interface - Common Gateway Interfaces (CGI): uses script files that perform specific functions based on the client’s parameters that are passed to the web server. Script file is a small program containing commands written in programming language. It can be used to connect to DB and retrieve data from it. The script converts the retrieved data to HTML format and passes the data to the web server, which sends the HTML-formatted page to the client. Disadvantage of using script is that it is external program that executes separately for each user request and therefore causes a resource bottleneck. - Application programming interface (API): it is more efficient and faster than a CGI script. They more efficient because they are implemented as a shared code or as a dynamic-link libraries (DLLs). This means the API is treated as part of the web server program that is dynamically invoked when needed. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace API faster than CGI scripts because the code resides in memory, so no need to run an external program for each request. Same API serves all requests. Another advantage is API can use a shared connection to the DB instead of creating a new one everytime. Disadvantage: Since API share same memory as the web server, an API error can bring down the web server. Also, APIs are specific to web server and OS. Web server, CGI and API (web interface architecture):  The connection that web-to-DB middleware program makes with the DB can be accomplished in 1 of the 2 ways: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace The Web Browser  Software such as Google Chrome, Microsoft Internet Explorer.  Each time user clicks on hyperlink, the browser generates an HTTP GET page request that is sent to the designated web server using TCP/IP Internet protocol.  Web browser’s job is to interpret the HTML code that it receives from the web server and to present the various page components in a certain way.  Browser’s capabilities not sufficient to develop web-based applications.  Web is a stateless system- at any given time, a web server does not know the status of any of the client’s communication with it. i.e. there is no open communication line between the server and each client.  Client and server computers interact in short conversations that follow the request-reply model.  Browser does not have computational abilities beyond formatting output text and accepting form field inputs. Client-Side Extensions  Client-side extensions add functionality to the web browser.  Most common forms are: - Plug-ins: are external applications that is automatically invoked by the browser when needed. It is associated with data object that allow the web server to properly handle data that is not originally supported. - Java and JavaScript: JavaScript is a scripting language that allows web authors to design interactive sites. The code is embedded in a webpage and executed after a specific event, such as mouse click. - ActiveX and VBScript: ActiveX is Microsoft’s alternative to Java. It is a specification for writing programs that run in Internet Explorer. VBScript is another Microsoft product that is used to extend browser functionality. Like JavaScript, VBScript code is embedded inside an HTML page and is activated by triggering events such as clicking a link. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Web Application Servers  A web application server is a middleware application that expands the functionality of a web servers by linking them to a wide range of services, such as DBs, directory systems, and search engines.  They also provides a consistent runtime environment for web applications.  They can be used to perform the following:  They provides the following features:  Examples are ColdFusion/JRun by Adobe, WebSphere Application Server by IBM, etc.  They offer the ability to connect web servers to multiple data sources and other services. Web DB Development  This deals with the process of interfacing DBs with the web browser, i.e. how to create webpages that access data in a DB. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace  WE will be illustrating some web-to-DB interface examples by using a DB with the name of CH15_Orderdb:  WE will be learning ColdFusion and PHP. Doing an example how to use ColdFusion (which is a web application server which does the following: connect web servers to multiple data sources and other services) and PHP to create a simple webpage to list the VENDOR rows. The scripts in these examples perform 2 basic tasks: The ColdFusion code to query the VENDOR table: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace The tags CFQUERY and CFOUTPUT are used in ColdFusion to query a DB table. You can find them in line 4 & 15. Describing these 2 tags: This tag sets the stage for the DB connection and the execution of the enclosed SQL statements. Uses the following parameters: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace This tag is used to display the results from a CFQUERY tag or call other ColdFusion variables or functions. Its parameters are: Next we show PHP code to query the same table: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Let’s have a look at the PHP functions: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace A Extensible Markup Langauge(XML)  HTML tags describe how something looks on the webpage, such as heading styles.  Tag: in markup languages (HTML or XML) a command inserted in a document to specify how the document should be formatted.  HTML document can only describe how to display the order in a web browser; it does not permit the manipulation of a document’s data elements. Thus, a new markup language know as XML was developed.  XML helps with e-commerce of mostly B2B.  XML is a meta-language used to represent and manipulate data elements.  XML is designed to facilitate the exchange of structured documents, such as orders and invoices, over the Internet.  XML is said to be extensible (due to the x)  XML derived from Standard Generalized Markup Language(SGML)  XML’s additional characteristics over SGML: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace  XML is concerned with the description and representation of the data, rather than the way the data is display as with HTML.  XML provides the semantics that facilitate the sharing, exchange, and manipulation of structured documents over organizational boundaries.  Example of B2B example where company A uses XLM to send product data with company B: productlist.xml: XML important features illustrated by example: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace But how can company receiving this know data is correct or what if the company expects another data elements as well. The following section describe such problems: Document Type Definitions (DTD) and XML Schemas  B2B companies that exchange data must have way to validate and understand each other’s tags. To accomplish this is through the use of document type definition.  A document type definition (DTD) is a file with a .dtd extension that describes XML elements. It provides the composition of the DB’s logical model and defines the syntax rules or valid elements for each type of XML document.  Companies that engage in e-commerce with 1 another must develop and share DTDs.  Figure of productlist.dtd document for the productlist.xml document shown earlier: This DTD file provides definitions of the elements in the productlist.xml document. Note the following: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace  The DTD must be referenced within a XML document to be able to use a DTD file to define elements within and XML document. Here is an example: The productlistv2.xml document that includes reference to productlist.dtd: Note that P_INDATe & P_MIN do not appear in all Product definitions because they were declared to be optional elements. The DTD can be referenced by many XML documents of the same type.  Consider another example of 2 companies exchanging order data: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace A DTD only provides descriptive information for understanding how the elements-root, parent, child, mandatory, or optional-relate to one another. DTD does not provide data type or validation rules.  To solve this problem an XML schema standard was design that better described XML data. XML schema is an advanced data definition language that is used to describe the structure of XML data documents. This include elements, data types, relationship types, ranges, and default values.  Main advantage of XML schema is that it is more closely maps to DB terminology and features over DTD. Using XML schema companies may check for data that is out of range.  Unlike DTD document, which uses a unique syntax, and XML schema definition (XSD) file uses a syntax that resembles an XML document. XML Presentation Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace o Main benefits of XML is it separates data structure from its presentation and processing. By separating the 2, you can present the same data in different ways. o Extensible Style Language (XSL) specification provides the mechanism to display XML data. It’s used to define the rules by which XML data is formatted and displayed. o XSL specification is divided into 2 parts: - Extensible Style language Transformation (XSLT): describes the general mechanism that is used to extract and process data from one XML document and enable its transformation within another document. With this you can extract data from XML document and convert it into a text file, HTML webpage, or a webpage - XSL style sheet: define the presentation rules applied to XML elements. Think style in HTML is to define its presentation. It describes the formatting options to apply to XML elements when displayed in a browser So with XSLT and XSL you can take an XML document and transform it into another document or you can translate it into viewable webpages. XML Applications Uses (applications) of XML: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Cloud Computing Services Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace E E - Cloud computing is a computing model for enabling ubiquitous convenient, on-demand network access to a shared pool of configurable computer resources that can be rapidly provisioned and released with minimal management effort or service provider interaction. - Cloud services refer to the services provided by cloud computing. - With cloud services you can scale up, scale down, add more processing power, etc. - Cloud computing eliminates financial and technological barriers so organizations can leverage DB technologies in their business processes with minimal effort and cost. - Cloud services allow any organization to quickly and economically add information technology services such as applications, storage, servers, processing power, DBs, and infrastructure to its IT profile. Cloud Implementation Types Cloud computing types of implementation based on target customers:  Public cloud: Built by 3 rd party organizations to sell cloud services to the general public.  Private cloud: Internal cloud that is built by an organization for the sole purpose of servicing its own needs.  Community cloud: Built for a specific group of organizations that share common trade. Characteristics of Cloud Services The characteristics are:  Ubiquitous(universal) access via Internet technologies  Shared infrastructure  Lower costs and variable pricing  Flexible and scalable services  Dynamic provisioning  Service orientation  Managed operations Types of Cloud Services Categories of cloud services: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace  Software as a Service (SaaS): Cloud service providers offers applications that run in the cloud. Software is application  Platform as a Service (PaaS): Cloud service provider offers the capability to build and deploy consumer-created applications using the provider’s cloud infrastructure. Platform, so build it on their platform Here the consumer can build, deploy and manage applications using the providers cloud tools.  Infrastructure as a Service (IaaS): The cloud service provider offers consumers the ability to provision their own resources on demand, such as storage, servers, DBs, processing units, and even virtualized desktop. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Cloud Services: Advantages and Disadvantages Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Top advantage is lower cost. Top disadvantage is data security and privacy. SQL Data Services o Cloud vendors have expanded their business to offer SQL data services o SQL data services (SDS) refers to a cloud computing-based data management service that provides relational data storage, access, and management to companies of all sizes without typically high costs of in-house hardware, software, infrastructure, and personnel. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace It is basically a relational DB that is provided to a company by another company via the cloud. SDS provide the following benefits: SDS advantages compared to in-house systems: You do not have to worry about backups, fault tolerance, scalability, and rouytine maintenance tasks when you use SDS. Summary Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Chapter 16 - DB administration and security Data as a corporate Asset  Need to consider data’s monetary value.  Data is valuable resource that can translate into information.  An organization is subject to a data-information-decisions cycle; that is, the data user applies intelligence to data to produce information that is the basis of knowledge used in decision making. Explained in this figure: Decisions made by high-level managers trigger actions within organization lower levels. These actions produce additional data to be used for monitoring company performance. This data must be recycled within the data-information-decision-making- cycle.  Most organizations continually seek new ways to leverage their data resources to get greater returns.  As organizations become more dependent on information, that information’s accuracy becomes more critical. Dirty data (data that Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace suffers from inaccuracies and inconsistencies) becomes an even greater threat. Data can become dirty from the following possible reasons: - Lack of enforcement of integrity constraints, e.g. NOT NULL - Data-entry errors and typographical errors - Use of synonyms and homonyms across systems - Nonstandard use of abbreviations in character data - Different decompositions of composite attributes into simple attributes across systems  Efforts to control dirty data are generally referred to as data quality initiatives.  Data quality is a comprehensive approach to ensuring the accuracy, validity, and timeliness of data. This approach important, because it involves more than just cleaning data, it also focuses on preventing future inaccuracies and building user confidence in the data.  Data quality involves the following: - Data governance structure responsible for data quality - Measurements of current data quality - Definition of data quality standards in alignment with business goals - Implementation of tools and processes to ensure future data quality.  Tools that can assist in data quality initiatives: - Data-profiling software: It gathers statistics; analyses existing data sources and metadata to determine data patterns; and compares the patterns against standards that the organization has defined. This help to asses quality of existing data and identify sources of dirty data. - Master data management(MDM) software: helps to prevent dirty data by coordinating common data across multiple systems. It provides a master copy of entities, such as customers, that appear in numerous systems throughout the organization Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace The need for a DB and its role in an organization  Data is used by different people in different departments for various reasons.  Used properly the DBMS facilitates: - Interpretation and presentation of data in useful formats by transforming raw data into information - Distribution of data and information to the right people at the right time - Data preservation and monitoring data usage for adequate periods of time - Control over data duplication and use, both internally and externally  Regardless of organization, the DB’s predominant role is to support managerial decision making at all levels in the organization while preserving data privacy and security.  Organization’s managerial structure divided into 3 levels: - Top level management: makes strategic decisions – effect long-term wellbeing of the company - Middle management: tactical decisions – involves longer time frame and effect larger-scale operations than operational decisions - Operational Management: working decisions/ operational decisions - Short term  DBMS must give each level of management a useful view of that data and support the required level of decision making.  Activities typical in each management level: Top management - Provide information necessary for strategic decision making, strategic planning, policy formulation, and goals definition - Provide access to external and internal data to identify growth opportunities and to chart the direction of such growth - Provide framework for defining and enforcing organizational policies that are translated into business rules at lower levels in organization. - Improve the likelihood of a positive return on investment by searching for new ways to reduce costs and boost productivity in company. Middle management - Deliver data necessary for tactical decisions and planning Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace - Monitor and control the allocation and use of company resources and evaluate the performadnce of various departments - Provide a framework for enforcing and ensuring the security and privacy of the data in DB. Security: protecting the data against accidental or international use by unauthorized users. Privacy: extent to which individuals and organizations have the right to determine the details of data usage Operational management: - Represent and support company operations as closely as possible. - Produce query results within specified performance levels - Enhance the company short-term operations by providing timely information for customer support and for application development and computer operations.  Company’s DB also known as corporate or enterprise DB. Enterprise DB defined as the company’s data representation that provides support for all present and expected future operations. Introduction of a DB: Special considerations o DBMS/DB management will NOT GAURENTEE that data used properly. o DBMS is tool for managing data, but it must be used effectively. o Introduction of DBMS affect organization o DBMS described as process that includes 3 important aspects: Technological: DBMS software and hardware Here it includes selecting, installing, configuring and monitoring the DBMS to make sure that it efficiently handles data storage, access and security DB administration staffing is key technological consideration because: personnel is in charge of installing DBMS and must have technical skills to provide or secure adequate support for various users of the system. Managerial: Administrative functions This aspect includes management of the services and the relationship with the cloud-based data service provider. Introduction of a DBMS requires careful planning to create an appropriate organizational structure and accommodate the personnel responsible for administering the system Top management must be committed to the new system and must define and support data administration functions, goals, and roles within the organization Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace E Cultural: Corporate resistance to change DBMS likely to affect people, functions, and interactions. E.g. additional personnel might be hired Desirable technical skills/understanding the DBA’s personnel should have: - They must have the right mix of technical and managerial skills to provide a smooth transition to the new shared-data-environment - They must have excellent interpersonal and communications skills combined with broad organizational and business understanding - Personnel must have broad DB design and programming skills The evolution of DB administration  DP (data processing) is centralized data administration which were used to administrate old file systems.  Introduction of DBMS and its shared view of data produces a new level of data managements and led to DP department to evolve into an information system (IS) department.  Responsibility of the IS department were broadened to include the following: - A service function to provide end users with data management support - A production function to provide end users with solutions for their information needs through integrated application or management ISs  Function of the IS department was reflected by its internal structure a typical structure shown in the following figure: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace  As IS evolved the IS application development segment was subdivided by the type of system it supported: accounting, inventory, marketing and so on.  As number of DB applications grew, data management became increasingly complex, thus leading to the development of DB administration. Person responsible for control of the centralized and shared DB became known as DB administrator(DBA).  On the organizational chart, the DBA function might be defined as either a staff or line position. In staff position, the DBA often takes on consulting role; the DBA can devise the data administration strategy but does not have the authority to enforce it or resolve possible conflicts. In line position, the DBA has both the responsibility and authority to plan, define, implement, and enforce the policies, standards, and procedures used in data administration.  Fast-paced changes in DBMS technology dictate changing organizational styles, e.g.: - Development of distributed DBs can force an organization to decentralize data administration further. Distributed DB require DBA to define and delegate responsibilities of each local DBA Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace - Growing use of Internet-accessible data and the growing number of data warehousing applications are likely to expand the DBA’s data- modelling and design activities. - The increasing sophistication and power of personal-computer-based DBMS packages provide an easy platform for developing user-friendly, cost-effective, and efficient solutions. - The increasing use of cloud data services is pushing many DB platforms and infrastructures into the cloud. - The growing of Big Data in an organizations can force the DBA to become more technology-oriented.  DBA operations are commonly defined and divided according to the phases of the DB Life Cycle (DBLC). If this approach used, the DBA function requires personnel to cover the following activities:  Company might have several DBMSs installed to support different operations. Company might have 1 DBA assigned for each DBMS. The general coordinator of all DBAs is known as systems administrator: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace  Some larger corporations make distinction between DBA and data administrator(DA). The DA, also known as information resource manager(IRM), usually reports directly to top management and is given a higher degree of responsibility and authority than the DBA, although the 2 roles may overlap. DA responsible for controlling the overall corporate data resources, both computerized and manual. DA controls data outside scope of DBMS in addition to computerized data. The DB environment’s Human component  Here we look at the DB from user perspective and the data administration activities that make a good DB design useful.  DBA must have considerable people skills. Both DA and DBA direct and control personnel staffing and training within their respective departments. Let’s look at table listing DA and DBA general characteristics. All activities assigned to DBA if organization do not have a DA. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace In general DA provide a global and comprehensive administrative strategy for organization’s data. i.e. DA’s plans must consider entire data spectrum, thus DA responsible for both manual and computerized data.  DA must also set data administration goals:  DA and DBA define and control the way company data is used  DA and DBA not have universally accepted administrative standards, it depends on and vary from company to company. i.e. Different DA’s don’t manage data the same way  The arbitration (negotiation) of interactions between the two most important assets of any organization, people and data, places the DBA in the dynamic environment portrayed in this figure: DBA defines and enforces the procedures and standards to be used by programmers and end users during their work with the DBMS. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace  DB users might be classified by the following: - Type of decision-making support required (operational, tactical, or strategic) - Degree of computer knowledge( novice, proficient, or expert) - Frequency of access( casual, periodic, or frequent) These classifications can overlap, e.g. operational user can be an expert with casual DB access. Thus each organization employs people of diff. DB expertise and DBA must be able to interact with all of them. The skills of a DBA can be divided into 2 categories: DBA must perform 2 roles: - DBA’s managerial role is focused on personnel management and in interaction with end users - DBA’s technical role involves the use of the DBMS- DB design, development, and implementation. The DBA’s Managerial Role o As manager, DBA must concentrate on control and planning of DB administration. DBA responsible for: More specifically the DBA’s responsibilities are: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace This means that DBA responsible for planning, organizing, testing, etc. (all of the left) of all the services on the right. These services might be performed by the DBA, although more likely to be performed by DBA’s personnel. The above mentioned services in greater detail:  End-user support DBA interacts with end users by providing data and information support to their departments. End-user support services include: - Gathering user requirements: - building end-user confidence: - Resolving conflicts and problems: - Finding solutions to information needs: - Ensuring quality and integrity of data and applications: - Managing the training and support of DBMS users:  Policies, procedures and standards Successful data administration strategy requires the continuous enforcement of policies, procedures, and standards for correct data creation, usage, and distribution within the DB. DBA must define, document and communicate the following before they can be enforced: Policies: general statements of direction or action that communicate and support DBA goals. Standards: describe the minimum requirements of a given DBA activity; more detailed than policies. They are rules that evaluate the quality of the activity Procedures: written instructions that describe a series of steps to be followed during the performance of a given activity. Examples: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Standards and procedures defined by the DBA apply to all end users. DBA must define, communicate and enforce procedures that cover areas such as: - End-user DB requirement gathering - DB design and modelling - Documentation and naming conventions - Design, coding, and testing of DB application programs - DB software selection - DB security and integrity - DB backup and recovery - DB maintenance and operation - End-user training  Data security, privacy, and integrity DBA must use the security and integrity mechanisms provided by the DBMS to enforce the DB administration policies  Data backup and recovery DBA must ensure that data can be fully recovered in case of data loss or loss of DB integrity Management of DB security, integrity, backup, and recovery is so critical that DBA departments have created a position called the DB security officer (DSO). DSO’s job is to ensure Db security and integrity. DSO’s activities classified as disaster management. Disaster management includes all of the DBA activities designed to secure data availability following a physical disaster or DB integrity failure. It includes all planning, organizing, and testing of DB contingency plans and recovery procedures. The backup and recovery measures of Disaster Management must include the following: - Periodic data and application backups: Full backup (Also called DB dumb, produces a complete copy of the entire DB), incremental backup(produces a backup of all data since last backup) and concurrent back(takes place while the user is working on the DB) Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace E - Proper backup identification: Backups must be clearly identified. - Convenient and safe backup storage: Multiple backups of the same data are required and each backup copy stored in a different location - Physical protection of both hardware and software - Personal access control to the software of a DB installation - Insurance coverage for the data in the DB  Data distribution and use DBA responsible for ensuring that data is distributed to the right people, at the right time, and in the right format. One way for users to gain access to data is applications programs, but new ways using query tools and new web front ends. The DBA’s Technical Role  DBA’s technical role requires a broad understanding of DBMS functions, configurations, programming languages, and data- modelling and design methodologies.  DBA’s job are rooted in the following areas of operation: - Evaluating, selecting, and installing the DBMS and related utilities: 1st step of DBA is to determine company needs and then selecting the DBMS, utility software and supporting hardware to meet these needs. - Designing and implementing DBs and applications: DBA provides data- modelling and design services to end-users. Primary activity of DBA is to determine and enforce standards and procedures to be used then the DBA must ensure the database-modelling and design activities must conform to the procedures and standards. - Testing and evaluating DBs and applications: DBA must provide testing and evaluation services for all DB and end user applications. - Operating the DBMS, utilities, and applications: The DBA must ensure system performance is satisfactory by establishing DBMS performance goals and then fixing performance issues. DBA must also consider available storage resources and deal with such issues. DBA must also manage backups effectively. - Training and supporting users: The DBA must train people to use the DBMS and it’s tools effectively. DBA must give support to end-users and programmers for when they encounter issues. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace - Maintaining the DBMS, utilities, and applications: The DBA must manage the maintenance of the DBMS by managing the physical or secondary storage devices. Maintenance for the DBA also includes upgrading the DBMS and utility software with added features. Now we explore the details of each area: - 1st and most important technical responsibility of DBA is selecting the DBMS, utility software, and supporting hardware to be used in the organization. 1st and most important step of the plan is te determine company needs. Once needs are identified, the objective of data administration can be clearly established and the DBMS features and selection criteria can be defined. To match DBMS capabilities to the organizations needs, the DBA develop a checklist of desirted DBMS features that addresses the folllwing issues:  DBMS model  DBMS storage capacity  Application development support  Security and integrity  Backup and recovery  Concurrency control  Performance  DB administration tools  Interoperability and data distribution  Portability and standards  Hardware  Data dictionary  Vendor training and support  Available 3rd-party tools  Costs If cloud data services are considered, these produce additional issues. Any potential cloud-based vendors need to be evaluated based on several factors, including:  Downtime history  Security  Support  Data loss contingencies DBMS requires support from collateral hardware, application software, and utility programs. E.g. DBMS’s use is likely to be constrained by the available CPU(s), front-end processor(s), storage devices, OS, communication devices, etc. Cost associated with the hardware and software must be included in estimation when selecting DBMS. DBA must supervise the installation of all software and hardware that supports the data administration strategy, and must thoroughly understand Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace A the components being installed, including their installation, configuration, and startup procedures. - DBA provides data-modelling and design services to end users. Primary activity to determine and enforce standards and procedures to be used. Once these are in place, the DBA must ensure that the DB modelling and design activities are performed within framework. DBA offers support to modelling. DBA also schedules desing jobs to coordinate the data design and modelling activities. DBA work with application programmers to ensure quality and integrity. Reviewing the DB application design to ensure traqnsactions are: Correct, efficient and compliant. DBA provide assistance durign physical designDBA implementation tasks also include the generation, compilation, and storage of the application’s access plan. An access plan is a set of instructions generated when the application is compiled that predetermines how the application will access the DB at run time. Before application come online, DBA must develop, test and implement the operational procedures required by the new system. DBA must also authorize application users to access the DB from which the applications draw the required data. - DBA must provide testing and evaluation services for all DB and end-user applications. Testing and evaluation must be maintained independently. Testings purpose is to check the data definition and integroty riles of DB and application programs. Evaluation covers the following:  Technical aspects of both the applications and the DB  Evaluation of the written documentation and procedures to ensure that they are accurate and easy to follow  Observance of standards for naming, documenting, and coding  Checking for data duplication conflicts with existing data  The enforcement of all data validation rules - DBMS operations can be divided into 4 main areas: System support activities: cover all tasks directly related to the day-to-day operations of the DBMS and its applications. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Performance monitoring and tuning activites are designed to ensure that the DBMS. Utilities, and applications maintain satisfactory performance levels. To carry out these tasks, the DBA must: Most performance monitoring tools allow DBA to focus on selecting system bottlenecks. DBA must train programmers in proper use of SQL statements. DBA must be familiar with the factors that influence concurrency. DBA must also consider available storage resources in terms of both primary and secondary memory. Storage configuration parameters can be used to determine: Performance-monitoring issues are DBMS-specific. Backup and recovery DBA must establish a schedule for backing up DB and log files at appropriate intervals, because backup and recovering is crucial. Security auditing and monitoring assumes the appropriate assignment of access rights and the proper use of access privileges by programmers and end users. DBA must create users, assigning access rights, and using SQL commands to grant and revoke access rights to users and DB objects. Dba must also periodically generate an audit trail report to find actual or attempted security violations. - DBA must train people to use DBMS and its tools is part of the DBA’s technical activities. DBA provides or secures technical training for application programmers in the use of the DBMS and its utilities. - DBA maintenance activities are an extension of the operational activities. Maintenance activities are dedicated to the preservation for the DBMS environment. DBMS maintenance activities include management of physical or secondary storage devices; reorganizing physical location of data in the DB and upgrading DBMS and utility software. Maintenance efforts of the Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace DBA include migration and conversion services for data in incompatible formats. Security  Information system security refers to activities and measures that ensure the confidentiality, integrity, and availability of an information system and its main asset, data.  Securing data include securing all the process and systems around it, e.g. hardware systems, software applications, network, users, etc.  Consider the security goals in more detail:  Confidentiality: deals with ensuring that data is protected against unauthorized access, and if the data is accessed by an authorized user, that it is used only for an authorized purpose. i.e. it entails safeguarding data against disclosure of any information that would violate the privacy rights of a person or organization. Data must evaluated and classified according to level of confidentiality: Highly restricted (few people have access), confidential and unrestricted. Compliance refers to activities that meet data privacy and security reporting guidelines. Great time is spent ensuring the organization is in compliance with desired levels of confidentiality.  Integrity: is concerned with keeping data consistent and free of errors or anomalies.  Availability: refers to the accessibility of data whenever required by authorized users and for authorized purposes. Security policies:  Tasks of securing the system and its main asset, the data, are performed by the DB security officer and the DB admins.  Such strategies begins with defining a sound and comprehensive security policy. A security policy is a collection of standards, policies, and procedures created to guarantee the security of a system and ensure auditing and compliance.  Security audit (assessment) process starts by identifying security vulnerabilities. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Security vulnerabilities o Security vulnerability is a weakness in a system component that could be exploited to allow unauthorized access or cause service disruptions. o Security vulnerabilities categories: o When security vulnerability is left unchecked, it could become a security threat. Security threat is an imminent security violation. o A security breach occurs when a security threat is exploited to endanger the integrity, confidentiality, or availability of the system. Security breach can lead to DB whose integrity is either preserved or corrupted: Security vulnerabilities of system components and typical protective measures against them: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace DB security  DB security refers to DBMS features and other related measures that comply with the organization’s security requirements. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace  From DBA point of view, security measures should be implemented to protect the DBMS against service degradation and to protect the DB against loss, corruption, or mishandling.  To protect the DBMS against service degradation, these security safeguards are recommended:  DBA should work with network admin. to implement network security  Protecting the data in the DB is a function of authorization management. Authorization management defines procedures to protect and guarantee DB security and integrity. Procedures in it includes:  User access managements: This function is designed to limit access to the DB. It includes the following procedures:  Define each user to the DB  Assign passwords to each user  Define user groups  Assign access privileges  Control physical access  View definition: The DBA must define data views to protect and control the scope of the data that are accessible to an authorized user.  DBMS access control: DB access can be controlled by placing limits on the user of DBMS query and reporting tools  DBMS usage monitoring: The DBA must also audit the use of data in the DB. Several DBMS packages contain features that allow the creation of an audit log, which automatically records a brief description of DB operations performed by all users. DB administration Tools  You can find data administration tools for:  DB monitoring  DB load testing Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace  DB performance tuning  SQL code optimization  DB bottleneck identification and remediation  DB modelling and design  DB data extraction, transformation, and loading  All of these tools expand the data dictionary  Data dictionary as a DBA tool is very important The data dictionary (DD) o 2 types of data dictionaries exist: integrated (included with the DBMS) and standalone (not included). o Data dictionaries can also be classified as active or passive. An active data dictionary is automatically updated by the DBMS with every DB access to keep its access information up to data. A passive data dictionary is not updated automatically and usually requires running a batch process. o Data dictionary’s main function is to store the description of all objects that interact with the DB. o Typically the data dictionary stores descriptions of the following:  Data elements that are defined in all tables of all DBs  Tables defined in all DBs  Indexes defined for each DB table  Defined DBs  End users and administrators of the DB  Programs that access the DB  Access authorizations for all users of all DBs  Relationships among data elements o DD can be organized to include data external to the DBMS itself- thus makes it possible to manage the use and allocation of all the organization’s information-thus DD considers to be key element of information resource management-thus D can be described as the information resource dictionary. o Information stored in the DD is in table format, thus DBA can query the DB with SQL commands. o How a DD is used to derive information: Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace o DD can be a tool for monitoring DB security by checking the assignment of data access privileges. o Information about the application programs that access the DB can also be drawn from the DD. o To conclude a DD is very useful tool for DBA to ensure security, consistency, etc. Case Tools  Also a tool to help DBA to manage DB.  CASE is acronym for Computer-aided systems engineering. It provides an automated framework for the SDLC.  CASE tools classified to extent of support they provide for the SDLC: Front-end CASE tools provide support for the planning, analysis, and design phases; Back-end CASE tools provide support for the coding and implementation phases.  Benefits of case tools:  Most important component of CASE tool is an extensive DD, which keeps track of all objects created by the systems designer. E.g. The CASE DD stores data flow diagrams, structure charts, descriptions of all external and internal entities, data stores, etc.  The CASE tool integrates all systems development information in a common repository, which the DBA can check for consistency and accuracy.  The DBA can use the CASE tool to check the definition of the application’s data schema, the observance of naming conventions, the duplications of data elements, validation rules for the data Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace elements, and a host of other development and managerial variables.  A typical CASE tool provide 5 components:  Most CASE tools produce fully documented ER diagrams that can be displayed at different abstraction levels. Developing a Data administration Strategy  The DB administration strategy must not conflict with the IS plans, because these plans are derived from a detailed analysis of the company’s goals.  Information engineering (IE) allows for translation of the company’s strategic goals into the data and applications that will help the company achieve those goals.  Output of the IE process is an Information systems architecture (ISA) that serves as the basis for planning, development, and control of future ISs.  An ISA provides a framework that includes computerized, automated and integrated tools such as DBMS and CASE tools. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace  Success of overall IS strategy and data administration strategy depends on the following ciritcal factors that the DBA needs to understand: The DBA’s Role in the cloud  Some tasks are now split between the internal DBA and the cloud service provider with the introduction of cloud based technology.  In general the cloud services partner company provides:  DBMS installation and updates: Now DBA’s role is to coordinate updates with the external cloud-based data service provider  Server/network management  Backup and recovery operations  These services free the DBA from these tasks.  DBA may evaluate different DBMSs offers by cloud vendors. Also, the DBA must work with the cloud data services provider to reconcile the required DB technical features with the ones supported by the cloud data service provider and ensure data availability, security, and integrity. Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace  Regardless of whether the DB is stored in the enterprise’s server or in the cloud, the DBA must ensure the data’s availability, security, and integrity. The DBA at work: Using Oracle for DB administration  This section provides a more detailed look at how a DBA might handle the following technical tasks in a specific DBMS:  Oracle is used to illustrate how these tasks are used.  Now we will discuss Oracle’s DB administration tools and its procedures: Oracle DB administration Tools In Oracle you perform most DBA tasks via the Oracle Enterprise Manger interface. Ensuring that the RDBMS Starts Automatically One of DBA’s basic tasks is to ensure DB access starts automatically when you turn on computer. A service is the Windows name for a special program that runs auto. as part of OS. Note following Oracle services: END Of chapter summary Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Stuvia.com - The study-notes marketplace Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal Stuvia.com - The study-notes marketplace Powered by TCPDF (www.tcpdf.org) Downloaded by: YeNahGetThat | likewise52@protonmail.com Distribution of this document is illegal","libVersion":"0.2.3","langs":""}